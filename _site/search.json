[
  {
    "objectID": "pyTutorial8.html",
    "href": "pyTutorial8.html",
    "title": "Tutorial 8 - Métodos de Classificação",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing"
  },
  {
    "objectID": "pyTutorial8.html#carregando-os-pacotes",
    "href": "pyTutorial8.html#carregando-os-pacotes",
    "title": "Tutorial 8 - Métodos de Classificação",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing"
  },
  {
    "objectID": "pyTutorial8.html#classificação",
    "href": "pyTutorial8.html#classificação",
    "title": "Tutorial 8 - Métodos de Classificação",
    "section": "Classificação",
    "text": "Classificação\nEm machine learning, um problema de classificação é a tarefa de ensinar um computador a categorizar um item em uma de várias classes ou grupos pré-definidos, com base em suas características. Imagine que você tem um conjunto de dados históricos, como e-mails já rotulados como “spam” ou “não spam”, ou peças de um motor identificadas como “aptas” ou “defeituosas”. O algoritmo aprende os padrões nesses dados para construir um modelo. O objetivo final é que esse modelo, ao receber um novo e-mail ou os dados de uma nova peça, consiga prever com alta precisão a qual classe ele pertence. Essencialmente, a classificação responde à pergunta: “A qual categoria este item pertence?”.\n\n\n\nO problema de classificação em ML. Fonte: Gong (2022)"
  },
  {
    "objectID": "pyTutorial8.html#os-algoritmos",
    "href": "pyTutorial8.html#os-algoritmos",
    "title": "Tutorial 8 - Métodos de Classificação",
    "section": "Os algoritmos",
    "text": "Os algoritmos\n\n1. Regressão Logística\n\n\n\nRegressão Logística. Fonte: Gong (2022)\n\n\nA Regressão Logística usa a função sigmoide para retornar a probabilidade de um rótulo. Ela é muito utilizada quando o problema de classificação é binário — por exemplo, verdadeiro ou falso, ganhar ou perder, positivo ou negativo.\nA função sigmoide gera um valor de probabilidade e, ao compará-lo com um limiar (threshold) pré-definido, o item é classificado com o rótulo correspondente.\n\n\n2. Árvore de Decisão\n\n\n\nÁrvore de Decisão. Fonte: Gong (2022)\n\n\nA Árvore de Decisão cria seus ramos de forma hierárquica, onde cada ramo pode ser entendido como uma estrutura condicional if-else.\nOs ramos são desenvolvidos dividindo-se o conjunto de dados (dataset) em subconjuntos, com base nas características (features) de maior importância. A classificação final ocorre nas folhas da árvore.\n\n\n3. Random Forest\n\n\n\nRandom Forests. Fonte: Gong (2022)\n\n\nComo o próprio nome sugere, o Random Forest (ou Floresta Aleatória) é um conjunto de árvores de decisão. É um tipo comum de método de ensemble, que agrega os resultados de múltiplos preditores.\nO Random Forest utiliza adicionalmente a técnica de bagging, que permite que cada árvore seja treinada com uma amostragem aleatória do conjunto de dados original e, ao final, considera o voto da maioria das árvores para o resultado.\nEm comparação com uma única árvore de decisão, este método tem uma melhor generalização, mas é menos interpretável, devido às múltiplas camadas de complexidade adicionadas ao modelo."
  },
  {
    "objectID": "pyTutorial8.html#lendo-os-dados",
    "href": "pyTutorial8.html#lendo-os-dados",
    "title": "Tutorial 8 - Métodos de Classificação",
    "section": "Lendo os dados",
    "text": "Lendo os dados\n\ndf = pd.read_csv('pydata5/card_transdata.csv')\n\ndf = df.sample(10000).copy()\n\n\ndf.head(10)\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\n486856\n9.214883\n0.431098\n0.314582\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n515789\n11.667397\n0.089375\n1.406865\n1.0\n0.0\n1.0\n1.0\n0.0\n\n\n443778\n22.532558\n1.506233\n0.404679\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n398263\n12.377269\n0.179923\n1.373075\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n887112\n4.046479\n0.049327\n0.505305\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n92342\n196.562848\n66.249490\n0.189499\n1.0\n0.0\n0.0\n1.0\n1.0\n\n\n755895\n9.842020\n77.916720\n1.805561\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n654083\n39.544694\n3.382505\n4.591887\n1.0\n0.0\n0.0\n1.0\n1.0\n\n\n199387\n26.615302\n4.439700\n1.520829\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n516171\n13.182010\n8.921574\n1.021895\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10000 entries, 486856 to 21156\nData columns (total 8 columns):\n #   Column                          Non-Null Count  Dtype  \n---  ------                          --------------  -----  \n 0   distance_from_home              10000 non-null  float64\n 1   distance_from_last_transaction  10000 non-null  float64\n 2   ratio_to_median_purchase_price  10000 non-null  float64\n 3   repeat_retailer                 10000 non-null  float64\n 4   used_chip                       10000 non-null  float64\n 5   used_pin_number                 10000 non-null  float64\n 6   online_order                    10000 non-null  float64\n 7   fraud                           10000 non-null  float64\ndtypes: float64(8)\nmemory usage: 703.1 KB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n26.629818\n5.456363\n1.872987\n0.879200\n0.351200\n0.099400\n0.650300\n0.090200\n\n\nstd\n67.546517\n24.969811\n2.957269\n0.325911\n0.477369\n0.299213\n0.476899\n0.286482\n\n\nmin\n0.037396\n0.001225\n0.013200\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n3.848924\n0.283383\n0.475306\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n9.873617\n0.994411\n1.013134\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n\n\n75%\n25.464573\n3.432359\n2.134546\n1.000000\n1.000000\n0.000000\n1.000000\n0.000000\n\n\nmax\n3243.926263\n1261.525290\n62.227930\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\n\ndf.query('distance_from_home &gt; 1000')\n\n\n\n\n\n\n\n\ndistance_from_home\ndistance_from_last_transaction\nratio_to_median_purchase_price\nrepeat_retailer\nused_chip\nused_pin_number\nonline_order\nfraud\n\n\n\n\n915645\n1741.752772\n0.660238\n0.792538\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n297794\n1464.765479\n11.120252\n0.289282\n1.0\n0.0\n0.0\n1.0\n1.0\n\n\n475654\n3243.926263\n0.962096\n2.584001\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n507911\n1204.761613\n351.530503\n0.502558\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n89453\n1005.276050\n3.224338\n0.903404\n1.0\n0.0\n0.0\n1.0\n1.0\n\n\n740780\n1031.638377\n45.573304\n0.376136\n1.0\n0.0\n0.0\n0.0\n0.0"
  },
  {
    "objectID": "pyTutorial8.html#análise-exploratória",
    "href": "pyTutorial8.html#análise-exploratória",
    "title": "Tutorial 8 - Métodos de Classificação",
    "section": "Análise exploratória",
    "text": "Análise exploratória\n\ncolors = ['#d62828', '#f77f00', '#003049']\n\nnum_list = ['distance_from_home', 'distance_from_last_transaction', 'ratio_to_median_purchase_price']\nfig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 10))\n\nfor i, column in enumerate(num_list):\n  df[column].plot(kind='kde', ax=axes[i], color=colors[i], lw=3)\n  axes[i].set_title(f'Distribuição de \"{column}\"', fontsize=14)\n  axes[i].set_xlabel('')\n  axes[i].set_xscale('log')\n  \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFeatures Categóricas e o target:\n\ncat_list = ['repeat_retailer','used_chip','used_pin_number','online_order']\nfig = plt.figure(figsize=(8,4))\n\nfor i in range(len(cat_list)):\n  column = cat_list[i]\n  sub = fig.add_subplot(2,2, i+1)\n  chart = sns.countplot(data=df, x=column, hue='fraud', palette = colors[0:2])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFeatures numéricas e o target:\n\nfig = plt.figure(figsize = (6,12))\nfor i, column in enumerate(num_list):\n  \n  ax = fig.add_subplot(3, 1, i + 1)\n  sns.boxplot(x='fraud', y=column, data=df, hue='fraud', palette=colors[0:2], ax=ax)\n  ax.set_yscale('log')\n  ax.set_title(f'Distribuição de \"{column}\" por Fraude (Escala Log)', fontsize=14)\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "pyTutorial8.html#split-dataset",
    "href": "pyTutorial8.html#split-dataset",
    "title": "Tutorial 8 - Métodos de Classificação",
    "section": "Split dataset",
    "text": "Split dataset\n\nX = df.drop(['fraud'], axis=1)\ny = df[\"fraud\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2025)"
  },
  {
    "objectID": "pyTutorial8.html#regressão-logística-1",
    "href": "pyTutorial8.html#regressão-logística-1",
    "title": "Tutorial 8 - Métodos de Classificação",
    "section": "Regressão Logística",
    "text": "Regressão Logística\n\nfrom sklearn.linear_model import LogisticRegression\n\nPara modelos com poucas observações usar solver='liblinear', já para modelos com datasets grandes, usar solver='saga'.\n\nmodelo_logreg = LogisticRegression(\n    solver='liblinear', \n    n_jobs=-1,\n    random_state=42,\n    max_iter=5000 \n)\n\n\n# Medir o tempo de treinamento\n\n#start_time = time.time()\nmodelo_logreg.fit(X_train, y_train)\n#end_time = time.time()\n#training_time = end_time - start_time\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning:\n\n'n_jobs' &gt; 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 10.\n\n\n\nLogisticRegression(max_iter=5000, n_jobs=-1, random_state=42,\n                   solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=5000, n_jobs=-1, random_state=42,\n                   solver='liblinear')\n\n\nFazer Previsões com os Dados de Teste:\n\ny_pred_logreg = modelo_logreg.predict(X_test)\n\n\nAvaliação do Modelo\nA Acurácia é o indicador mais direto do desempenho do modelo. Ela mede o percentual de previsões corretas:\n\\[\nA=\\frac{TP+TN}{TP+FP+FN+TN}\n\\]\nAgora vamos calcular alguns indicadores de acurácia.\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n\nacuracia_logreg = accuracy_score(y_test, y_pred_logreg)\nprint(f\"\\nAcurácia do modelo: {acuracia_logreg:.4f}\")\n\n\nAcurácia do modelo: 0.9430\n\n\nOutro indicador da acurácia diz respeito às vezes que o modelo acertou, para isto é necessário calcular a Matriz de Confusão.\n\n\n\nMatriz de Confusão. Fonte: van Otten (2024)\n\n\n\ncm_log_reg = confusion_matrix(y_test, y_pred_logreg)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(\n    cm_log_reg, \n    annot=True,\n    fmt='d', \n    cmap='Reds',\n    xticklabels=['Classe 0', 'Classe 1'],\n    yticklabels=['Classe 0', 'Classe 1']\n)\nplt.title('Matriz de Confusão', fontsize=16)\nplt.ylabel('Rótulo Verdadeiro', fontsize=12)\nplt.xlabel('Rótulo Previsto', fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\nO próximo indicador é o AUC (área sob a curva) que é calculada a partir da ROC (curva característica do processo). Imagine que você está avaliando um médico.\n\n\n\nCurva ROC. Fonte: Gong (2022)\n\n\nA Curva ROC é um gráfico que mostra o quão bom o médico (seu modelo) é em fazer diagnósticos. Ela mostra o equilíbrio entre duas coisas:\n\nAcertar os doentes (Eixo Y): A taxa de “Verdadeiros Positivos”. O ideal é que seja alta.\nErrar com os saudáveis (Eixo X): A taxa de “Falsos Positivos”, ou seja, quantas vezes ele diagnostica a doença em quem não a tem. O ideal é que seja baixa.\n\nO objetivo é ter uma curva que “suba rápido” e fique o mais próximo possível do canto superior esquerdo, o que significa acertar muitos doentes sem errar muito com os saudáveis.\nA AUC (Área Sob a Curva) é a nota final que o médico recebe, resumindo o gráfico inteiro em um único número.\n\n\\(AUC = 1.0\\): Um médico perfeito. Nunca erra.\n\\(AUC &gt; 0.8\\): Um médico excelente, muito confiável.\n\\(AUC = 0.5\\): Um médico inútil. O desempenho dele é igual a jogar uma moeda para decidir.\n\nEm resumo: A Curva ROC é o relatório de desempenho completo, e a AUC é a nota final que diz, de 0.5 a 1.0, o quão bom seu modelo é em separar as classes.\nPara obter o valor de AUC, é necessário calcular as probabilidades:\n\ny_pred_logreg_proba = modelo_logreg.predict_proba(X_test)[:, 1]\n\n\nauc_score_logreg = roc_auc_score(y_test, y_pred_logreg_proba)\nprint(f\"Pontuação ROC AUC: {auc_score_logreg:.4f}\")\n\nPontuação ROC AUC: 0.9635\n\n\nCalcular a curva ROC:\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_logreg_proba)\n\nplt.figure(figsize=(8, 4))\n# Plotar a curva do nosso modelo\nplt.plot(fpr, tpr, color=colors[0], lw=2, label=f'Curva ROC (AUC = {auc_score_logreg:.4f})')\n# Plotar a linha de referência de um classificador aleatório (AUC = 0.5)\nplt.plot([0, 1], [0, 1], color=colors[1], lw=2, linestyle='--', label='Classificador Aleatório')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Taxa de Falsos Positivos (FPR)', fontsize=12)\nplt.ylabel('Taxa de Verdadeiros Positivos (TPR)', fontsize=12)\nplt.title('Curva ROC (Receiver Operating Characteristic)', fontsize=16)\nplt.legend(loc=\"lower right\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "pyTutorial8.html#árvore-de-decisão-1",
    "href": "pyTutorial8.html#árvore-de-decisão-1",
    "title": "Tutorial 8 - Métodos de Classificação",
    "section": "Árvore de Decisão",
    "text": "Árvore de Decisão\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\nVamos treinar o modelo:\n\nmodelo_tree = DecisionTreeClassifier(\n    max_depth=6,\n    class_weight='balanced',\n    random_state=2025\n)\n\nmodelo_tree.fit(X_train, y_train)\n\nDecisionTreeClassifier(class_weight='balanced', max_depth=6, random_state=2025)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(class_weight='balanced', max_depth=6, random_state=2025)\n\n\nFazemos as previsões:\n\ny_pred_tree = modelo_tree.predict(X_test)\ny_pred_tree_proba = modelo_tree.predict_proba(X_test)[:, 1] #classe com fraude\n\n\nAvaliação do Modelo\n\nacuracia_tree = accuracy_score(y_test, y_pred_tree)\nprint(f\"\\nAcurácia do modelo: {acuracia_tree:.4f}\")\n\n\nAcurácia do modelo: 0.9953\n\n\n\nauc_score_tree = roc_auc_score(y_test, y_pred_tree_proba)\nprint(f\"Pontuação ROC AUC: {auc_score_tree:.4f}\")\n\nPontuação ROC AUC: 0.9911\n\n\nE a curva ROC:\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_tree_proba)\n\nplt.figure(figsize=(8, 8))\nplt.plot(fpr, tpr, color=colors[0], lw=2, label=f'Curva ROC (AUC = {auc_score_tree:.4f})')\nplt.plot([0, 1], [0, 1], color=colors[1], lw=2, linestyle='--', label='Classificador Aleatório')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Taxa de Falsos Positivos (FPR)', fontsize=12)\nplt.ylabel('Taxa de Verdadeiros Positivos (TPR)', fontsize=12)\nplt.title('Curva ROC (Receiver Operating Characteristic)', fontsize=16)\nplt.legend(loc=\"lower right\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "pyTutorial8.html#random-forest-1",
    "href": "pyTutorial8.html#random-forest-1",
    "title": "Tutorial 8 - Métodos de Classificação",
    "section": "Random Forest",
    "text": "Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nconstruimos o modelo:\n\nmodelo_rf = RandomForestClassifier(\n    n_estimators=100,\n    class_weight='balanced',\n    random_state=42,\n    n_jobs=-1)\n\ntreinamos o modelo:\n\nmodelo_rf.fit(X_train, y_train)\n\nRandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=42)\n\n\nconstruimos as previsões:\n\ny_pred_rf = modelo_rf.predict(X_test)\ny_pred_rf_proba = modelo_rf.predict_proba(X_test)[:, 1]\n\n\nAvaliação do Modelo\n\nacuracia_rf = accuracy_score(y_test, y_pred_rf)\nprint(f\"\\nAcurácia do modelo: {acuracia_rf:.4f}\")\n\n\nAcurácia do modelo: 0.9970\n\n\n\nauc_score_rf = roc_auc_score(y_test, y_pred_rf_proba)\nprint(f\"Pontuação ROC AUC: {auc_score_rf:.8f}\")\n\nPontuação ROC AUC: 0.99959412\n\n\nMatriz de confusão:\n\ncm_rf = confusion_matrix(y_test, y_pred_rf)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(\n    cm_rf, \n    annot=True,\n    fmt='d', \n    cmap='Reds',\n    xticklabels=['Classe 0', 'Classe 1'],\n    yticklabels=['Classe 0', 'Classe 1']\n)\nplt.title('Matriz de Confusão', fontsize=16)\nplt.ylabel('Rótulo Verdadeiro', fontsize=12)\nplt.xlabel('Rótulo Previsto', fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\nE a curva ROC:\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_rf_proba)\n\nplt.figure(figsize=(8, 8))\nplt.plot(fpr, tpr, color=colors[0], lw=2, label=f'Curva ROC (AUC = {auc_score_rf:.8f})')\nplt.plot([0, 1], [0, 1], color=colors[1], lw=2, linestyle='--', label='Classificador Aleatório')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Taxa de Falsos Positivos (FPR)', fontsize=12)\nplt.ylabel('Taxa de Verdadeiros Positivos (TPR)', fontsize=12)\nplt.title('Curva ROC (Receiver Operating Characteristic)', fontsize=16)\nplt.legend(loc=\"lower right\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "pyTutorial8.html#comparando-os-modelos",
    "href": "pyTutorial8.html#comparando-os-modelos",
    "title": "Tutorial 8 - Métodos de Classificação",
    "section": "Comparando os modelos",
    "text": "Comparando os modelos\n\nresults_data = {\n    'Modelo': ['Regressão Logística', 'Árvore de Decisão', 'Random Forest'],\n    'Acurácia': [acuracia_logreg, acuracia_tree, acuracia_rf],\n    'AUC': [auc_score_logreg, auc_score_tree, auc_score_rf]\n}\n\nresultados_df = pd.DataFrame(results_data)\n\n\nresultados_df\n\n\n\n\n\n\n\n\nModelo\nAcurácia\nAUC\n\n\n\n\n0\nRegressão Logística\n0.943000\n0.963492\n\n\n1\nÁrvore de Decisão\n0.995333\n0.991128\n\n\n2\nRandom Forest\n0.997000\n0.999594"
  },
  {
    "objectID": "Tutorial0.html",
    "href": "Tutorial0.html",
    "title": "Tutorial 0 - Conhecendo o R",
    "section": "",
    "text": "Conhecendo o R\nO R é uma linguagem de programação muito poderosa e amplamente utilizada em data science.\nPara se ter uma noção do que é possível fazer com o R e do que vocês irão realizar em algumas semanas acesse um exemplo de dashboard neste link.\n\n\nRStudio, Positron e Datacamp Datalab\nO RStudio é o software IDE (Interface Development Environment) mais conhecido e popular para trabalhar com o R. Tem uma interface informativa e uma simbiose muito forte com o R. Iremos usar o RStudio localmente nos nossos computadores.\nAlternativamente, iremos usar o Positron, uma nova IDE da mesma empresa desenvolvedora do RStudio.\nJá o Datacamp Datalab é um software em nuvem que está gratuitamente disponível para nós por sermos instituição educacional. A interface do Datacamp Datalab é mais simples e mais clean. Iremos usar o Datacamp Datalab na nuvem.\n\n\nDatacamp\nAntes de prosseguir, confira que seu cadastro no Datacamp está ok. Caso não o tenha feito ainda, aproveite este momento para fazê-lo, lembre de cadastrar seu usuário com o email \\(@grad.ufsc.br\\) e use o link disponível no Moodle para facilitar o cadastro.\n\n\nInstalando pacotes no RStudio/Positron\nEm paralelo, vamos instalar localmente os pacotes que vamos usar ao longo do semestre no RStudio/Positron. No Datacamp Datalab não há necessidade pois os pacotes já vem pré-configurados.\nUma vez baixado e instalado, abra um novo arquivo .qmd no RStudio/Positron, copie e cole o código a seguir e rode-o:\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"flexdashboard\")\ninstall.packages(\"forecast\")\ninstall.packages(\"highcharter\")\ninstall.packages(\"treemap\")\ninstall.packages(\"viridisLite\")\ninstall.packages(\"prettydoc\")\ninstall.packages(\"openxlsx\")\ninstall.packages(\"ggrepel\")\ninstall.packages(\"bookdown\")\ninstall.packages(\"tufte\")\ninstall.packages(\"devtools\")\ninstall.packages(\"ranger\")\ninstall.packages(\"tidymodels\")\ninstall.packages(\"leaflet\")\ninstall.packages(\"DT\")\ninstall.packages(\"kableExtra\")\ninstall.packages(\"plotly\")\ninstall.packages(\"titanic\")\ninstall.packages(\"broom\")\ninstall.packages(\"pROC\")\ninstall.packages(\"WVPlots\")\ninstall.packages(\"rpart\")\ninstall.packages(\"rpart.plot\")\ninstall.packages(\"vtreat\")\ninstall.packages(\"cluster\")\ninstall.packages(\"shiny\")\ninstall.packages(\"skimr\")\ninstall.packages(\"caret\")\ninstall.packages(\"GGally\")\ninstall.packages(\"vip\")\ninstall.packages(\"remotes\")\ninstall.packages(\"gt\")"
  },
  {
    "objectID": "Tutorial_av_ggplot.html",
    "href": "Tutorial_av_ggplot.html",
    "title": "Tutorial 3a - Ggplot Avançado",
    "section": "",
    "text": "Com ggplot podemos fazer mapas para descrever informações geográficas e visualizar sua distribuição espacial.\n\n\nVamos utilizar o pacote sf e o pacote geobr.\n\n#install.packages(\"sf\")\n#install.packages(\"geobr\")\n\n\nlibrary(geobr)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(knitr)\nlibrary(ggrepel)\nlibrary(patchwork)\n\nO pacote geobr serve para fazer download de mapas do BR, mais informações aqui.\nVamos mostrar como baixar os dados do Brasil:\n\nestados &lt;- read_state(\n  year = 2020\n)\n\nUsing year 2020\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\nhead(estados)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -73.99045 ymin: -13.6937 xmax: -46.06142 ymax: 5.271841\nGeodetic CRS:  SIRGAS 2000\n  code_state abbrev_state name_state code_region name_region\n1         11           RO   Rondônia           1       Norte\n2         12           AC       Acre           1       Norte\n3         13           AM   Amazônas           1       Norte\n4         14           RR    Roraima           1       Norte\n5         15           PA       Pará           1       Norte\n6         16           AP      Amapá           1       Norte\n                            geom\n1 MULTIPOLYGON (((-65.3815 -1...\n2 MULTIPOLYGON (((-71.07772 -...\n3 MULTIPOLYGON (((-69.83766 -...\n4 MULTIPOLYGON (((-63.96008 2...\n5 MULTIPOLYGON (((-51.43248 -...\n6 MULTIPOLYGON (((-50.45011 2...\n\n\nVamos fazer um gráfico do mapa:\n\nestados %&gt;% \n  ggplot()+\n  geom_sf()\n\n\n\n\n\n\n\n\nPodemos colorir o mapa com uma cor pré-definida:\n\nestados %&gt;% \n  ggplot()+\n  geom_sf(fill = \"#e32d91\",\n          color = \"white\")+\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nVamos fazer o mapa de SC com todos seus municípios.\n\nmunicipiosSC &lt;- read_municipality(\n  code_muni = \"SC\",\n  year = 2020,\n  simplified = TRUE #esta opção em FALSE traz uma resolução maior\n)\n\nUsing year/date 2020\n\n\n\nDownloading: 2 kB     \nDownloading: 2 kB     \nDownloading: 2 kB     \nDownloading: 2 kB     \nDownloading: 10 kB     \nDownloading: 10 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 67 kB     \nDownloading: 67 kB     \nDownloading: 99 kB     \nDownloading: 99 kB     \nDownloading: 99 kB     \nDownloading: 99 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 140 kB     \nDownloading: 140 kB     \nDownloading: 140 kB     \nDownloading: 140 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 160 kB     \nDownloading: 160 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 210 kB     \nDownloading: 210 kB     \nDownloading: 220 kB     \nDownloading: 220 kB     \nDownloading: 230 kB     \nDownloading: 230 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 280 kB     \nDownloading: 280 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 300 kB     \nDownloading: 300 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 360 kB     \nDownloading: 360 kB     \nDownloading: 360 kB     \nDownloading: 360 kB     \nDownloading: 370 kB     \nDownloading: 370 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 400 kB     \nDownloading: 400 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 420 kB     \nDownloading: 420 kB     \nDownloading: 420 kB     \nDownloading: 420 kB     \nDownloading: 430 kB     \nDownloading: 430 kB     \nDownloading: 430 kB     \nDownloading: 430 kB     \nDownloading: 440 kB     \nDownloading: 440 kB     \nDownloading: 440 kB     \nDownloading: 440 kB     \nDownloading: 450 kB     \nDownloading: 450 kB     \nDownloading: 460 kB     \nDownloading: 460 kB     \nDownloading: 460 kB     \nDownloading: 460 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 490 kB     \nDownloading: 490 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 520 kB     \nDownloading: 520 kB     \nDownloading: 520 kB     \nDownloading: 520 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 550 kB     \nDownloading: 550 kB     \nDownloading: 550 kB     \nDownloading: 550 kB     \nDownloading: 580 kB     \nDownloading: 580 kB     \nDownloading: 580 kB     \nDownloading: 580 kB     \nDownloading: 620 kB     \nDownloading: 620 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 690 kB     \nDownloading: 690 kB     \nDownloading: 690 kB     \nDownloading: 690 kB     \nDownloading: 700 kB     \nDownloading: 700 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 720 kB     \nDownloading: 720 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \n\n\n\nmunicipiosSC %&gt;% \n  ggplot()+\n  geom_sf(fill = \"#2d3e50\",\n          color = \"white\",\n          linewidth = 0.03)+\n  theme_minimal()+\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nTambém podemos colorir o mapa utilizando uma 3a variável, que devemos anexar ao objeto municipiosSC ou estados.\nComo exemplo, vamos pegar dados da expectativa de vida para os estados brasileiros:\n\nvida &lt;- utils::read.csv(system.file(\"extdata/br_states_lifexpect2017.csv\", package = \"geobr\"), encoding = \"UTF-8\")\n\nVamos unir as informações dos dois dataframes.\n\nminexp = min(vida$ESPVIDA2017)\nmaxexp = max(vida$ESPVIDA2017)\n\nestados %&gt;% \n  left_join(vida, by=c(\"name_state\"=\"uf\")) %&gt;% \n  ggplot()+\n  geom_sf(aes(fill=ESPVIDA2017), color = \"white\", linewidth = 0.1)+\n  scale_fill_distiller(palette = \"Reds\", name = \"%\", limits = c(minexp,maxexp))+\n  theme_void()+\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nInstalar os pacotes rnaturalearthhires e rnaturalearth:\n\nlibrary(devtools)\n\nLoading required package: usethis\n\n#devtools::install_github(\"ropensci/rnaturalearthhires\")\n#install.packages(\"rnaturalearth\")\n#install.packages(\"rnaturalearthdata\")\nlibrary(rnaturalearth)\n\nPara obter o mapa de um país, por exemplo India:\n\nmap1 &lt;- ne_countries(type = \"countries\", country = \"India\",\n                     scale = \"medium\", returnclass = \"sf\")\n\n\nmap1 %&gt;% \n  ggplot()+\n  geom_sf()\n\n\n\n\n\n\n\n\nPara obter o mapa dos Estados de um determinado país:\n\nmap2 &lt;- ne_states(\"India\", returnclass = \"sf\")\n\n\nmap2 %&gt;% \n  ggplot()+\n  geom_sf()\n\n\n\n\n\n\n\n\n\n\n\nVamos importar dados de energia solar fotovoltaica na India.\n\n\nVocê pode baixar os dados aqui.\n\n\n\nindia &lt;- read_xlsx(\"datamap/importIN.xlsx\")\n\nVamos fazer uma breve análise exploratória:\n\nglimpse(india) %&gt;% \n  kable()\n\nRows: 38\nColumns: 15\n$ Year   &lt;chr&gt; \"Andhra Pradesh\", \"Arunachal Pradesh\", \"Assam\", \"Bihar\", \"Chhat…\n$ `2010` &lt;dbl&gt; 0.10, 0.03, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.0…\n$ `2011` &lt;dbl&gt; 2.10, 0.03, 0.00, 0.00, 0.00, 0.00, 5.00, 0.00, 0.00, 0.00, 0.0…\n$ `2012` &lt;dbl&gt; 21.75, 0.03, 0.00, 0.00, 4.00, 0.00, 604.89, 16.80, 0.00, 0.00,…\n$ `2013` &lt;dbl&gt; 23.35, 0.03, 0.00, 0.00, 4.00, 0.00, 857.90, 7.80, 0.00, 0.00, …\n$ `2014` &lt;dbl&gt; 131.84, 0.03, 0.00, 0.00, 7.10, 0.00, 916.40, 10.30, 0.00, 0.00…\n$ `2015` &lt;dbl&gt; 242.86, 0.03, 0.00, 0.00, 7.60, 0.00, 1000.05, 12.80, 0.00, 0.0…\n$ `2016` &lt;dbl&gt; 572.96, 0.27, 0.00, 5.10, 93.58, 0.00, 1119.17, 15.39, 0.20, 1.…\n$ `2017` &lt;dbl&gt; 1867.23, 0.27, 11.78, 108.52, 128.86, 0.71, 1249.37, 81.40, 0.7…\n$ `2018` &lt;dbl&gt; 2195.46, 5.39, 12.45, 142.45, 231.35, 0.91, 1588.00, 216.85, 0.…\n$ `2019` &lt;dbl&gt; 3085.68, 5.39, 22.40, 142.45, 231.35, 3.89, 2440.13, 224.52, 22…\n$ `2020` &lt;dbl&gt; 3610.02, 5.61, 41.23, 151.57, 231.35, 4.78, 2948.37, 252.14, 32…\n$ `2021` &lt;dbl&gt; 4203.00, 5.61, 42.99, 159.51, 252.48, 7.44, 4430.82, 407.83, 42…\n$ `2022` &lt;dbl&gt; 4386.76, 11.23, 117.94, 190.63, 518.08, 19.95, 7180.03, 910.63,…\n$ `2023` &lt;dbl&gt; 4534.19, 11.64, 147.92, 192.89, 948.82, 26.49, 9254.57, 1029.16…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n\n\n\n\nAndhra Pradesh\n0.10\n2.10\n21.75\n23.35\n131.84\n242.86\n572.96\n1867.23\n2195.46\n3085.68\n3610.02\n4203.00\n4386.76\n4534.19\n\n\nArunachal Pradesh\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.27\n0.27\n5.39\n5.39\n5.61\n5.61\n11.23\n11.64\n\n\nAssam\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n11.78\n12.45\n22.40\n41.23\n42.99\n117.94\n147.92\n\n\nBihar\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n5.10\n108.52\n142.45\n142.45\n151.57\n159.51\n190.63\n192.89\n\n\nChhattisgarh\n0.00\n0.00\n4.00\n4.00\n7.10\n7.60\n93.58\n128.86\n231.35\n231.35\n231.35\n252.48\n518.08\n948.82\n\n\nGoa\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.71\n0.91\n3.89\n4.78\n7.44\n19.95\n26.49\n\n\nGujarat\n0.00\n5.00\n604.89\n857.90\n916.40\n1000.05\n1119.17\n1249.37\n1588.00\n2440.13\n2948.37\n4430.82\n7180.03\n9254.57\n\n\nHaryana\n0.00\n0.00\n16.80\n7.80\n10.30\n12.80\n15.39\n81.40\n216.85\n224.52\n252.14\n407.83\n910.63\n1029.16\n\n\nHimachal Pradesh\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.20\n0.73\n0.73\n22.68\n32.93\n42.73\n76.16\n87.49\n\n\nJammu & Kashmir\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n1.36\n1.36\n14.83\n19.30\n20.73\n54.73\n49.44\n\n\nJharkhand\n0.00\n0.00\n4.00\n16.00\n16.00\n16.00\n16.19\n23.27\n25.67\n34.95\n38.40\n52.06\n88.79\n105.84\n\n\nKarnataka\n6.00\n6.00\n9.00\n14.00\n31.00\n77.22\n145.46\n1027.84\n4944.12\n6095.55\n7277.93\n7355.17\n7590.81\n8241.41\n\n\nKerala\n0.03\n0.03\n0.84\n0.03\n0.03\n0.03\n13.04\n74.20\n107.94\n138.59\n142.23\n257.00\n363.18\n761.44\n\n\nLadakh\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n7.80\n\n\nMadhya Pradesh\n0.10\n0.10\n2.10\n37.32\n347.17\n558.58\n776.37\n857.04\n1305.35\n1840.16\n2258.46\n2463.22\n2717.95\n2802.14\n\n\nMaharashtra\n0.00\n4.00\n20.00\n100.00\n249.25\n360.75\n385.76\n452.37\n1239.18\n1633.54\n1801.80\n2289.97\n2631.02\n4722.90\n\n\nManipur\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.03\n0.06\n3.44\n5.16\n6.36\n12.25\n12.28\n\n\nMeghalaya\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.02\n0.12\n0.12\n0.12\n4.15\n4.15\n\n\nMizoram\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.50\n1.52\n1.53\n7.90\n28.01\n\n\nNagaland\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.50\n1.00\n1.00\n1.00\n1.00\n3.04\n3.04\n\n\nOdisha\n0.00\n0.00\n13.00\n13.00\n30.50\n31.76\n66.92\n79.42\n79.57\n394.73\n397.84\n401.72\n451.24\n453.17\n\n\nPunjab\n1.33\n2.33\n9.33\n9.33\n16.85\n185.27\n405.06\n793.95\n905.62\n905.62\n947.10\n959.50\n1100.07\n1167.26\n\n\nRajasthan\n0.15\n5.15\n197.65\n552.90\n730.10\n942.10\n1269.93\n1812.93\n2332.77\n3226.79\n5137.91\n5732.58\n12564.87\n17055.70\n\n\nSikkim\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.07\n0.07\n4.68\n4.68\n\n\nTamil Nadu\n0.05\n5.05\n15.05\n17.11\n98.36\n142.58\n1061.82\n1691.83\n1908.57\n2575.22\n3915.88\n4475.21\n5067.18\n6736.43\n\n\nTelangana\n0.00\n0.00\n0.00\n0.00\n0.00\n61.25\n527.84\n1286.98\n3291.25\n3592.09\n3620.75\n3953.12\n4520.48\n4666.03\n\n\nTripura\n0.00\n0.00\n0.00\n0.00\n0.00\n5.00\n5.00\n5.09\n5.09\n5.09\n9.41\n9.41\n14.89\n17.60\n\n\nUttar Pradesh\n0.38\n0.38\n12.38\n17.38\n21.08\n71.26\n143.50\n336.73\n694.41\n960.10\n1095.10\n1712.50\n2244.43\n2515.22\n\n\nUttarakhand\n0.05\n0.05\n5.05\n5.05\n5.05\n5.00\n41.15\n233.49\n260.08\n306.75\n315.90\n368.41\n573.54\n575.53\n\n\nWest Bengal\n1.15\n1.15\n2.05\n2.05\n7.05\n7.21\n7.77\n26.14\n37.32\n75.95\n114.46\n149.84\n166.00\n179.98\n\n\nAndaman & Nicobar\n0.10\n0.10\n0.01\n5.10\n5.10\n5.10\n5.10\n6.56\n6.56\n11.73\n12.19\n29.22\n29.49\n29.91\n\n\nChandigarh\n0.00\n0.00\n0.00\n0.00\n2.00\n4.50\n6.81\n17.32\n25.20\n34.71\n40.55\n45.16\n55.17\n58.69\n\n\nDadar & Nagar Haveli\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.97\n5.46\n5.46\n5.46\n5.46\n5.46\n5.46\n\n\nDaman & Diu\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n4.00\n10.46\n3.86\n14.47\n19.86\n40.55\n40.72\n41.01\n\n\nDelhi\n0.05\n2.14\n2.53\n2.56\n5.15\n5.47\n14.28\n40.27\n69.57\n126.89\n165.16\n192.97\n211.12\n218.26\n\n\nLakshadweep\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n0.71\n0.75\n0.75\n0.75\n0.75\n3.27\n3.27\n\n\nPuducherry\n0.03\n0.79\n0.03\n0.03\n0.03\n0.03\n0.02\n0.08\n0.16\n3.14\n5.51\n9.33\n13.69\n35.53\n\n\nOthers\n0.00\n0.00\n0.00\n0.79\n0.82\n0.79\n58.31\n58.31\n0.00\n0.00\n0.00\n0.00\n45.01\n45.01\n\n\n\n\nskimr::skim(india)\n\n\nData summary\n\n\nName\nindia\n\n\nNumber of rows\n38\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nYear\n0\n1\n3\n20\n0\n38\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\n2010\n0\n1\n0.27\n1.00\n0.00\n0.00\n0.00\n0.05\n6.00\n▇▁▁▁▁\n\n\n2011\n0\n1\n0.92\n1.75\n0.00\n0.00\n0.00\n0.78\n6.00\n▇▁▁▁▁\n\n\n2012\n0\n1\n24.77\n101.82\n0.00\n0.00\n0.03\n8.01\n604.89\n▇▁▁▁▁\n\n\n2013\n0\n1\n44.38\n162.77\n0.00\n0.00\n0.39\n12.08\n857.90\n▇▁▁▁▁\n\n\n2014\n0\n1\n69.26\n194.76\n0.00\n0.00\n0.78\n16.64\n916.40\n▇▁▁▁▁\n\n\n2015\n0\n1\n98.53\n237.73\n0.00\n0.00\n4.75\n53.88\n1000.05\n▇▁▁▁▁\n\n\n2016\n0\n1\n177.97\n343.78\n0.00\n0.12\n7.29\n131.02\n1269.93\n▇▁▁▁▁\n\n\n2017\n0\n1\n323.39\n562.96\n0.00\n0.72\n24.70\n310.92\n1867.23\n▇▁▁▁▁\n\n\n2018\n0\n1\n569.60\n1092.11\n0.00\n0.93\n25.44\n585.83\n4944.12\n▇▁▁▁▁\n\n\n2019\n0\n1\n741.60\n1374.44\n0.00\n4.19\n34.83\n777.90\n6095.55\n▇▁▁▁▁\n\n\n2020\n0\n1\n911.26\n1715.51\n0.00\n5.47\n40.89\n809.79\n7277.93\n▇▁▁▁▁\n\n\n2021\n0\n1\n1054.88\n1899.38\n0.00\n6.63\n48.61\n821.58\n7355.17\n▇▁▁▁▁\n\n\n2022\n0\n1\n1420.96\n2741.79\n0.00\n13.99\n103.37\n1052.71\n12564.87\n▇▁▁▁▁\n\n\n2023\n0\n1\n1757.38\n3510.26\n3.04\n26.87\n126.88\n1132.73\n17055.70\n▇▁▁▁▁\n\n\n\n\n\nPrecisamos converter as colunas character para numeric:\n\nindia &lt;- india %&gt;% \n  mutate(across(2:15, as.numeric)) %&gt;% \n  rename(name = Year)\n\nindia %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n\n\n\n\nAndhra Pradesh\n0.10\n2.10\n21.75\n23.35\n131.84\n242.86\n572.96\n1867.23\n2195.46\n3085.68\n3610.02\n4203.00\n4386.76\n4534.19\n\n\nArunachal Pradesh\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.27\n0.27\n5.39\n5.39\n5.61\n5.61\n11.23\n11.64\n\n\nAssam\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n11.78\n12.45\n22.40\n41.23\n42.99\n117.94\n147.92\n\n\nBihar\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n5.10\n108.52\n142.45\n142.45\n151.57\n159.51\n190.63\n192.89\n\n\nChhattisgarh\n0.00\n0.00\n4.00\n4.00\n7.10\n7.60\n93.58\n128.86\n231.35\n231.35\n231.35\n252.48\n518.08\n948.82\n\n\nGoa\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.71\n0.91\n3.89\n4.78\n7.44\n19.95\n26.49\n\n\nGujarat\n0.00\n5.00\n604.89\n857.90\n916.40\n1000.05\n1119.17\n1249.37\n1588.00\n2440.13\n2948.37\n4430.82\n7180.03\n9254.57\n\n\nHaryana\n0.00\n0.00\n16.80\n7.80\n10.30\n12.80\n15.39\n81.40\n216.85\n224.52\n252.14\n407.83\n910.63\n1029.16\n\n\nHimachal Pradesh\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.20\n0.73\n0.73\n22.68\n32.93\n42.73\n76.16\n87.49\n\n\nJammu & Kashmir\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n1.36\n1.36\n14.83\n19.30\n20.73\n54.73\n49.44\n\n\nJharkhand\n0.00\n0.00\n4.00\n16.00\n16.00\n16.00\n16.19\n23.27\n25.67\n34.95\n38.40\n52.06\n88.79\n105.84\n\n\nKarnataka\n6.00\n6.00\n9.00\n14.00\n31.00\n77.22\n145.46\n1027.84\n4944.12\n6095.55\n7277.93\n7355.17\n7590.81\n8241.41\n\n\nKerala\n0.03\n0.03\n0.84\n0.03\n0.03\n0.03\n13.04\n74.20\n107.94\n138.59\n142.23\n257.00\n363.18\n761.44\n\n\nLadakh\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n7.80\n\n\nMadhya Pradesh\n0.10\n0.10\n2.10\n37.32\n347.17\n558.58\n776.37\n857.04\n1305.35\n1840.16\n2258.46\n2463.22\n2717.95\n2802.14\n\n\nMaharashtra\n0.00\n4.00\n20.00\n100.00\n249.25\n360.75\n385.76\n452.37\n1239.18\n1633.54\n1801.80\n2289.97\n2631.02\n4722.90\n\n\nManipur\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.03\n0.06\n3.44\n5.16\n6.36\n12.25\n12.28\n\n\nMeghalaya\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.02\n0.12\n0.12\n0.12\n4.15\n4.15\n\n\nMizoram\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.50\n1.52\n1.53\n7.90\n28.01\n\n\nNagaland\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.50\n1.00\n1.00\n1.00\n1.00\n3.04\n3.04\n\n\nOdisha\n0.00\n0.00\n13.00\n13.00\n30.50\n31.76\n66.92\n79.42\n79.57\n394.73\n397.84\n401.72\n451.24\n453.17\n\n\nPunjab\n1.33\n2.33\n9.33\n9.33\n16.85\n185.27\n405.06\n793.95\n905.62\n905.62\n947.10\n959.50\n1100.07\n1167.26\n\n\nRajasthan\n0.15\n5.15\n197.65\n552.90\n730.10\n942.10\n1269.93\n1812.93\n2332.77\n3226.79\n5137.91\n5732.58\n12564.87\n17055.70\n\n\nSikkim\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.07\n0.07\n4.68\n4.68\n\n\nTamil Nadu\n0.05\n5.05\n15.05\n17.11\n98.36\n142.58\n1061.82\n1691.83\n1908.57\n2575.22\n3915.88\n4475.21\n5067.18\n6736.43\n\n\nTelangana\n0.00\n0.00\n0.00\n0.00\n0.00\n61.25\n527.84\n1286.98\n3291.25\n3592.09\n3620.75\n3953.12\n4520.48\n4666.03\n\n\nTripura\n0.00\n0.00\n0.00\n0.00\n0.00\n5.00\n5.00\n5.09\n5.09\n5.09\n9.41\n9.41\n14.89\n17.60\n\n\nUttar Pradesh\n0.38\n0.38\n12.38\n17.38\n21.08\n71.26\n143.50\n336.73\n694.41\n960.10\n1095.10\n1712.50\n2244.43\n2515.22\n\n\nUttarakhand\n0.05\n0.05\n5.05\n5.05\n5.05\n5.00\n41.15\n233.49\n260.08\n306.75\n315.90\n368.41\n573.54\n575.53\n\n\nWest Bengal\n1.15\n1.15\n2.05\n2.05\n7.05\n7.21\n7.77\n26.14\n37.32\n75.95\n114.46\n149.84\n166.00\n179.98\n\n\nAndaman & Nicobar\n0.10\n0.10\n0.01\n5.10\n5.10\n5.10\n5.10\n6.56\n6.56\n11.73\n12.19\n29.22\n29.49\n29.91\n\n\nChandigarh\n0.00\n0.00\n0.00\n0.00\n2.00\n4.50\n6.81\n17.32\n25.20\n34.71\n40.55\n45.16\n55.17\n58.69\n\n\nDadar & Nagar Haveli\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.97\n5.46\n5.46\n5.46\n5.46\n5.46\n5.46\n\n\nDaman & Diu\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n4.00\n10.46\n3.86\n14.47\n19.86\n40.55\n40.72\n41.01\n\n\nDelhi\n0.05\n2.14\n2.53\n2.56\n5.15\n5.47\n14.28\n40.27\n69.57\n126.89\n165.16\n192.97\n211.12\n218.26\n\n\nLakshadweep\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n0.71\n0.75\n0.75\n0.75\n0.75\n3.27\n3.27\n\n\nPuducherry\n0.03\n0.79\n0.03\n0.03\n0.03\n0.03\n0.02\n0.08\n0.16\n3.14\n5.51\n9.33\n13.69\n35.53\n\n\nOthers\n0.00\n0.00\n0.00\n0.79\n0.82\n0.79\n58.31\n58.31\n0.00\n0.00\n0.00\n0.00\n45.01\n45.01\n\n\n\n\n\nTratando os dados:\n\nindia %&gt;% \n  pivot_longer(2:15, names_to = \"year\", values_to = \"mw\") %&gt;% \n  mutate(year = as.numeric(year)) %&gt;% \n  ggplot(aes(x=year, y=mw, fill=name))+\n  geom_col()+\n  facet_wrap(vars(name),\n             scales = \"free_y\")+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nggsave(\"all.jpg\",\n       height = 10,\n       width = 12)\n\n\n\n\nminexp = min(india$`2023`)\nmaxexp = max(india$`2023`)\n\nmaxcities &lt;- india %&gt;% \n  select(name, `2015`) %&gt;% \n  filter(`2015` &gt; mean(`2015`))\n\nmaxcities_sf &lt;- maxcities %&gt;% \n  left_join(map2)\n\nJoining with `by = join_by(name)`\n\n\nMapa para 2015:\n\nindia2015 &lt;- india %&gt;%\n   select(1,`2015`) %&gt;% \n   rename(mw = `2015`) %&gt;% \n   left_join(map2) %&gt;% \n   ggplot()+\n   geom_sf(aes(fill=mw/1000, geometry = geometry), color = \"white\", linewidth = 0.1)+\n  geom_text_repel(data = maxcities_sf,\n                  aes(x=longitude,\n                      y=latitude,\n                      label = name),\n                  fontface = \"bold\", \n                  nudge_x = c(10, -5, 10, -5, -3, -5,5), \n                  nudge_y = c(0.25,-0.25, 6, -0.5, 2,1,1))+\n  scale_fill_distiller(palette = \"Reds\", \n                       name = \"GW\",\n                       limits = c(minexp/1000,maxexp/1000),\n                       direction = 1)+\n  theme_void()+\n  theme(panel.grid = element_blank())+\n  labs(title = \"Solar PV Installed Capacity in 2015\")\n\nJoining with `by = join_by(name)`\n\nindia2015\n\n\n\n\n\n\n\n\n\n\n\n\nmaxcities &lt;- india %&gt;% \n  select(name, `2023`) %&gt;% \n  filter(`2023` &gt; mean(`2023`))\n\nmaxcities_sf &lt;- maxcities %&gt;% \n  left_join(map2)\n\nJoining with `by = join_by(name)`\n\n\nMapa para 2023:\n\nindia2023 &lt;- india %&gt;%\n   select(1,`2023`) %&gt;% \n   rename(mw = `2023`) %&gt;% \n   left_join(map2) %&gt;% \n   ggplot()+\n   geom_sf(aes(fill=mw/1000, geometry = geometry), \n           color = \"white\", linewidth = 0.1)+\n  geom_text_repel(data = maxcities_sf,\n                  aes(x=longitude,\n                      y=latitude,\n                      label = name),\n                  fontface = \"bold\", \n                  nudge_x = c(10, -5, -5, 12, -5,-5,-5,5,-3), \n                  nudge_y = c(0.25,-0.25, 0.5, 8, -0.5,2,-2,2,5))+\n  scale_fill_distiller(palette = \"Reds\", \n                       name = \"GW\",\n                       limits = c(minexp/1000,maxexp/1000),\n                       direction = 1)+\n  theme_void()+\n  theme(panel.grid = element_blank())+\n  labs(title = \"Solar PV Installed Capacity in 2023\")\n\nJoining with `by = join_by(name)`\n\nindia2023\n\n\n\n\n\n\n\n\n\n\n\n\nindia_2015_2023 &lt;- india2015 + india2023 + plot_layout(ncol=1)\n\nindia_2015_2023\n\n\n\n\n\n\n\nggsave(\"india2015_2023.jpg\",\n       width = 7,\n       height = 10)\n\nCalculando o market share de cada estado:\n\ntotal &lt;- sum(india$`2023`)\nmaxcities_total &lt;- sum(maxcities$`2023`)\n\ncolnames(maxcities) &lt;- c(\"name\",\"mw\")\n\nother &lt;- data.frame(name = \"Other\",\n                    mw = total - maxcities_total)\n\nmaxcities &lt;- rbind(maxcities, other)\n\nmaxcities %&gt;% \n  mutate(market = round(100*mw/total,2),\n         mw_2030 = 300000*market/100)\n\n# A tibble: 10 × 4\n   name               mw market mw_2030\n   &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 Andhra Pradesh  4534.   6.79   20370\n 2 Gujarat         9255.  13.9    41580\n 3 Karnataka       8241.  12.3    37020\n 4 Madhya Pradesh  2802.   4.2    12600\n 5 Maharashtra     4723.   7.07   21210\n 6 Rajasthan      17056.  25.5    76620\n 7 Tamil Nadu      6736.  10.1    30270\n 8 Telangana       4666.   6.99   20970\n 9 Uttar Pradesh   2515.   3.77   11310\n10 Other           6252.   9.36   28080"
  },
  {
    "objectID": "Tutorial_av_ggplot.html#pacote-geobr-para-mapas-do-brasil",
    "href": "Tutorial_av_ggplot.html#pacote-geobr-para-mapas-do-brasil",
    "title": "Tutorial 3a - Ggplot Avançado",
    "section": "",
    "text": "Vamos utilizar o pacote sf e o pacote geobr.\n\n#install.packages(\"sf\")\n#install.packages(\"geobr\")\n\n\nlibrary(geobr)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(knitr)\nlibrary(ggrepel)\nlibrary(patchwork)\n\nO pacote geobr serve para fazer download de mapas do BR, mais informações aqui.\nVamos mostrar como baixar os dados do Brasil:\n\nestados &lt;- read_state(\n  year = 2020\n)\n\nUsing year 2020\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\nhead(estados)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -73.99045 ymin: -13.6937 xmax: -46.06142 ymax: 5.271841\nGeodetic CRS:  SIRGAS 2000\n  code_state abbrev_state name_state code_region name_region\n1         11           RO   Rondônia           1       Norte\n2         12           AC       Acre           1       Norte\n3         13           AM   Amazônas           1       Norte\n4         14           RR    Roraima           1       Norte\n5         15           PA       Pará           1       Norte\n6         16           AP      Amapá           1       Norte\n                            geom\n1 MULTIPOLYGON (((-65.3815 -1...\n2 MULTIPOLYGON (((-71.07772 -...\n3 MULTIPOLYGON (((-69.83766 -...\n4 MULTIPOLYGON (((-63.96008 2...\n5 MULTIPOLYGON (((-51.43248 -...\n6 MULTIPOLYGON (((-50.45011 2...\n\n\nVamos fazer um gráfico do mapa:\n\nestados %&gt;% \n  ggplot()+\n  geom_sf()\n\n\n\n\n\n\n\n\nPodemos colorir o mapa com uma cor pré-definida:\n\nestados %&gt;% \n  ggplot()+\n  geom_sf(fill = \"#e32d91\",\n          color = \"white\")+\n  theme_void()"
  },
  {
    "objectID": "Tutorial_av_ggplot.html#para-fazer-o-mapa-de-um-estado",
    "href": "Tutorial_av_ggplot.html#para-fazer-o-mapa-de-um-estado",
    "title": "Tutorial 3a - Ggplot Avançado",
    "section": "",
    "text": "Vamos fazer o mapa de SC com todos seus municípios.\n\nmunicipiosSC &lt;- read_municipality(\n  code_muni = \"SC\",\n  year = 2020,\n  simplified = TRUE #esta opção em FALSE traz uma resolução maior\n)\n\nUsing year/date 2020\n\n\n\nDownloading: 2 kB     \nDownloading: 2 kB     \nDownloading: 2 kB     \nDownloading: 2 kB     \nDownloading: 10 kB     \nDownloading: 10 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 34 kB     \nDownloading: 67 kB     \nDownloading: 67 kB     \nDownloading: 99 kB     \nDownloading: 99 kB     \nDownloading: 99 kB     \nDownloading: 99 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 130 kB     \nDownloading: 140 kB     \nDownloading: 140 kB     \nDownloading: 140 kB     \nDownloading: 140 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 150 kB     \nDownloading: 160 kB     \nDownloading: 160 kB     \nDownloading: 190 kB     \nDownloading: 190 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 200 kB     \nDownloading: 210 kB     \nDownloading: 210 kB     \nDownloading: 220 kB     \nDownloading: 220 kB     \nDownloading: 230 kB     \nDownloading: 230 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 240 kB     \nDownloading: 280 kB     \nDownloading: 280 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 290 kB     \nDownloading: 300 kB     \nDownloading: 300 kB     \nDownloading: 310 kB     \nDownloading: 310 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 320 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 330 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 340 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 350 kB     \nDownloading: 360 kB     \nDownloading: 360 kB     \nDownloading: 360 kB     \nDownloading: 360 kB     \nDownloading: 370 kB     \nDownloading: 370 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 380 kB     \nDownloading: 390 kB     \nDownloading: 390 kB     \nDownloading: 400 kB     \nDownloading: 400 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 410 kB     \nDownloading: 420 kB     \nDownloading: 420 kB     \nDownloading: 420 kB     \nDownloading: 420 kB     \nDownloading: 430 kB     \nDownloading: 430 kB     \nDownloading: 430 kB     \nDownloading: 430 kB     \nDownloading: 440 kB     \nDownloading: 440 kB     \nDownloading: 440 kB     \nDownloading: 440 kB     \nDownloading: 450 kB     \nDownloading: 450 kB     \nDownloading: 460 kB     \nDownloading: 460 kB     \nDownloading: 460 kB     \nDownloading: 460 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 470 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 480 kB     \nDownloading: 490 kB     \nDownloading: 490 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 500 kB     \nDownloading: 520 kB     \nDownloading: 520 kB     \nDownloading: 520 kB     \nDownloading: 520 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 530 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 540 kB     \nDownloading: 550 kB     \nDownloading: 550 kB     \nDownloading: 550 kB     \nDownloading: 550 kB     \nDownloading: 580 kB     \nDownloading: 580 kB     \nDownloading: 580 kB     \nDownloading: 580 kB     \nDownloading: 620 kB     \nDownloading: 620 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 640 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 650 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 660 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 670 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 680 kB     \nDownloading: 690 kB     \nDownloading: 690 kB     \nDownloading: 690 kB     \nDownloading: 690 kB     \nDownloading: 700 kB     \nDownloading: 700 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 710 kB     \nDownloading: 720 kB     \nDownloading: 720 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \nDownloading: 760 kB     \n\n\n\nmunicipiosSC %&gt;% \n  ggplot()+\n  geom_sf(fill = \"#2d3e50\",\n          color = \"white\",\n          linewidth = 0.03)+\n  theme_minimal()+\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nTambém podemos colorir o mapa utilizando uma 3a variável, que devemos anexar ao objeto municipiosSC ou estados.\nComo exemplo, vamos pegar dados da expectativa de vida para os estados brasileiros:\n\nvida &lt;- utils::read.csv(system.file(\"extdata/br_states_lifexpect2017.csv\", package = \"geobr\"), encoding = \"UTF-8\")\n\nVamos unir as informações dos dois dataframes.\n\nminexp = min(vida$ESPVIDA2017)\nmaxexp = max(vida$ESPVIDA2017)\n\nestados %&gt;% \n  left_join(vida, by=c(\"name_state\"=\"uf\")) %&gt;% \n  ggplot()+\n  geom_sf(aes(fill=ESPVIDA2017), color = \"white\", linewidth = 0.1)+\n  scale_fill_distiller(palette = \"Reds\", name = \"%\", limits = c(minexp,maxexp))+\n  theme_void()+\n  theme(panel.grid = element_blank())"
  },
  {
    "objectID": "Tutorial_av_ggplot.html#pacote-rnaturalearth-para-mapas-de-outros-países",
    "href": "Tutorial_av_ggplot.html#pacote-rnaturalearth-para-mapas-de-outros-países",
    "title": "Tutorial 3a - Ggplot Avançado",
    "section": "",
    "text": "Instalar os pacotes rnaturalearthhires e rnaturalearth:\n\nlibrary(devtools)\n\nLoading required package: usethis\n\n#devtools::install_github(\"ropensci/rnaturalearthhires\")\n#install.packages(\"rnaturalearth\")\n#install.packages(\"rnaturalearthdata\")\nlibrary(rnaturalearth)\n\nPara obter o mapa de um país, por exemplo India:\n\nmap1 &lt;- ne_countries(type = \"countries\", country = \"India\",\n                     scale = \"medium\", returnclass = \"sf\")\n\n\nmap1 %&gt;% \n  ggplot()+\n  geom_sf()\n\n\n\n\n\n\n\n\nPara obter o mapa dos Estados de um determinado país:\n\nmap2 &lt;- ne_states(\"India\", returnclass = \"sf\")\n\n\nmap2 %&gt;% \n  ggplot()+\n  geom_sf()"
  },
  {
    "objectID": "Tutorial_av_ggplot.html#exemplo-india",
    "href": "Tutorial_av_ggplot.html#exemplo-india",
    "title": "Tutorial 3a - Ggplot Avançado",
    "section": "",
    "text": "Vamos importar dados de energia solar fotovoltaica na India.\n\n\nVocê pode baixar os dados aqui.\n\n\n\nindia &lt;- read_xlsx(\"datamap/importIN.xlsx\")\n\nVamos fazer uma breve análise exploratória:\n\nglimpse(india) %&gt;% \n  kable()\n\nRows: 38\nColumns: 15\n$ Year   &lt;chr&gt; \"Andhra Pradesh\", \"Arunachal Pradesh\", \"Assam\", \"Bihar\", \"Chhat…\n$ `2010` &lt;dbl&gt; 0.10, 0.03, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.0…\n$ `2011` &lt;dbl&gt; 2.10, 0.03, 0.00, 0.00, 0.00, 0.00, 5.00, 0.00, 0.00, 0.00, 0.0…\n$ `2012` &lt;dbl&gt; 21.75, 0.03, 0.00, 0.00, 4.00, 0.00, 604.89, 16.80, 0.00, 0.00,…\n$ `2013` &lt;dbl&gt; 23.35, 0.03, 0.00, 0.00, 4.00, 0.00, 857.90, 7.80, 0.00, 0.00, …\n$ `2014` &lt;dbl&gt; 131.84, 0.03, 0.00, 0.00, 7.10, 0.00, 916.40, 10.30, 0.00, 0.00…\n$ `2015` &lt;dbl&gt; 242.86, 0.03, 0.00, 0.00, 7.60, 0.00, 1000.05, 12.80, 0.00, 0.0…\n$ `2016` &lt;dbl&gt; 572.96, 0.27, 0.00, 5.10, 93.58, 0.00, 1119.17, 15.39, 0.20, 1.…\n$ `2017` &lt;dbl&gt; 1867.23, 0.27, 11.78, 108.52, 128.86, 0.71, 1249.37, 81.40, 0.7…\n$ `2018` &lt;dbl&gt; 2195.46, 5.39, 12.45, 142.45, 231.35, 0.91, 1588.00, 216.85, 0.…\n$ `2019` &lt;dbl&gt; 3085.68, 5.39, 22.40, 142.45, 231.35, 3.89, 2440.13, 224.52, 22…\n$ `2020` &lt;dbl&gt; 3610.02, 5.61, 41.23, 151.57, 231.35, 4.78, 2948.37, 252.14, 32…\n$ `2021` &lt;dbl&gt; 4203.00, 5.61, 42.99, 159.51, 252.48, 7.44, 4430.82, 407.83, 42…\n$ `2022` &lt;dbl&gt; 4386.76, 11.23, 117.94, 190.63, 518.08, 19.95, 7180.03, 910.63,…\n$ `2023` &lt;dbl&gt; 4534.19, 11.64, 147.92, 192.89, 948.82, 26.49, 9254.57, 1029.16…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n\n\n\n\nAndhra Pradesh\n0.10\n2.10\n21.75\n23.35\n131.84\n242.86\n572.96\n1867.23\n2195.46\n3085.68\n3610.02\n4203.00\n4386.76\n4534.19\n\n\nArunachal Pradesh\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.27\n0.27\n5.39\n5.39\n5.61\n5.61\n11.23\n11.64\n\n\nAssam\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n11.78\n12.45\n22.40\n41.23\n42.99\n117.94\n147.92\n\n\nBihar\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n5.10\n108.52\n142.45\n142.45\n151.57\n159.51\n190.63\n192.89\n\n\nChhattisgarh\n0.00\n0.00\n4.00\n4.00\n7.10\n7.60\n93.58\n128.86\n231.35\n231.35\n231.35\n252.48\n518.08\n948.82\n\n\nGoa\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.71\n0.91\n3.89\n4.78\n7.44\n19.95\n26.49\n\n\nGujarat\n0.00\n5.00\n604.89\n857.90\n916.40\n1000.05\n1119.17\n1249.37\n1588.00\n2440.13\n2948.37\n4430.82\n7180.03\n9254.57\n\n\nHaryana\n0.00\n0.00\n16.80\n7.80\n10.30\n12.80\n15.39\n81.40\n216.85\n224.52\n252.14\n407.83\n910.63\n1029.16\n\n\nHimachal Pradesh\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.20\n0.73\n0.73\n22.68\n32.93\n42.73\n76.16\n87.49\n\n\nJammu & Kashmir\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n1.36\n1.36\n14.83\n19.30\n20.73\n54.73\n49.44\n\n\nJharkhand\n0.00\n0.00\n4.00\n16.00\n16.00\n16.00\n16.19\n23.27\n25.67\n34.95\n38.40\n52.06\n88.79\n105.84\n\n\nKarnataka\n6.00\n6.00\n9.00\n14.00\n31.00\n77.22\n145.46\n1027.84\n4944.12\n6095.55\n7277.93\n7355.17\n7590.81\n8241.41\n\n\nKerala\n0.03\n0.03\n0.84\n0.03\n0.03\n0.03\n13.04\n74.20\n107.94\n138.59\n142.23\n257.00\n363.18\n761.44\n\n\nLadakh\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n7.80\n\n\nMadhya Pradesh\n0.10\n0.10\n2.10\n37.32\n347.17\n558.58\n776.37\n857.04\n1305.35\n1840.16\n2258.46\n2463.22\n2717.95\n2802.14\n\n\nMaharashtra\n0.00\n4.00\n20.00\n100.00\n249.25\n360.75\n385.76\n452.37\n1239.18\n1633.54\n1801.80\n2289.97\n2631.02\n4722.90\n\n\nManipur\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.03\n0.06\n3.44\n5.16\n6.36\n12.25\n12.28\n\n\nMeghalaya\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.02\n0.12\n0.12\n0.12\n4.15\n4.15\n\n\nMizoram\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.50\n1.52\n1.53\n7.90\n28.01\n\n\nNagaland\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.50\n1.00\n1.00\n1.00\n1.00\n3.04\n3.04\n\n\nOdisha\n0.00\n0.00\n13.00\n13.00\n30.50\n31.76\n66.92\n79.42\n79.57\n394.73\n397.84\n401.72\n451.24\n453.17\n\n\nPunjab\n1.33\n2.33\n9.33\n9.33\n16.85\n185.27\n405.06\n793.95\n905.62\n905.62\n947.10\n959.50\n1100.07\n1167.26\n\n\nRajasthan\n0.15\n5.15\n197.65\n552.90\n730.10\n942.10\n1269.93\n1812.93\n2332.77\n3226.79\n5137.91\n5732.58\n12564.87\n17055.70\n\n\nSikkim\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.07\n0.07\n4.68\n4.68\n\n\nTamil Nadu\n0.05\n5.05\n15.05\n17.11\n98.36\n142.58\n1061.82\n1691.83\n1908.57\n2575.22\n3915.88\n4475.21\n5067.18\n6736.43\n\n\nTelangana\n0.00\n0.00\n0.00\n0.00\n0.00\n61.25\n527.84\n1286.98\n3291.25\n3592.09\n3620.75\n3953.12\n4520.48\n4666.03\n\n\nTripura\n0.00\n0.00\n0.00\n0.00\n0.00\n5.00\n5.00\n5.09\n5.09\n5.09\n9.41\n9.41\n14.89\n17.60\n\n\nUttar Pradesh\n0.38\n0.38\n12.38\n17.38\n21.08\n71.26\n143.50\n336.73\n694.41\n960.10\n1095.10\n1712.50\n2244.43\n2515.22\n\n\nUttarakhand\n0.05\n0.05\n5.05\n5.05\n5.05\n5.00\n41.15\n233.49\n260.08\n306.75\n315.90\n368.41\n573.54\n575.53\n\n\nWest Bengal\n1.15\n1.15\n2.05\n2.05\n7.05\n7.21\n7.77\n26.14\n37.32\n75.95\n114.46\n149.84\n166.00\n179.98\n\n\nAndaman & Nicobar\n0.10\n0.10\n0.01\n5.10\n5.10\n5.10\n5.10\n6.56\n6.56\n11.73\n12.19\n29.22\n29.49\n29.91\n\n\nChandigarh\n0.00\n0.00\n0.00\n0.00\n2.00\n4.50\n6.81\n17.32\n25.20\n34.71\n40.55\n45.16\n55.17\n58.69\n\n\nDadar & Nagar Haveli\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.97\n5.46\n5.46\n5.46\n5.46\n5.46\n5.46\n\n\nDaman & Diu\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n4.00\n10.46\n3.86\n14.47\n19.86\n40.55\n40.72\n41.01\n\n\nDelhi\n0.05\n2.14\n2.53\n2.56\n5.15\n5.47\n14.28\n40.27\n69.57\n126.89\n165.16\n192.97\n211.12\n218.26\n\n\nLakshadweep\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n0.71\n0.75\n0.75\n0.75\n0.75\n3.27\n3.27\n\n\nPuducherry\n0.03\n0.79\n0.03\n0.03\n0.03\n0.03\n0.02\n0.08\n0.16\n3.14\n5.51\n9.33\n13.69\n35.53\n\n\nOthers\n0.00\n0.00\n0.00\n0.79\n0.82\n0.79\n58.31\n58.31\n0.00\n0.00\n0.00\n0.00\n45.01\n45.01\n\n\n\n\nskimr::skim(india)\n\n\nData summary\n\n\nName\nindia\n\n\nNumber of rows\n38\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nYear\n0\n1\n3\n20\n0\n38\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\n2010\n0\n1\n0.27\n1.00\n0.00\n0.00\n0.00\n0.05\n6.00\n▇▁▁▁▁\n\n\n2011\n0\n1\n0.92\n1.75\n0.00\n0.00\n0.00\n0.78\n6.00\n▇▁▁▁▁\n\n\n2012\n0\n1\n24.77\n101.82\n0.00\n0.00\n0.03\n8.01\n604.89\n▇▁▁▁▁\n\n\n2013\n0\n1\n44.38\n162.77\n0.00\n0.00\n0.39\n12.08\n857.90\n▇▁▁▁▁\n\n\n2014\n0\n1\n69.26\n194.76\n0.00\n0.00\n0.78\n16.64\n916.40\n▇▁▁▁▁\n\n\n2015\n0\n1\n98.53\n237.73\n0.00\n0.00\n4.75\n53.88\n1000.05\n▇▁▁▁▁\n\n\n2016\n0\n1\n177.97\n343.78\n0.00\n0.12\n7.29\n131.02\n1269.93\n▇▁▁▁▁\n\n\n2017\n0\n1\n323.39\n562.96\n0.00\n0.72\n24.70\n310.92\n1867.23\n▇▁▁▁▁\n\n\n2018\n0\n1\n569.60\n1092.11\n0.00\n0.93\n25.44\n585.83\n4944.12\n▇▁▁▁▁\n\n\n2019\n0\n1\n741.60\n1374.44\n0.00\n4.19\n34.83\n777.90\n6095.55\n▇▁▁▁▁\n\n\n2020\n0\n1\n911.26\n1715.51\n0.00\n5.47\n40.89\n809.79\n7277.93\n▇▁▁▁▁\n\n\n2021\n0\n1\n1054.88\n1899.38\n0.00\n6.63\n48.61\n821.58\n7355.17\n▇▁▁▁▁\n\n\n2022\n0\n1\n1420.96\n2741.79\n0.00\n13.99\n103.37\n1052.71\n12564.87\n▇▁▁▁▁\n\n\n2023\n0\n1\n1757.38\n3510.26\n3.04\n26.87\n126.88\n1132.73\n17055.70\n▇▁▁▁▁\n\n\n\n\n\nPrecisamos converter as colunas character para numeric:\n\nindia &lt;- india %&gt;% \n  mutate(across(2:15, as.numeric)) %&gt;% \n  rename(name = Year)\n\nindia %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n\n\n\n\nAndhra Pradesh\n0.10\n2.10\n21.75\n23.35\n131.84\n242.86\n572.96\n1867.23\n2195.46\n3085.68\n3610.02\n4203.00\n4386.76\n4534.19\n\n\nArunachal Pradesh\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.27\n0.27\n5.39\n5.39\n5.61\n5.61\n11.23\n11.64\n\n\nAssam\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n11.78\n12.45\n22.40\n41.23\n42.99\n117.94\n147.92\n\n\nBihar\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n5.10\n108.52\n142.45\n142.45\n151.57\n159.51\n190.63\n192.89\n\n\nChhattisgarh\n0.00\n0.00\n4.00\n4.00\n7.10\n7.60\n93.58\n128.86\n231.35\n231.35\n231.35\n252.48\n518.08\n948.82\n\n\nGoa\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.71\n0.91\n3.89\n4.78\n7.44\n19.95\n26.49\n\n\nGujarat\n0.00\n5.00\n604.89\n857.90\n916.40\n1000.05\n1119.17\n1249.37\n1588.00\n2440.13\n2948.37\n4430.82\n7180.03\n9254.57\n\n\nHaryana\n0.00\n0.00\n16.80\n7.80\n10.30\n12.80\n15.39\n81.40\n216.85\n224.52\n252.14\n407.83\n910.63\n1029.16\n\n\nHimachal Pradesh\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.20\n0.73\n0.73\n22.68\n32.93\n42.73\n76.16\n87.49\n\n\nJammu & Kashmir\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n1.00\n1.36\n1.36\n14.83\n19.30\n20.73\n54.73\n49.44\n\n\nJharkhand\n0.00\n0.00\n4.00\n16.00\n16.00\n16.00\n16.19\n23.27\n25.67\n34.95\n38.40\n52.06\n88.79\n105.84\n\n\nKarnataka\n6.00\n6.00\n9.00\n14.00\n31.00\n77.22\n145.46\n1027.84\n4944.12\n6095.55\n7277.93\n7355.17\n7590.81\n8241.41\n\n\nKerala\n0.03\n0.03\n0.84\n0.03\n0.03\n0.03\n13.04\n74.20\n107.94\n138.59\n142.23\n257.00\n363.18\n761.44\n\n\nLadakh\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n7.80\n\n\nMadhya Pradesh\n0.10\n0.10\n2.10\n37.32\n347.17\n558.58\n776.37\n857.04\n1305.35\n1840.16\n2258.46\n2463.22\n2717.95\n2802.14\n\n\nMaharashtra\n0.00\n4.00\n20.00\n100.00\n249.25\n360.75\n385.76\n452.37\n1239.18\n1633.54\n1801.80\n2289.97\n2631.02\n4722.90\n\n\nManipur\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.03\n0.06\n3.44\n5.16\n6.36\n12.25\n12.28\n\n\nMeghalaya\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.02\n0.12\n0.12\n0.12\n4.15\n4.15\n\n\nMizoram\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.10\n0.10\n0.20\n0.50\n1.52\n1.53\n7.90\n28.01\n\n\nNagaland\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.50\n1.00\n1.00\n1.00\n1.00\n3.04\n3.04\n\n\nOdisha\n0.00\n0.00\n13.00\n13.00\n30.50\n31.76\n66.92\n79.42\n79.57\n394.73\n397.84\n401.72\n451.24\n453.17\n\n\nPunjab\n1.33\n2.33\n9.33\n9.33\n16.85\n185.27\n405.06\n793.95\n905.62\n905.62\n947.10\n959.50\n1100.07\n1167.26\n\n\nRajasthan\n0.15\n5.15\n197.65\n552.90\n730.10\n942.10\n1269.93\n1812.93\n2332.77\n3226.79\n5137.91\n5732.58\n12564.87\n17055.70\n\n\nSikkim\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.07\n0.07\n4.68\n4.68\n\n\nTamil Nadu\n0.05\n5.05\n15.05\n17.11\n98.36\n142.58\n1061.82\n1691.83\n1908.57\n2575.22\n3915.88\n4475.21\n5067.18\n6736.43\n\n\nTelangana\n0.00\n0.00\n0.00\n0.00\n0.00\n61.25\n527.84\n1286.98\n3291.25\n3592.09\n3620.75\n3953.12\n4520.48\n4666.03\n\n\nTripura\n0.00\n0.00\n0.00\n0.00\n0.00\n5.00\n5.00\n5.09\n5.09\n5.09\n9.41\n9.41\n14.89\n17.60\n\n\nUttar Pradesh\n0.38\n0.38\n12.38\n17.38\n21.08\n71.26\n143.50\n336.73\n694.41\n960.10\n1095.10\n1712.50\n2244.43\n2515.22\n\n\nUttarakhand\n0.05\n0.05\n5.05\n5.05\n5.05\n5.00\n41.15\n233.49\n260.08\n306.75\n315.90\n368.41\n573.54\n575.53\n\n\nWest Bengal\n1.15\n1.15\n2.05\n2.05\n7.05\n7.21\n7.77\n26.14\n37.32\n75.95\n114.46\n149.84\n166.00\n179.98\n\n\nAndaman & Nicobar\n0.10\n0.10\n0.01\n5.10\n5.10\n5.10\n5.10\n6.56\n6.56\n11.73\n12.19\n29.22\n29.49\n29.91\n\n\nChandigarh\n0.00\n0.00\n0.00\n0.00\n2.00\n4.50\n6.81\n17.32\n25.20\n34.71\n40.55\n45.16\n55.17\n58.69\n\n\nDadar & Nagar Haveli\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.97\n5.46\n5.46\n5.46\n5.46\n5.46\n5.46\n\n\nDaman & Diu\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n4.00\n10.46\n3.86\n14.47\n19.86\n40.55\n40.72\n41.01\n\n\nDelhi\n0.05\n2.14\n2.53\n2.56\n5.15\n5.47\n14.28\n40.27\n69.57\n126.89\n165.16\n192.97\n211.12\n218.26\n\n\nLakshadweep\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n0.75\n0.71\n0.75\n0.75\n0.75\n0.75\n3.27\n3.27\n\n\nPuducherry\n0.03\n0.79\n0.03\n0.03\n0.03\n0.03\n0.02\n0.08\n0.16\n3.14\n5.51\n9.33\n13.69\n35.53\n\n\nOthers\n0.00\n0.00\n0.00\n0.79\n0.82\n0.79\n58.31\n58.31\n0.00\n0.00\n0.00\n0.00\n45.01\n45.01\n\n\n\n\n\nTratando os dados:\n\nindia %&gt;% \n  pivot_longer(2:15, names_to = \"year\", values_to = \"mw\") %&gt;% \n  mutate(year = as.numeric(year)) %&gt;% \n  ggplot(aes(x=year, y=mw, fill=name))+\n  geom_col()+\n  facet_wrap(vars(name),\n             scales = \"free_y\")+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nggsave(\"all.jpg\",\n       height = 10,\n       width = 12)\n\n\n\n\nminexp = min(india$`2023`)\nmaxexp = max(india$`2023`)\n\nmaxcities &lt;- india %&gt;% \n  select(name, `2015`) %&gt;% \n  filter(`2015` &gt; mean(`2015`))\n\nmaxcities_sf &lt;- maxcities %&gt;% \n  left_join(map2)\n\nJoining with `by = join_by(name)`\n\n\nMapa para 2015:\n\nindia2015 &lt;- india %&gt;%\n   select(1,`2015`) %&gt;% \n   rename(mw = `2015`) %&gt;% \n   left_join(map2) %&gt;% \n   ggplot()+\n   geom_sf(aes(fill=mw/1000, geometry = geometry), color = \"white\", linewidth = 0.1)+\n  geom_text_repel(data = maxcities_sf,\n                  aes(x=longitude,\n                      y=latitude,\n                      label = name),\n                  fontface = \"bold\", \n                  nudge_x = c(10, -5, 10, -5, -3, -5,5), \n                  nudge_y = c(0.25,-0.25, 6, -0.5, 2,1,1))+\n  scale_fill_distiller(palette = \"Reds\", \n                       name = \"GW\",\n                       limits = c(minexp/1000,maxexp/1000),\n                       direction = 1)+\n  theme_void()+\n  theme(panel.grid = element_blank())+\n  labs(title = \"Solar PV Installed Capacity in 2015\")\n\nJoining with `by = join_by(name)`\n\nindia2015\n\n\n\n\n\n\n\n\n\n\n\n\nmaxcities &lt;- india %&gt;% \n  select(name, `2023`) %&gt;% \n  filter(`2023` &gt; mean(`2023`))\n\nmaxcities_sf &lt;- maxcities %&gt;% \n  left_join(map2)\n\nJoining with `by = join_by(name)`\n\n\nMapa para 2023:\n\nindia2023 &lt;- india %&gt;%\n   select(1,`2023`) %&gt;% \n   rename(mw = `2023`) %&gt;% \n   left_join(map2) %&gt;% \n   ggplot()+\n   geom_sf(aes(fill=mw/1000, geometry = geometry), \n           color = \"white\", linewidth = 0.1)+\n  geom_text_repel(data = maxcities_sf,\n                  aes(x=longitude,\n                      y=latitude,\n                      label = name),\n                  fontface = \"bold\", \n                  nudge_x = c(10, -5, -5, 12, -5,-5,-5,5,-3), \n                  nudge_y = c(0.25,-0.25, 0.5, 8, -0.5,2,-2,2,5))+\n  scale_fill_distiller(palette = \"Reds\", \n                       name = \"GW\",\n                       limits = c(minexp/1000,maxexp/1000),\n                       direction = 1)+\n  theme_void()+\n  theme(panel.grid = element_blank())+\n  labs(title = \"Solar PV Installed Capacity in 2023\")\n\nJoining with `by = join_by(name)`\n\nindia2023\n\n\n\n\n\n\n\n\n\n\n\n\nindia_2015_2023 &lt;- india2015 + india2023 + plot_layout(ncol=1)\n\nindia_2015_2023\n\n\n\n\n\n\n\nggsave(\"india2015_2023.jpg\",\n       width = 7,\n       height = 10)\n\nCalculando o market share de cada estado:\n\ntotal &lt;- sum(india$`2023`)\nmaxcities_total &lt;- sum(maxcities$`2023`)\n\ncolnames(maxcities) &lt;- c(\"name\",\"mw\")\n\nother &lt;- data.frame(name = \"Other\",\n                    mw = total - maxcities_total)\n\nmaxcities &lt;- rbind(maxcities, other)\n\nmaxcities %&gt;% \n  mutate(market = round(100*mw/total,2),\n         mw_2030 = 300000*market/100)\n\n# A tibble: 10 × 4\n   name               mw market mw_2030\n   &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 Andhra Pradesh  4534.   6.79   20370\n 2 Gujarat         9255.  13.9    41580\n 3 Karnataka       8241.  12.3    37020\n 4 Madhya Pradesh  2802.   4.2    12600\n 5 Maharashtra     4723.   7.07   21210\n 6 Rajasthan      17056.  25.5    76620\n 7 Tamil Nadu      6736.  10.1    30270\n 8 Telangana       4666.   6.99   20970\n 9 Uttar Pradesh   2515.   3.77   11310\n10 Other           6252.   9.36   28080"
  },
  {
    "objectID": "data1ggplot/Tutorial2.html",
    "href": "data1ggplot/Tutorial2.html",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl) # para fazer leitura de arquivos excel\nlibrary(knitr) # para formatar tabelas"
  },
  {
    "objectID": "data1ggplot/Tutorial2.html#exemplo-base",
    "href": "data1ggplot/Tutorial2.html#exemplo-base",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nPara realizar gráficos de dispersão, precisamos passar a função geom_point() como uma nova camada do ggplot. As coordenadas x e y devem ser numéricas necessariamente. Vamos usar como exemplo, o dataframe imdb_filmes.\nNo exemplo, a posição do ponto no eixo x pode ser dada pela coluna orcamento e a posição do ponto no eixo y pela coluna receita.\n\nimdb_filmes %&gt;% \n  ggplot()"
  },
  {
    "objectID": "data1ggplot/Tutorial2.html#adicionando-cores",
    "href": "data1ggplot/Tutorial2.html#adicionando-cores",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Adicionando cores",
    "text": "Adicionando cores\nPodemos adicionar cores grupalmente para todos os pontos ou podemos usar alguma outra variável para criar cores em gradiente\nPara ver mais detalhamento da função geom_point() recomenda-se a leitura do Capítulo 8 do Livro “Curso-R”."
  },
  {
    "objectID": "data1ggplot/Tutorial2.html#exemplo-base-1",
    "href": "data1ggplot/Tutorial2.html#exemplo-base-1",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nPara realizar gráficos de barras usamos geom_col(). Vamos usar o dataframe imdb_filmes para exemplificar, ordenando as linhas por ordem decrescente de UserRating. Primeiro vamos gerar o gráfico com as configuração padrão.\n\nimdb_filmes %&gt;% \n  arrange(desc(nota_imdb)) %&gt;% \n  head(10)  # adicionar código do ggplot\n\n# A tibble: 10 × 20\n   id_filme  titulo   ano data_lancamento generos duracao pais  idioma orcamento\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 tt102189… As I …  2019 2019-12-06      Drama,…      62 USA   Engli…     10000\n 2 tt6735740 Love …  2019 2019-06-23      Comedy      100 USA   Engli…   3000000\n 3 tt0111161 The S…  1994 1995-02-10      Drama       142 USA   Engli…  25000000\n 4 tt0068646 The G…  1972 1972-09-21      Crime,…     175 USA   Engli…   6000000\n 5 tt5980638 The T…  2018 2020-06-19      Music,…      96 USA   Engli…     90000\n 6 tt0071562 The G…  1974 1975-09-25      Crime,…     202 USA   Engli…  13000000\n 7 tt0050083 12 An…  1957 1957-09-04      Crime,…      96 USA   Engli…    350000\n 8 tt0110912 Pulp …  1994 1994-10-28      Crime,…     154 USA   Engli…   8000000\n 9 tt0108052 Schin…  1993 1994-03-11      Biogra…     195 USA   Engli…  22000000\n10 tt0419781 Grave…  2005 2005-04-22      Thrill…      90 USA   Engli…   1930000\n# ℹ 11 more variables: receita &lt;dbl&gt;, receita_eua &lt;dbl&gt;, nota_imdb &lt;dbl&gt;,\n#   num_avaliacoes &lt;dbl&gt;, direcao &lt;chr&gt;, roteiro &lt;chr&gt;, producao &lt;chr&gt;,\n#   elenco &lt;chr&gt;, descricao &lt;chr&gt;, num_criticas_publico &lt;dbl&gt;,\n#   num_criticas_critica &lt;dbl&gt;"
  },
  {
    "objectID": "data1ggplot/Tutorial2.html#mudando-os-eixos-e-adicionando-cores",
    "href": "data1ggplot/Tutorial2.html#mudando-os-eixos-e-adicionando-cores",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Mudando os eixos e adicionando cores",
    "text": "Mudando os eixos e adicionando cores\nPodemos mudar os eixos (trocar de eixo) usando a função coord_flip()\n\nimdb_filmes %&gt;% \n  arrange(desc(nota_imdb)) %&gt;% \n  head(10)\n\n# A tibble: 10 × 20\n   id_filme  titulo   ano data_lancamento generos duracao pais  idioma orcamento\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 tt102189… As I …  2019 2019-12-06      Drama,…      62 USA   Engli…     10000\n 2 tt6735740 Love …  2019 2019-06-23      Comedy      100 USA   Engli…   3000000\n 3 tt0111161 The S…  1994 1995-02-10      Drama       142 USA   Engli…  25000000\n 4 tt0068646 The G…  1972 1972-09-21      Crime,…     175 USA   Engli…   6000000\n 5 tt5980638 The T…  2018 2020-06-19      Music,…      96 USA   Engli…     90000\n 6 tt0071562 The G…  1974 1975-09-25      Crime,…     202 USA   Engli…  13000000\n 7 tt0050083 12 An…  1957 1957-09-04      Crime,…      96 USA   Engli…    350000\n 8 tt0110912 Pulp …  1994 1994-10-28      Crime,…     154 USA   Engli…   8000000\n 9 tt0108052 Schin…  1993 1994-03-11      Biogra…     195 USA   Engli…  22000000\n10 tt0419781 Grave…  2005 2005-04-22      Thrill…      90 USA   Engli…   1930000\n# ℹ 11 more variables: receita &lt;dbl&gt;, receita_eua &lt;dbl&gt;, nota_imdb &lt;dbl&gt;,\n#   num_avaliacoes &lt;dbl&gt;, direcao &lt;chr&gt;, roteiro &lt;chr&gt;, producao &lt;chr&gt;,\n#   elenco &lt;chr&gt;, descricao &lt;chr&gt;, num_criticas_publico &lt;dbl&gt;,\n#   num_criticas_critica &lt;dbl&gt;\n\n\nVamos aplicar tudo o aprendido num segundo exemplo, usando o dataframe imdb para mostrar os seriados com maior número de votos UserVotes.\n\nimdb %&gt;% \n  group_by(series_name) %&gt;% \n  summarize(Votos = sum(UserVotes)) %&gt;% \n  arrange(desc(Votos)) #adicionar ggplot\n\n# A tibble: 27 × 2\n   series_name                  Votos\n   &lt;chr&gt;                        &lt;dbl&gt;\n 1 The Walking Dead           2394586\n 2 Arquivo X                   969628\n 3 Friends                     853287\n 4 Sobrenatural                499046\n 5 Era Uma Vez                 397332\n 6 Stranger Things             391288\n 7 Sherlock                    373114\n 8 Lúcifer                     298412\n 9 Dark                        264631\n10 Como Defender um Assassino  228352\n# ℹ 17 more rows\n\n\nVocê pode ver mais exemplos no Capítulo 8 do livro Curso-R"
  },
  {
    "objectID": "data1ggplot/Tutorial2.html#exemplo-base-2",
    "href": "data1ggplot/Tutorial2.html#exemplo-base-2",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nVamos fazer um gráfico de linha usando a função geom_line() como camada do ggplot usando o dataset netflix_filmes para mostrar o total de capítulos assistidos por dia.\n\nnetflix_filmes %&gt;%  #colocar código ggplot\n  group_by(Date) %&gt;% \n  summarize(filmes=n())\n\n# A tibble: 114 × 2\n   Date       filmes\n   &lt;date&gt;      &lt;int&gt;\n 1 2015-08-24      2\n 2 2015-10-12      1\n 3 2015-11-16      1\n 4 2015-12-23      1\n 5 2016-01-17      2\n 6 2016-01-31      1\n 7 2016-12-29      1\n 8 2017-01-29      1\n 9 2017-03-05      2\n10 2017-03-12      1\n# ℹ 104 more rows\n\n\nPodemos agrupar contagens usando a função floor_date do pacote lubridate para meses, trimestres, etc. Adicionalmente, podemos incluir cores.\n\nnetflix_filmes %&gt;% \n  count(mes = lubridate::floor_date(Date, \"month\"))\n\n# A tibble: 48 × 2\n   mes            n\n   &lt;date&gt;     &lt;int&gt;\n 1 2015-08-01     2\n 2 2015-10-01     1\n 3 2015-11-01     1\n 4 2015-12-01     1\n 5 2016-01-01     3\n 6 2016-12-01     1\n 7 2017-01-01     1\n 8 2017-03-01     3\n 9 2017-05-01     1\n10 2017-06-01     8\n# ℹ 38 more rows"
  },
  {
    "objectID": "data1ggplot/Tutorial2.html#adicionando-mais-de-uma-linha",
    "href": "data1ggplot/Tutorial2.html#adicionando-mais-de-uma-linha",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Adicionando mais de uma linha",
    "text": "Adicionando mais de uma linha\nVamos usar o dataset imdb_filmes para comparar o desempenho dos filmes (nota imdb) em função de ter lucro ou não. Vamos adicionar também uma camada extra para nos mostrar a tendência de ambas curvas, usando geom_smooth().\n\nimdb_filmes %&gt;% \n  mutate(lucro = receita - orcamento,\n         lucro_factor = ifelse(lucro &gt; 0, \"Sim\",\"Não\")) %&gt;% \n  filter(!is.na(lucro)) %&gt;% \n  group_by(lucro_factor,ano) %&gt;% \n  summarise(nota_media=mean(nota_imdb,na.rm=TRUE))\n\n`summarise()` has grouped output by 'lucro_factor'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 169 × 3\n# Groups:   lucro_factor [2]\n   lucro_factor   ano nota_media\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 Não           1921       8.3 \n 2 Não           1923       7   \n 3 Não           1925       8.2 \n 4 Não           1927       7.8 \n 5 Não           1928       8.1 \n 6 Não           1931       7.85\n 7 Não           1932       7.9 \n 8 Não           1933       7.65\n 9 Não           1934       7.55\n10 Não           1935       7.3 \n# ℹ 159 more rows\n\n\nVocê pode ver mais exemplos do geom_line() no livro Curso-R"
  },
  {
    "objectID": "data1ggplot/Tutorial2.html#exemplo-base-3",
    "href": "data1ggplot/Tutorial2.html#exemplo-base-3",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nVamos usar os dados do dataset imdb para comparar as notas de UserRating em cada temporada do seriado The Walking Dead.\n\nseriado_escolhido &lt;- \"The Walking Dead\"\n\nimdb %&gt;% \n  filter(series_name == seriado_escolhido) %&gt;% \n  select(1,2,4,7) \n\n# A tibble: 288 × 4\n   series_name      Episode                     season UserRating\n   &lt;chr&gt;            &lt;chr&gt;                        &lt;dbl&gt;      &lt;dbl&gt;\n 1 The Walking Dead Seed                             3        8.9\n 2 The Walking Dead Sick                             3        8.6\n 3 The Walking Dead Walk with Me                     3        8.2\n 4 The Walking Dead Killer Within                    3        9.3\n 5 The Walking Dead Say the Word                     3        8.2\n 6 The Walking Dead Hounded                          3        8.3\n 7 The Walking Dead When the Dead Come Knocking      3        8.6\n 8 The Walking Dead Made to Suffer                   3        9  \n 9 The Walking Dead The Suicide King                 3        8.2\n10 The Walking Dead Home                             3        8.7\n# ℹ 278 more rows"
  },
  {
    "objectID": "data1ggplot/Tutorial2.html#adicionando-cores-por-meio-de-paletas-pré-definidas",
    "href": "data1ggplot/Tutorial2.html#adicionando-cores-por-meio-de-paletas-pré-definidas",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Adicionando cores por meio de paletas pré-definidas",
    "text": "Adicionando cores por meio de paletas pré-definidas\nVamos adicionar cores para cada temporada, para isto precisamos incluir o argumento fill dentro da camada de estetica aes. Podemos também escolher as cores de paletas pré-definidas como: scale_fill_viridis_d() e scale_fill_brewer()\n\nimdb %&gt;% \n  filter(series_name == seriado_escolhido) %&gt;% \n  select(1,2,4,7) \n\n# A tibble: 288 × 4\n   series_name      Episode                     season UserRating\n   &lt;chr&gt;            &lt;chr&gt;                        &lt;dbl&gt;      &lt;dbl&gt;\n 1 The Walking Dead Seed                             3        8.9\n 2 The Walking Dead Sick                             3        8.6\n 3 The Walking Dead Walk with Me                     3        8.2\n 4 The Walking Dead Killer Within                    3        9.3\n 5 The Walking Dead Say the Word                     3        8.2\n 6 The Walking Dead Hounded                          3        8.3\n 7 The Walking Dead When the Dead Come Knocking      3        8.6\n 8 The Walking Dead Made to Suffer                   3        9  \n 9 The Walking Dead The Suicide King                 3        8.2\n10 The Walking Dead Home                             3        8.7\n# ℹ 278 more rows"
  },
  {
    "objectID": "data1ggplot/Tutorial2.html#exemplo-base-4",
    "href": "data1ggplot/Tutorial2.html#exemplo-base-4",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nVamos continuar com os dados sobre UserRating usados anteriormente para graficar Boxplots. Neste caso, vamos graficar a distribuição do UserRating considerando apenas a contagem. Podemos usar o argumento bins ou binwidth para ajustar melhor o resultado.\n\nimdb %&gt;% \n  filter(series_name == seriado_escolhido) %&gt;% \n  select(1,2,4,7) \n\n# A tibble: 288 × 4\n   series_name      Episode                     season UserRating\n   &lt;chr&gt;            &lt;chr&gt;                        &lt;dbl&gt;      &lt;dbl&gt;\n 1 The Walking Dead Seed                             3        8.9\n 2 The Walking Dead Sick                             3        8.6\n 3 The Walking Dead Walk with Me                     3        8.2\n 4 The Walking Dead Killer Within                    3        9.3\n 5 The Walking Dead Say the Word                     3        8.2\n 6 The Walking Dead Hounded                          3        8.3\n 7 The Walking Dead When the Dead Come Knocking      3        8.6\n 8 The Walking Dead Made to Suffer                   3        9  \n 9 The Walking Dead The Suicide King                 3        8.2\n10 The Walking Dead Home                             3        8.7\n# ℹ 278 more rows\n\n\nAgora, vamos usar o dataset imdb_filmes para verificar quão lucrativo é um determinado genero.\n\ngenero_escolhido &lt;- \"Comedy\"\n\nimdb_filmes %&gt;% \n  #filter(str_detect(string = generos, pattern = genero_escolhido)) %&gt;% \n  filter(generos == genero_escolhido) %&gt;% \n  mutate(lucro = receita - orcamento)\n\n# A tibble: 560 × 21\n   id_filme  titulo   ano data_lancamento generos duracao pais  idioma orcamento\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 tt0183505 Me, M…  2000 2000-09-08      Comedy      116 USA   Engli…  51000000\n 2 tt1242423 Dear …  2009 2011-07-01      Comedy       87 USA   Engli…        NA\n 3 tt0120434 Vegas…  1997 1997-02-14      Comedy       93 USA   Engli…  25000000\n 4 tt5617916 Airpl…  2019 2019-08-02      Comedy       80 USA   Engli…        NA\n 5 tt7054636 A Mad…  2019 2019-03-01      Comedy      109 USA   Engli…  20000000\n 6 tt0460925 The S…  2006 2006-01-01      Comedy       86 USA   Engli…        NA\n 7 tt0071514 For P…  1974 1974-09-13      Comedy       90 USA   Engli…        NA\n 8 tt0064683 Mondo…  1969 1969-03-14      Comedy       95 USA   Engli…      2100\n 9 tt1073498 Meet …  2008 2008-04-24      Comedy       87 USA   Engli…  30000000\n10 tt0489085 I-See…  2006 2006-03-08      Comedy       92 USA   Engli…   6200000\n# ℹ 550 more rows\n# ℹ 12 more variables: receita &lt;dbl&gt;, receita_eua &lt;dbl&gt;, nota_imdb &lt;dbl&gt;,\n#   num_avaliacoes &lt;dbl&gt;, direcao &lt;chr&gt;, roteiro &lt;chr&gt;, producao &lt;chr&gt;,\n#   elenco &lt;chr&gt;, descricao &lt;chr&gt;, num_criticas_publico &lt;dbl&gt;,\n#   num_criticas_critica &lt;dbl&gt;, lucro &lt;dbl&gt;"
  },
  {
    "objectID": "data1ggplot/Tutorial2.html#adicionando-cores-1",
    "href": "data1ggplot/Tutorial2.html#adicionando-cores-1",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Adicionando cores",
    "text": "Adicionando cores\nPodemos melhorar a apresentação, escolhendo cores diferentes adicionando o argumento fill dentro de geom_histogram().\n\nimdb_filmes %&gt;% \n  #filter(str_detect(string = generos, pattern = genero_escolhido)) %&gt;% \n  filter(generos == genero_escolhido) %&gt;% \n  mutate(lucro = receita - orcamento) \n\n# A tibble: 560 × 21\n   id_filme  titulo   ano data_lancamento generos duracao pais  idioma orcamento\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 tt0183505 Me, M…  2000 2000-09-08      Comedy      116 USA   Engli…  51000000\n 2 tt1242423 Dear …  2009 2011-07-01      Comedy       87 USA   Engli…        NA\n 3 tt0120434 Vegas…  1997 1997-02-14      Comedy       93 USA   Engli…  25000000\n 4 tt5617916 Airpl…  2019 2019-08-02      Comedy       80 USA   Engli…        NA\n 5 tt7054636 A Mad…  2019 2019-03-01      Comedy      109 USA   Engli…  20000000\n 6 tt0460925 The S…  2006 2006-01-01      Comedy       86 USA   Engli…        NA\n 7 tt0071514 For P…  1974 1974-09-13      Comedy       90 USA   Engli…        NA\n 8 tt0064683 Mondo…  1969 1969-03-14      Comedy       95 USA   Engli…      2100\n 9 tt1073498 Meet …  2008 2008-04-24      Comedy       87 USA   Engli…  30000000\n10 tt0489085 I-See…  2006 2006-03-08      Comedy       92 USA   Engli…   6200000\n# ℹ 550 more rows\n# ℹ 12 more variables: receita &lt;dbl&gt;, receita_eua &lt;dbl&gt;, nota_imdb &lt;dbl&gt;,\n#   num_avaliacoes &lt;dbl&gt;, direcao &lt;chr&gt;, roteiro &lt;chr&gt;, producao &lt;chr&gt;,\n#   elenco &lt;chr&gt;, descricao &lt;chr&gt;, num_criticas_publico &lt;dbl&gt;,\n#   num_criticas_critica &lt;dbl&gt;, lucro &lt;dbl&gt;\n\n\nVocê pode ver mais exemplos do Histogramas e Boxplots no Cap. 8 do livro Curso-R\nPodemos usar alternativamente um outro tipo de visualização muito similar, que é a geom_density(). Vamos usar os mesmos exemplos anteriores.\n\nimdb %&gt;% \n  filter(series_name == seriado_escolhido) %&gt;% \n  select(1,2,4,7)\n\n# A tibble: 288 × 4\n   series_name      Episode                     season UserRating\n   &lt;chr&gt;            &lt;chr&gt;                        &lt;dbl&gt;      &lt;dbl&gt;\n 1 The Walking Dead Seed                             3        8.9\n 2 The Walking Dead Sick                             3        8.6\n 3 The Walking Dead Walk with Me                     3        8.2\n 4 The Walking Dead Killer Within                    3        9.3\n 5 The Walking Dead Say the Word                     3        8.2\n 6 The Walking Dead Hounded                          3        8.3\n 7 The Walking Dead When the Dead Come Knocking      3        8.6\n 8 The Walking Dead Made to Suffer                   3        9  \n 9 The Walking Dead The Suicide King                 3        8.2\n10 The Walking Dead Home                             3        8.7\n# ℹ 278 more rows\n\n\nPodemos também customizar a cor da linha e da área abaixo da densidade usando os argumentos fill e color dentro do geom_density() bem como a transparência usando alpha.\n\nimdb_filmes %&gt;% \n  #filter(str_detect(string = generos, pattern = genero_escolhido)) %&gt;% \n  filter(generos == genero_escolhido) %&gt;% \n  mutate(lucro = receita - orcamento)\n\n# A tibble: 560 × 21\n   id_filme  titulo   ano data_lancamento generos duracao pais  idioma orcamento\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 tt0183505 Me, M…  2000 2000-09-08      Comedy      116 USA   Engli…  51000000\n 2 tt1242423 Dear …  2009 2011-07-01      Comedy       87 USA   Engli…        NA\n 3 tt0120434 Vegas…  1997 1997-02-14      Comedy       93 USA   Engli…  25000000\n 4 tt5617916 Airpl…  2019 2019-08-02      Comedy       80 USA   Engli…        NA\n 5 tt7054636 A Mad…  2019 2019-03-01      Comedy      109 USA   Engli…  20000000\n 6 tt0460925 The S…  2006 2006-01-01      Comedy       86 USA   Engli…        NA\n 7 tt0071514 For P…  1974 1974-09-13      Comedy       90 USA   Engli…        NA\n 8 tt0064683 Mondo…  1969 1969-03-14      Comedy       95 USA   Engli…      2100\n 9 tt1073498 Meet …  2008 2008-04-24      Comedy       87 USA   Engli…  30000000\n10 tt0489085 I-See…  2006 2006-03-08      Comedy       92 USA   Engli…   6200000\n# ℹ 550 more rows\n# ℹ 12 more variables: receita &lt;dbl&gt;, receita_eua &lt;dbl&gt;, nota_imdb &lt;dbl&gt;,\n#   num_avaliacoes &lt;dbl&gt;, direcao &lt;chr&gt;, roteiro &lt;chr&gt;, producao &lt;chr&gt;,\n#   elenco &lt;chr&gt;, descricao &lt;chr&gt;, num_criticas_publico &lt;dbl&gt;,\n#   num_criticas_critica &lt;dbl&gt;, lucro &lt;dbl&gt;\n\n\nE por fim, podemos comparar duas ou mais categorias. Vamos fazer isso, comparando os generos “Comedy” e “Drama” para saber se há diferenças significativas em termos de lucro. Para isto, vamos incluir o argumento group dentro da camada aes além de fill e color. Adicionalmente podemos incluir alpha dentro de geom_density().\n\nimdb_filmes %&gt;% \n  #filter(str_detect(string = generos, pattern = genero_escolhido)) %&gt;% \n  filter(generos %in% c(\"Comedy\",\"Drama\")) %&gt;% \n  mutate(lucro = receita - orcamento)\n\n# A tibble: 1,188 × 21\n   id_filme  titulo   ano data_lancamento generos duracao pais  idioma orcamento\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 tt0183505 Me, M…  2000 2000-09-08      Comedy      116 USA   Engli…  51000000\n 2 tt3703836 Henry…  2015 2016-01-08      Drama        87 USA   Engli…        NA\n 3 tt1142798 The F…  2008 2008-09-12      Drama       111 USA   Engli…        NA\n 4 tt1242423 Dear …  2009 2011-07-01      Comedy       87 USA   Engli…        NA\n 5 tt0109514 Curse…  1994 1995-05-05      Drama       102 USA   Engli…  12577385\n 6 tt0120434 Vegas…  1997 1997-02-14      Comedy       93 USA   Engli…  25000000\n 7 tt0068528 The E…  1972 1973-05-30      Drama       100 USA   Engli…        NA\n 8 tt5617916 Airpl…  2019 2019-08-02      Comedy       80 USA   Engli…        NA\n 9 tt0049800 Storm…  1956 1956-07-31      Drama        85 USA   Engli…        NA\n10 tt7054636 A Mad…  2019 2019-03-01      Comedy      109 USA   Engli…  20000000\n# ℹ 1,178 more rows\n# ℹ 12 more variables: receita &lt;dbl&gt;, receita_eua &lt;dbl&gt;, nota_imdb &lt;dbl&gt;,\n#   num_avaliacoes &lt;dbl&gt;, direcao &lt;chr&gt;, roteiro &lt;chr&gt;, producao &lt;chr&gt;,\n#   elenco &lt;chr&gt;, descricao &lt;chr&gt;, num_criticas_publico &lt;dbl&gt;,\n#   num_criticas_critica &lt;dbl&gt;, lucro &lt;dbl&gt;"
  },
  {
    "objectID": "Tutorial6.html",
    "href": "Tutorial6.html",
    "title": "Tutorial 6 - Classificação com Tidymodels",
    "section": "",
    "text": "Vamos usar um dataset sobre churn.\n\n\nVocê pode baixar os dados aqui.\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(fst)\nlibrary(knitr)\n\nchurn &lt;- read_fst(\"data6/churn.fst\")\n\nchurn$has_churned &lt;- as_factor(churn$has_churned)\n\n\nset.seed(1212)\n\nsplit &lt;- initial_split(churn, prop = 0.7, strata = has_churned)\n\ntrain &lt;- training(split)\ntest &lt;- testing(split)"
  },
  {
    "objectID": "Tutorial6.html#carregando-os-dados",
    "href": "Tutorial6.html#carregando-os-dados",
    "title": "Tutorial 6 - Classificação com Tidymodels",
    "section": "",
    "text": "Vamos usar um dataset sobre churn.\n\n\nVocê pode baixar os dados aqui.\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(fst)\nlibrary(knitr)\n\nchurn &lt;- read_fst(\"data6/churn.fst\")\n\nchurn$has_churned &lt;- as_factor(churn$has_churned)\n\n\nset.seed(1212)\n\nsplit &lt;- initial_split(churn, prop = 0.7, strata = has_churned)\n\ntrain &lt;- training(split)\ntest &lt;- testing(split)"
  },
  {
    "objectID": "Tutorial6.html#feature-engineering",
    "href": "Tutorial6.html#feature-engineering",
    "title": "Tutorial 6 - Classificação com Tidymodels",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nPara aprimorar a robustez das nossas estimações, iremos usar um procedimento denominado k-fold cross validation ou validação cruzada. Neste procedimento, os dados de treino são subdivididos aleatoriamente em treino e teste e seus parâmetros de acurácia são calculados. Este procedimento é repetido ‘k’ vezes de forma que sejam calculados os parâmetros de acurácia de cada ‘k’. Uma vez que o procedimento é completado (ajustando o modelo a cada ‘k’ subdataset de treino e teste), calcula-se a média de todos os parâmetros de acurácia. Mais detalhes aqui.\n\nfold &lt;- vfold_cv(train)\n\nrec &lt;- recipe(has_churned ~ ., data = train) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric())\n\nwf &lt;- workflow() %&gt;% \n  add_recipe(rec)"
  },
  {
    "objectID": "Tutorial6.html#treinamento-dos-modelos",
    "href": "Tutorial6.html#treinamento-dos-modelos",
    "title": "Tutorial 6 - Classificação com Tidymodels",
    "section": "Treinamento dos modelos",
    "text": "Treinamento dos modelos\nVamos treinar um modelo de Regressão Logística (usando glm), logo um de Árvore de Decisão (usando rpart) e finalmente um de Random Forest (usando ranger)\n\nglm_spec &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\")\n\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\nrf_spec &lt;- rand_forest(trees=1000) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"classification\")"
  },
  {
    "objectID": "Tutorial6.html#regressão-logística",
    "href": "Tutorial6.html#regressão-logística",
    "title": "Tutorial 6 - Classificação com Tidymodels",
    "section": "Regressão Logística",
    "text": "Regressão Logística\nSegue os resultados do modelo de Regressão Logística:\n\ndoParallel::registerDoParallel()\n\nlogreg &lt;- wf %&gt;% \n  add_model(glm_spec) %&gt;% \n  fit_resamples(resamples=fold,\n                control=control_resamples(save_pred=TRUE))\n\n\ncollect_metrics(logreg) %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\naccuracy\nbinary\n0.6357143\n10\n0.0309524\nPreprocessor1_Model1\n\n\nroc_auc\nbinary\n0.6497342\n10\n0.0435525\nPreprocessor1_Model1\n\n\nsensitivity\nbinary\n0.5311459\n10\n0.0570226\nPreprocessor1_Model1\n\n\nspecificity\nbinary\n0.7235152\n10\n0.0452038\nPreprocessor1_Model1\n\n\n\n\nlogreg %&gt;% \n  conf_mat_resampled() %&gt;% \n  kable()\n\n\n\n\nPrediction\nTruth\nFreq\n\n\n\n\n0\n0\n7.7\n\n\n0\n1\n3.9\n\n\n1\n0\n6.3\n\n\n1\n1\n10.1\n\n\n\n\nlogreg %&gt;% \n  collect_predictions() %&gt;% \n  group_by(id) %&gt;% \n  roc_curve(has_churned, .pred_0) %&gt;% \n  autoplot()"
  },
  {
    "objectID": "Tutorial6.html#árvore-de-decisão",
    "href": "Tutorial6.html#árvore-de-decisão",
    "title": "Tutorial 6 - Classificação com Tidymodels",
    "section": "Árvore de Decisão",
    "text": "Árvore de Decisão\nVamos treinar o modelo:\n\ntree &lt;- wf %&gt;% \n  add_model(tree_spec) %&gt;% \n  fit_resamples(resamples=fold,\n                control=control_resamples(save_pred=TRUE))\n\nSegue os resultados do modelo de Árvore de Decisão:\n\ncollect_metrics(tree)\n\n# A tibble: 4 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.582    10  0.0261 Preprocessor1_Model1\n2 roc_auc     binary     0.590    10  0.0343 Preprocessor1_Model1\n3 sensitivity binary     0.479    10  0.0582 Preprocessor1_Model1\n4 specificity binary     0.680    10  0.0416 Preprocessor1_Model1\n\ntree %&gt;% \n  conf_mat_resampled()\n\n# A tibble: 4 × 3\n  Prediction Truth  Freq\n  &lt;fct&gt;      &lt;fct&gt; &lt;dbl&gt;\n1 0          0       6.8\n2 0          1       4.5\n3 1          0       7.2\n4 1          1       9.5\n\ntree %&gt;% \n  collect_predictions() %&gt;% \n  group_by(id) %&gt;% \n  roc_curve(has_churned, .pred_0) %&gt;% \n  autoplot()"
  },
  {
    "objectID": "Tutorial6.html#random-forest",
    "href": "Tutorial6.html#random-forest",
    "title": "Tutorial 6 - Classificação com Tidymodels",
    "section": "Random Forest",
    "text": "Random Forest\nVamos treinar o modelo:\n\nrf &lt;- wf %&gt;% \n  add_model(rf_spec) %&gt;% \n  fit_resamples(resamples=fold,\n                control=control_resamples(save_pred=TRUE))\n\nSegue os resultados do modelo do Random Forest:\n\ncollect_metrics(rf)\n\n# A tibble: 4 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.582    10  0.0333 Preprocessor1_Model1\n2 roc_auc     binary     0.619    10  0.0409 Preprocessor1_Model1\n3 sensitivity binary     0.545    10  0.0554 Preprocessor1_Model1\n4 specificity binary     0.621    10  0.0438 Preprocessor1_Model1\n\nrf %&gt;% \n  conf_mat_resampled()\n\n# A tibble: 4 × 3\n  Prediction Truth  Freq\n  &lt;fct&gt;      &lt;fct&gt; &lt;dbl&gt;\n1 0          0       7.7\n2 0          1       5.4\n3 1          0       6.3\n4 1          1       8.6\n\nrf %&gt;% \n  collect_predictions() %&gt;% \n  group_by(id) %&gt;% \n  roc_curve(has_churned, .pred_0) %&gt;% \n  autoplot()"
  },
  {
    "objectID": "Tutorial6.html#tuning-de-modelos",
    "href": "Tutorial6.html#tuning-de-modelos",
    "title": "Tutorial 6 - Classificação com Tidymodels",
    "section": "Tuning de Modelos",
    "text": "Tuning de Modelos\nO processo de tuning serve para procurar por valores para os parâmetros de um modelo de machine learning, que gerem a melhor previsão possível.\nCada modelo de machine learning tem seus próprios parâmetros, por exemplo, para árvores de decisão, alguns dos parâmetros são o cost complexity e a tree_depth já para um modelo de random forest, os principais parâmetros são o mtry, e o trees (número de árvores).\nPara encontrar os parâmetros para o modelo que você está usando acesse o site do tidymodels com uma tabela dinâmica que ajuda na busca aqui.\n\nÁrvore de Decisão\nVamos melhorar a precisão do nosso modelo utilizando o procedimento conhecido como autotune com dois parâmetros: cost_complexity e tree_depth.\n\ntune_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\nAgora precisamos criar uma grid para o algoritmo procurar os melhores valores para os parâmetros mencionados anteriormente.\n\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n\nComo temos dois parâmetros para tuning, o tree_grid retornará 25 valores, 5 para cada parâmetro.\n\ntree_grid\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# ℹ 15 more rows\n\n\nAgora vamos treinar nosso modelo, criando também previsões com validação cruzada.\n\ntree_tune &lt;- wf %&gt;%\n  add_model(tune_spec) %&gt;% \n  tune_grid(resamples = fold,\n            grid = tree_grid)\n\n\ntree_tune %&gt;% \n  collect_metrics() %&gt;% \n  filter(.metric == \"roc_auc\") %&gt;% \n  ggplot(aes(cost_complexity, mean, color = factor(tree_depth)))+\n  geom_line(linewidth = 1)+\n  scale_x_log10(labels = scales::label_number())\n\n\n\n\n\n\n\n\nO modelo que teve melhor desempenho foi o que tem a linha mais alta (tree_depth = 8), podemos conferir esse resultado com a função show_best():\n\ntree_tune %&gt;% \n  show_best()\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator  mean     n std_err .config     \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1    0.0000000001          8 roc_auc binary     0.617    10  0.0330 Preprocesso…\n2    0.0000000178          8 roc_auc binary     0.617    10  0.0330 Preprocesso…\n3    0.00000316            8 roc_auc binary     0.617    10  0.0330 Preprocesso…\n4    0.000562              8 roc_auc binary     0.617    10  0.0330 Preprocesso…\n5    0.0000000001         11 roc_auc binary     0.615    10  0.0361 Preprocesso…\n\n\nPor fim, podemos escolher automaticamente os dois melhores valores para nossos parâmetros utilizando a função select_best():\n\nbest_tree &lt;- tree_tune %&gt;% \n  select_best(\"roc_auc\")\n\nbest_tree\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001          8 Preprocessor1_Model11\n\n\nAgora podemos atualizar nosso objeto de workflow com os valores obtidos por select_best():\n\nfinal_tree_wf &lt;- wf %&gt;% \n  add_model(tune_spec) %&gt;% \n  finalize_workflow(best_tree)\n\nAgora podemos calcular os indicadores nos dados de teste, para isto, utilizamos a função last_fit() que automaticamente reconhece o dataset de teste no objeto split\n\nfinal_tree_fit &lt;- final_tree_wf %&gt;% \n  last_fit(split)\n\nfinal_tree_fit %&gt;% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.567 Preprocessor1_Model1\n2 roc_auc  binary         0.632 Preprocessor1_Model1\n\nfinal_tree_fit %&gt;% \n  collect_predictions() %&gt;% \n  roc_curve(has_churned, .pred_0) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\nUma vez finalizado o teste do modelo, podemos visualizar a arvore utilizando a função rpart.plot() do mesmo pacote.\n\nlibrary(rpart.plot)\n\n\n\nfinal_tree_fit %&gt;% \n  extract_fit_engine() %&gt;% \n  rpart.plot()\n\n\n\n\n\n\n\n\nE também podemos visualizar as variáveis mais importantes utilizando o pacote vip e a função vip():\n\nlibrary(vip)\n\nfinal_tree_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()"
  },
  {
    "objectID": "Tutorial4.html",
    "href": "Tutorial4.html",
    "title": "Tutorial 4 - Métodos Não Supervisionados",
    "section": "",
    "text": "Neste tutorial vamos aprender dois métodos de machine learning que servem para problemas de agrupamento. Neste tipo de problemas, não temos uma variável de saída conhecida, portanto não podemos usar os métodos supervisionados já conhecidos.\nVocê pode baixar os dados aqui."
  },
  {
    "objectID": "Tutorial4.html#k-means",
    "href": "Tutorial4.html#k-means",
    "title": "Tutorial 4 - Métodos Não Supervisionados",
    "section": "K-means",
    "text": "K-means\nVamos aprender inicialmente o método conhecido como ‘k-means’ que basicamente agrupa as observações por uma métrica de similaridade e distância e logo cria os grupos a partir da aproximação entre elas, ou seja, aquelas observações com distância pequena entre elas farão parte do mesmo grupo.\n\nCarregando os pacotes:\n\nlibrary(devtools)\n#install_github(\"thomasp85/patchwork\")\n#install.packages(\"skimr\")\n#install.packages('ggforce')\n#install.packages(\"factoextra\")\n#install.packages(\"FactoMineR\")\n\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(skimr)\nlibrary(ggforce)\nlibrary(factoextra)\nlibrary(FactoMineR)\n\ntheme_set(theme_minimal())\n\n\nminhascores &lt;- c(\"#9b5de5\",\"#f15bb5\",\"#fee440\",\"#00bbf9\",\"#00f5d4\",\"#264653\",\"#2A9D8F\",\"#E9C46A\",\"#F4A261\",\n                 \"#E76F51\")\n\nVamos usar o dataset ws_customers.rds\n\ncustomers &lt;- readRDS(\"data5/ws_customers.rds\")\n\nhead(customers)\n\n   Milk Grocery Frozen\n1 11103   12469    902\n2  2013    6550    909\n3  1897    5234    417\n4  1304    3643   3045\n5  3199    6986   1455\n6  4560    9965    934\n\n\nVamos plotar os dados:\n\np1 &lt;- customers %&gt;% \n  ggplot(aes(Milk, Grocery))+\n  geom_point(size=2, color=minhascores[1])\n\np2 &lt;- customers %&gt;% \n  ggplot(aes(Milk, Frozen))+\n  geom_point(size=2, color=minhascores[2])\n\np3 &lt;- customers %&gt;% \n  ggplot(aes(Frozen, Grocery))+\n  geom_point(size=2, color=minhascores[5])\n\n\np1 + p2 + p3\n\n\n\n\n\n\n\n\nPodemos ver a estrutura dos dados utilizando a skim() do pacote skimr:\n\nskim(customers)\n\n\nData summary\n\n\nName\ncustomers\n\n\nNumber of rows\n45\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nMilk\n0\n1\n4830.67\n5684.39\n333\n1375\n2335\n5302\n25071\n▇▁▂▁▁\n\n\nGrocery\n0\n1\n7830.00\n6620.53\n1330\n2743\n5332\n10790\n26839\n▇▃▂▁▁\n\n\nFrozen\n0\n1\n2869.60\n3523.56\n264\n824\n1455\n3046\n15601\n▇▁▁▁▁\n\n\n\n\n\nAgora podemos começar o tratamento dos dados.\n\ncustomers_scaled &lt;- scale(customers)\n\nRodando o algoritmo k-means:\n\nmodelo &lt;- kmeans(customers_scaled, centers = 3)\n\n\nmodelo\n\nK-means clustering with 3 clusters of sizes 35, 5, 5\n\nCluster means:\n        Milk    Grocery     Frozen\n1 -0.4458543 -0.4268538  0.1375719\n2  0.9889423  2.2234764 -0.4299061\n3  2.1320377  0.7645003 -0.5330972\n\nClustering vector:\n [1] 3 1 1 1 1 1 1 1 2 1 1 2 3 2 1 1 3 1 1 2 1 1 3 1 1 1 3 1 1 1 1 1 1 1 1 1 1 2\n[39] 1 1 1 1 1 1 1\n\nWithin cluster sum of squares by cluster:\n[1] 51.585843  3.715314  5.097218\n (between_SS / total_SS =  54.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\npronto, o modelo foi construido, agora vamos incorporar uma coluna no nosso datafram customers_df com o resultado do cluster.\n\ncustomers &lt;- customers %&gt;% \n  mutate(cluster = factor(modelo$cluster))\n\nskim(customers)\n\n\nData summary\n\n\nName\ncustomers\n\n\nNumber of rows\n45\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ncluster\n0\n1\nFALSE\n3\n1: 35, 2: 5, 3: 5\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nMilk\n0\n1\n4830.67\n5684.39\n333\n1375\n2335\n5302\n25071\n▇▁▂▁▁\n\n\nGrocery\n0\n1\n7830.00\n6620.53\n1330\n2743\n5332\n10790\n26839\n▇▃▂▁▁\n\n\nFrozen\n0\n1\n2869.60\n3523.56\n264\n824\n1455\n3046\n15601\n▇▁▁▁▁\n\n\n\n\n\nVamos refazer os gráficos, só que agora vamos colorir os pontos pelo cluster a qual pertencem.\n\np1 &lt;- customers %&gt;% \n  ggplot(aes(Milk, Grocery, color=cluster))+\n  geom_point(size=2)+\n  scale_color_manual(values=minhascores)+\n  theme(legend.position = \"none\")\n\np2 &lt;- customers %&gt;% \n  ggplot(aes(Milk, Frozen, color=cluster))+\n  geom_point(size=2)+\n  scale_color_manual(values=minhascores)+\n  theme(legend.position = \"none\")\n\np3 &lt;- customers %&gt;% \n  ggplot(aes(Frozen, Grocery, color=cluster))+\n  geom_point(size=2)+\n  scale_color_manual(values=minhascores)+\n  theme(legend.position = \"none\")\n\np1+p2+p3\n\n\n\n\n\n\n\n\n\np1cluster &lt;- p1 + \n  geom_mark_ellipse(aes(color=cluster))\n\n\np2cluster &lt;- p2 + \n  geom_mark_ellipse(aes(color=cluster))\n\np3cluster &lt;- p3 + \n  geom_mark_ellipse(aes(color=cluster))\n  \n  p1cluster+p2cluster+p3cluster\n\n\n\n\n\n\n\n\n\nEscolhendo o número ideal de clusters\n\nfviz_nbclust(customers, kmeans, method = \"wss\")\n\n\n\n\n\n\n\n\nEscolhe-se o ponto onde o ‘cotovelo’ é mais visível, neste caso, 2 clusters.\n\nmodelo_final &lt;- kmeans(customers_scaled, centers = 2)\n\ncustomers &lt;- customers %&gt;% \n  mutate(cluster = factor(modelo_final$cluster))\n\nVamos refazer os gráficos mais uma vez.\n\np1 &lt;- customers %&gt;% \n  ggplot(aes(Milk, Grocery, color=cluster))+\n  geom_point(size=2)+\n  scale_color_manual(values=minhascores)+\n  theme(legend.position = \"none\")\n\np2 &lt;- customers %&gt;% \n  ggplot(aes(Milk, Frozen, color=cluster))+\n  geom_point(size=2)+\n  scale_color_manual(values=minhascores)+\n  theme(legend.position = \"none\")\n\np3 &lt;- customers %&gt;% \n  ggplot(aes(Frozen, Grocery, color=cluster))+\n  geom_point(size=2)+\n  scale_color_manual(values=minhascores)+\n  theme(legend.position = \"none\")\n\np1cluster &lt;- p1 + \n  geom_mark_ellipse(aes(color=cluster))\n\n\np2cluster &lt;- p2 + \n  geom_mark_ellipse(aes(color=cluster))\n\np3cluster &lt;- p3 + \n  geom_mark_ellipse(aes(color=cluster))\n  \n  p1cluster+p2cluster+p3cluster\n\n\n\n\n\n\n\n\nUma melhor forma de gráficar é utilizar a redução de dimensionalidade.\n\nmodelo_final &lt;- eclust(customers_scaled, \"kmeans\", k=2)"
  },
  {
    "objectID": "Tutorial4.html#cluster-hierárquico",
    "href": "Tutorial4.html#cluster-hierárquico",
    "title": "Tutorial 4 - Métodos Não Supervisionados",
    "section": "Cluster Hierárquico",
    "text": "Cluster Hierárquico\nA diferença com o método anterior é que para o cluster hierárquico não é necessário pré-definir o número de clusters, pois o método agrupa as observações de forma sequencial, facilitando a identificação do número ideal de cluster.\n\nO ideal é que os dados estejam normalizados, pelo menos as colunas numéricas, pois este método aceita também colunas (atributos) categóricos.\nVamos usar a função eclust() do pacote factoextra.\n\nmodelo2 &lt;- eclust(customers_scaled, \"hclust\")\n\n\nfviz_dend(modelo2)"
  },
  {
    "objectID": "Tutorial4.html#análise-dos-componentes-principais",
    "href": "Tutorial4.html#análise-dos-componentes-principais",
    "title": "Tutorial 4 - Métodos Não Supervisionados",
    "section": "Análise dos Componentes Principais",
    "text": "Análise dos Componentes Principais\nA Análise de Componentes Principais (PCA, do inglês “Principal Component Analysis”) é uma técnica estatística usada para reduzir a dimensionalidade de um conjunto de dados, ao mesmo tempo em que preserva a maior quantidade possível de variação presente nesses dados. A PCA consegue isso transformando o conjunto original de variáveis em um novo conjunto de variáveis ortogonais (ou seja, não correlacionadas) chamadas de componentes principais. Esses componentes são ordenados de tal forma que o primeiro componente captura a maior parte da variação nos dados, o segundo componente captura a maior parte da variação restante, e assim por diante.\n\ncustomers &lt;- readRDS(\"data5/ws_customers.rds\")\n\nmodelo_pca &lt;- PCA(customers, graph = FALSE)\n\nAgora podemos aplicar vários tipos de visualizações do pacote factoExtra.\n\nfviz_screeplot(modelo_pca, ncp=10)\n\n\n\n\n\n\n\n\n\nfviz_screeplot(modelo_pca, ncp=10,\n               barfill = minhascores[4],\n               barcolor = minhascores[4])\n\n\n\n\n\n\n\n\n\nfviz_pca_var(modelo_pca)\n\n\n\n\n\n\n\n\n\nfviz_pca_var(modelo_pca,\n             col.var=\"contrib\")+\n  scale_color_gradient2(low=minhascores[1],\n                        mid=minhascores[2],\n                        high=minhascores[3],\n                        midpoint = 34)\n\n\n\n\n\n\n\n\n\ncustomers &lt;- customers %&gt;% \n  mutate(cluster=factor(modelo2$cluster))\n\nfviz_pca_ind(modelo_pca, \n             label=\"none\",\n             habillage = customers$cluster,\n             addEllipses = TRUE)+\n  scale_fill_manual(values = minhascores)+\n  scale_color_manual(values=minhascores)\n\n\n\n\n\n\n\n\n\nfviz_pca_biplot(modelo_pca,\n  label=\"var\",\n             habillage = customers$cluster,\n             addEllipses = TRUE)+\n  scale_fill_manual(values = minhascores)+\n  scale_color_manual(values=minhascores)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\nlibrary(fs)\nlibrary(stringr)\nrmd_names &lt;- dir_ls(path = \".\", glob = \"*.Rmd\")\nqmd_names &lt;- str_replace(string = rmd_names,\n                         pattern = \"Rmd\",\n                         replacement = \"qmd\")\nfile_move(path = rmd_names,\n          new_path = qmd_names)\n\n\n\n\n\n\nPhoto by Jaime Lopes on unsplash"
  },
  {
    "objectID": "Tutorial5.html",
    "href": "Tutorial5.html",
    "title": "Tutorial 6 - Métodos de Classificação simplificados",
    "section": "",
    "text": "Vamos usar um dataset sobre churn.\n\n\nVocê pode baixar os dados aqui.\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(fst)"
  },
  {
    "objectID": "Tutorial5.html#carregando-os-pacotes",
    "href": "Tutorial5.html#carregando-os-pacotes",
    "title": "Tutorial 6 - Métodos de Classificação simplificados",
    "section": "",
    "text": "Vamos usar um dataset sobre churn.\n\n\nVocê pode baixar os dados aqui.\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(fst)"
  },
  {
    "objectID": "Tutorial5.html#regressão-logística",
    "href": "Tutorial5.html#regressão-logística",
    "title": "Tutorial 6 - Métodos de Classificação simplificados",
    "section": "Regressão Logística",
    "text": "Regressão Logística\n\nCarregando o dataset\n\nchurn &lt;- read_fst(\"data6/churn.fst\")\n\nchurn$has_churned &lt;- as_factor(churn$has_churned)\n\nVamos fazer uma análise da estrutura dos dados:\n\nglimpse(churn)\n\nRows: 400\nColumns: 3\n$ has_churned               &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ time_since_first_purchase &lt;dbl&gt; -1.08922097, 1.18298297, -0.84615637, 0.0869…\n$ time_since_last_purchase  &lt;dbl&gt; -0.72132150, 3.63443539, -0.42758226, -0.535…\n\nsummary(churn)\n\n has_churned time_since_first_purchase time_since_last_purchase\n 0:200       Min.   :-1.27377          Min.   :-0.8707         \n 1:200       1st Qu.:-0.82838          1st Qu.:-0.6458         \n             Median :-0.15207          Median :-0.2650         \n             Mean   :-0.03437          Mean   : 0.1445         \n             3rd Qu.: 0.54483          3rd Qu.: 0.5712         \n             Max.   : 3.73831          Max.   : 5.9282         \n\n\nA regressão logística tem a propriedade de predizer valores entre o intervalo 0 e 1 (pense nesses valores como a probabilidades de churn de 0 a 100%).\n\n\nAnálise Exploratória de Dados\nUma breve EDA mostra a relação entre os tempos de primeira e última compras com o churn de clientes.\n\nchurn %&gt;%\nggplot(aes(time_since_first_purchase, fill=has_churned,\n          color=has_churned))+\ngeom_density(alpha = 0.5)+\nlabs(title = \"O tempo desde a primeira compra é maior para clientes perdidos\")+\ntheme_light()\n\n\n\n\n\n\n\n\nJá para o tempo desde a última compra:\n\nchurn %&gt;%\nggplot(aes(time_since_last_purchase, fill=has_churned,\n          color=has_churned))+\ngeom_density(alpha = 0.5)+\nlabs(title = \"Clientes perdidos demoram mais tempo para voltar a comprar\")+\ntheme_light()\n\n\n\n\n\n\n\n\n\n\nModelo\nO primeiro passo será separar os dados em treino e teste.\n\nset.seed(1212)\n\nsplit &lt;- initial_split(churn, prop = 0.7, strata = has_churned)\n\ntrain &lt;- training(split)\ntest &lt;- testing(split)\n\nPara treinar um modelo de regressão logística, usamos a função glm() e o argumento family = \"binomial\"\n\nlogmodel &lt;- glm(has_churned ~ ., data = train, family = \"binomial\")\n\nlogmodel\n\n\nCall:  glm(formula = has_churned ~ ., family = \"binomial\", data = train)\n\nCoefficients:\n              (Intercept)  time_since_first_purchase  \n                  -0.1129                    -0.7370  \n time_since_last_purchase  \n                   0.5950  \n\nDegrees of Freedom: 279 Total (i.e. Null);  277 Residual\nNull Deviance:      388.2 \nResidual Deviance: 358.6    AIC: 364.6\n\n\nPara se ter uma noção sobre a curva de ajuste, a regressão logística segue o formato de uma curva em “S” onde as probabilidades (neste caso de churn) sempre estarão entre 0 e 1\n\ncurva &lt;- churn %&gt;%\nggplot(aes(time_since_last_purchase, has_churned))+\ngeom_point()+\nscale_x_continuous(limits = c(-20,20))\n\ncurva\n\n\n\n\n\n\n\n\nadicionamos uma camada: stat_smooth(method=\"glm\", method.args=list(family=\"binomial\"), fullrange = TRUE, se = FALSE) ao ggplot:\n\ncurva +\n stat_smooth(method=\"glm\", \n             method.args=list(family=\"binomial\"), \n             fullrange = TRUE, \n             se = FALSE)\n\n\n\n\n\n\n\n\nPara predizer valores a partir de novos dados de time_since_last_purchase e time_since_first_purchase deve-se usar a função predict(). Podemos criar um novo dataframe denominado churn_df_log e incluir uma nova coluna com predict() dentro do mutate()\n\npred &lt;- predict(logmodel,\n                   test,\n                   type = \"response\")\n\nchurn_df_log &lt;- test %&gt;%\n    mutate(pred_log = pred)\n\nAgora vamos plotar a curva ROC e calcular a área abaixo da curva ROC, conhecida como AUC.\n\n# Plotar a curva ROC com autoplot\nautoplot(roc_curve(churn_df_log, has_churned, pred_log))\n\n\n\n\n\n\n\n# Calcular AUC usando a função roc_auc\nroc_auc(churn_df_log, has_churned, pred_log)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.397"
  },
  {
    "objectID": "Tutorial5.html#árvores-de-decisão",
    "href": "Tutorial5.html#árvores-de-decisão",
    "title": "Tutorial 6 - Métodos de Classificação simplificados",
    "section": "Árvores de Decisão",
    "text": "Árvores de Decisão\nÁrvores de Decisão, ou Decision Trees, são algoritmos de machine learning largamente utilizados, com uma estrutura de simples compreensão e que costumam apresentar bons resultados em suas previsões.\nEles também são a base do funcionamento de outros poderosos algoritmos, como o Random Forest, por exemplo.\nAs decision trees estão entre os primeiros algoritmos aprendidos por iniciantes no mundo do aprendizado de máquina.\nMais detalhes sobre como funciona em: didatica.tech\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\ntree_model &lt;- rpart(has_churned ~.,\n             data = train,\n             method = \"class\")\n\n\nrpart.plot(tree_model)\n\n\n\n\n\n\n\n\nAgora que a árvore de decisão foi criada, podemos verificar a sua precisão comparando as previsões com a coluna original.\n\npred_tree &lt;- predict(tree_model,\n                    test,\n                    type = \"class\")\n\n\nchurn_df_tree &lt;- test %&gt;%\n    mutate(pred_tree = pred_tree)\n\nAntes de continuar, precisamos converter a coluna has_churned para factor e a coluna pred_tree para numeric\n\n#convertendo has_churned para factor\n\nchurn_df_tree$has_churned &lt;- factor(churn_df_tree$has_churned, levels = c(0,1))\n\n#convertendo pred_tree para numeric\nchurn_df_tree$pred_tree &lt;- as.character(churn_df_tree$pred_tree)\nchurn_df_tree$pred_tree &lt;- as.numeric(churn_df_tree$pred_tree)\n\nAgora sim, podemos plotar a curva ROC\n\nautoplot(roc_curve(churn_df_tree, truth = has_churned, pred_tree))\n\n\n\n\n\n\n\n\nE a AUC:\n\nroc_auc(churn_df_tree, truth = has_churned, pred_tree)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.442"
  },
  {
    "objectID": "Tutorial5.html#random-forests",
    "href": "Tutorial5.html#random-forests",
    "title": "Tutorial 6 - Métodos de Classificação simplificados",
    "section": "Random Forests",
    "text": "Random Forests\nEm português, Random Forest significa floresta aleatória. Este nome explica muito bem o funcionamento do algoritmo.\nEm resumo, o Random Forest irá criar muitas árvores de decisão, de maneira aleatória, formando o que podemos enxergar como uma floresta, onde cada árvore será utilizada na escolha do resultado final, em uma espécie de votação. Mais detalhes em: didatica.tech.\nVamos carregar o pacote ranger que serve para treinar Random Forests\n\nlibrary(ranger)\n\n\n# treinar um modelo usando o pacote `ranger`\n\nrf_model &lt;- ranger(has_churned ~ ., \n                              data = train,\n                              num.trees = 500,\n                       classification = TRUE)\n\nUma vez que o modelo foi treinado, vamos calcular o indicadores de acurácia utilizando os procedimentos já vistos. Porém, há um passo intermediário que serve para extrair as predições do objeto criado pelo predict() do ranger.\n\nrf_predict &lt;- predict(rf_model, data = test, type = \"response\")$predictions\n\nAgora sim, podemos utilizar o código dos modelos anteriores, adaptando-o ligeiramente para incluir o vetor criado acima com os valores preditos.\n\nchurn_df_rf &lt;- test %&gt;%\n    mutate(pred_rf = rf_predict)\n\nComo no caso anterior, a coluna has_churned precisa estar no formato factor, e a coluna pred_rf no formato numeric.\n\n#convertendo has_churned para factor\n\nchurn_df_rf$has_churned &lt;- factor(churn_df_rf$has_churned, levels = c(0,1))\n\n#convertendo pred_rf para numeric\nchurn_df_rf$pred_rf &lt;- as.character(churn_df_rf$pred_rf)\nchurn_df_rf$pred_rf &lt;- as.numeric(churn_df_rf$pred_rf)\n\nAgora sim, podemos plotar a curva ROC\n\nautoplot(roc_curve(churn_df_rf, truth = has_churned, pred_rf))\n\n\n\n\n\n\n\n\n\nroc_auc(churn_df_rf, truth = has_churned, pred_rf)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.417"
  },
  {
    "objectID": "Lab_gabaritos/Lab1_gabarito.html",
    "href": "Lab_gabaritos/Lab1_gabarito.html",
    "title": "Lab 1 - EDA",
    "section": "",
    "text": "Carregando os dados\nNeste Lab iremos explorar os dados sobre seriados de TV, usando dois datasets, um do Netflix (informações do viewing history) e outro do IMDb, banco de dados com informações sobre o rating de seriados e filmes.\nInicialmente vamos carregar os dados. Os dados do Netflix vem do site onde o usuário pode fazer download do seu histórico após o login. Já os dados o IMDb usam um script que encontra-se em: https://github.com/nazareno/imdb-series para fazer o web scrapping do site oficial do IMDb.\nCarregando os dados do IMDb e mostrando os primeiras 10 linhas:\n\nlibrary(tidyverse)\nlibrary(readxl)\nimdb &lt;- read_xlsx(\"imdb_series.xlsx\")\n\nhead(imdb, 10)\n\nCarregando os dados do Netflix e mostrando as primeiras 10 linhas:\n\nnetflix &lt;- read.csv(\"NetflixViewingHistory.csv\")\n\nhead(netflix, 10)\n\nObserve-se que os dados de Date não estão no formato certo, pois eles aparecem como character o que pode causar problemas posteriores. Portanto antes de continuar, precisamos arrumar o formato para “data” com os comandos a seguir:\n\nnetflix$Date &lt;- as.Date(netflix$Date, \"%m/%d/%y\")\n\nglimpse(netflix)\n\nComo pode se ver, a estrutura dos dados do IMDb é mais complexa, vamos ver mais de perto os dados, usando a função str():\n\nstr(imdb)\n\nglimpse(imdb)\n\nNos dados do IMDb, as últimas 10 colunas representam a proporção de usuários que votou o capítulo específico com a nota n, sendo que, por exemplo, a coluna r9 representa a proporção de usuários que votou com a nota 9.\n\n\nLimpando os dados do Netflix\nPrimeiro, vamos explorar mais os dados do Netflix, que conta com apenas duas colunas: o Título e a data de visualização. Para isto, vamos filtrar o dataset original para ficarmos apenas com os seriados dos filmes, usando os comandos do Tidyverse: filter , str_detect e separate.\n\nnetflix_series &lt;- netflix %&gt;% \n  filter(str_detect(Title, \":\")) %&gt;% \n  separate(Title,c(\"series_title\",\"season\",\"episode\"),\":\")\n\nCom os comandos anteriores conseguimos ter uma base limpa, onde apenas consideramos seriados e temos as informações separadas para cada capítulo e temporada. Porém, pode ser que o nosso processo de filtragem com str_detect() não tenha sido perfeito.\nVamos fazer um teste verificando se existem registros com o campo episode com NA:\n\nsum(is.na(netflix_series$episode))\n\nnas &lt;- sum(is.na(netflix_series$episode))\nnas &lt;- as.character(nas)\n\nOk, identificamos xx registros que apresentam NA, vamos explorá-los com mais cuidado para confirmar se de fato se trata de um problema do nosso filtrado inicial. E se de fato encontrarmos registros mal filtrados, faremos a limpeza manual.\n\nnetflix_series %&gt;% \n  filter(is.na(episode))\n\nNa verdade esses xx registros são de fato filmes com subtítulos (por isso o nosso filtro com str_detect() os pegou. Devemos retirá-los da nossa base de seriados.\n\nnetflix_series_limpo &lt;- netflix_series %&gt;% \n  filter(!is.na(episode))\n\n\n\nExplorando as séries de Netflix\nAgora, podemos iniciar algumas visualizações, por exemplo, podemos fazer uma tabela que nos indique o número de capítulos assistidos por cada seriado. Para isto, vamos usar os comandos do dplyr:\n\nnetflix_series_limpo %&gt;% \n  group_by(series_title) %&gt;% \n  count() %&gt;% \n  arrange(desc(n)) %&gt;% \n  ungroup() %&gt;% \n  top_n(10, wt=n)\n\nUma outra forma de fazer esta mesma análise seria filtrar por seriados onde mais de x capítulos foram assistidos, por exemplo, seriados com mais de 20 capítulos assistidos:\n\nnetflix_series_limpo %&gt;% \n  group_by(series_title) %&gt;% \n  summarize(assistidos = n())%&gt;% \n  filter(assistidos &gt; 19) %&gt;% \n  arrange(desc(assistidos))\n\nTambém podemos fazer uma análise do número de capítulos assistidos em um ano específico, por exemplo, em 2020:\n\nseries_mais_assistidas &lt;- netflix_series_limpo %&gt;% \n  mutate(ano = as.numeric(format(Date,'%Y'))) %&gt;% \n  filter(ano == \"2020\") %&gt;% \n  group_by(series_title) %&gt;% \n  tally() \n\nseries_mais_assistidas %&gt;% \n  ggplot(aes(reorder(series_title,-n), n))+\n  geom_bar(stat=\"identity\")+\n  coord_flip()+\n  labs(title = \"Seriados mais assistidos em 2020\")\n\nPodemos ver que os seriados mais assistidos foram “Greenleaf”, “How to get away with murder”, “Once upon a Time”, “Away”, “Bates Motel”, “Dark”, “Money Heist” e “Ratched”.\nTambém podemos ver o número de capítulos totais assistidos por ano:\n\nnetflix_series_limpo %&gt;% \n  mutate(ano = as.numeric(format(Date,'%Y'))) %&gt;% \n  group_by(ano) %&gt;% \n  tally() %&gt;% \n  ggplot(aes(factor(ano), n))+\n  geom_bar(stat=\"identity\", fill=\"#e32d91\")\n\nnetflix_series_limpo %&gt;% \n  mutate(ano = as.numeric(format(Date,'%Y'))) %&gt;% \n  group_by(ano) %&gt;% \n  tally() %&gt;% \n  ggplot(aes(ano,n))+\n  geom_line(color = \"#e32d91\", size = 1.2)\n\nE também ver a distribuição dos seriados por ano, por exemplo para o seriado “Bates Motel”:\n\nnetflix_series_limpo %&gt;% \n  mutate(ano = as.numeric(format(Date,'%Y'))) %&gt;% \n  group_by(series_title, ano) %&gt;% \n  summarize(assistidos = n()) %&gt;% \n  filter(series_title == \"Bates Motel\") %&gt;% \n  ggplot(aes(ano, assistidos))+ \n  geom_bar(stat=\"identity\", fill = \"#5b59d6\")+\n  labs(title=\"Bates Motel - capítulos assistidos por ano\")\n\nOu ver a distribuição por data:\n\nnetflix_series_limpo %&gt;% \n  #filter(series_title == \"Bates Motel\") %&gt;%\n  count(Date) %&gt;% \n  ggplot(aes(Date, n))+\n  geom_col(color = c(\"#f16727\"))\n\nFinalmente, podemos ver a distribuição dos seriados num ano específico:\n\nnetflix_series_limpo %&gt;% \n  filter(Date &gt;= \"2020-01-01\") %&gt;%\n  group_by(series_title) %&gt;% \n  ggplot(aes(Date, fill = series_title))+\n  geom_bar()+\n  labs(title = \"Capítulos assistidos por seriado em 2020\")\n\n\n\nExplorando os dados do IMDb\nAgora vamos iniciar a análise dos dados do IMDB. Primeiro vamos contabilizar o número de capítulos por seriado e vamos ordenar usando a função arrange\n\nimdb %&gt;% \n  group_by(series_name) %&gt;% \n  count() %&gt;% \n  arrange(desc(n))\n\nClaramente têm seriados com muitos capítulos e outros com poucos, pode ser por causa do número de temporadas? vamos conferir:\n\nimdb %&gt;% \n  group_by(series_name, season) %&gt;% \n  count() %&gt;% \n  ungroup() %&gt;% \n  group_by(series_name) %&gt;% \n  top_n(1, wt = season) %&gt;% \n  arrange(desc(season))\n\nSerá que há relação entre o número de capítulos e o número de temporadas?\n\nimdb %&gt;% \n  group_by(series_name, season) %&gt;% \n  count() %&gt;% \n  ungroup() %&gt;% \n  group_by(series_name) %&gt;% \n  top_n(1, wt = season) %&gt;% \n  arrange(desc(season)) %&gt;% \n  ggplot(aes(season, n))+\n  geom_point()+\n  geom_label(aes(label = series_name))\n\npodemos ampliar a escala usando transformação log:\n\nimdb %&gt;% \n  group_by(series_name, season) %&gt;% \n  count() %&gt;% \n  ungroup() %&gt;% \n  group_by(series_name) %&gt;% \n  top_n(1, wt = season) %&gt;% \n  arrange(desc(season)) %&gt;% \n  ggplot(aes(season, n))+\n  geom_point()+\n  geom_label(aes(label = series_name))+\n  scale_x_continuous(trans = \"log2\")+\n  scale_y_continuous(trans = \"log2\")\n\nQuais são as séries com as avaliações mais altas e baixas em média?\n\nimdb_rating &lt;- imdb %&gt;% \n  group_by(series_name) %&gt;% \n  summarize(media = mean(UserRating)) %&gt;% \n  arrange(desc(media)) \n\nimdb_rating\n\nSe compararmos estes resultados com os obtidos acima, referentes aos seriados mais assistidos no Netflix (“Greenleaf”, “How to get away with murder”, “Once upon a Time”, “Away”, “Bates Motel”, “Dark”, “Money Heist” e “Ratched”), há alguma relação entre a média de avaliação e o número de capítulos assistidos?\n\nseries_mais_assistidas_total &lt;- netflix_series_limpo %&gt;% \n  group_by(series_title) %&gt;% \n  count()\n\nseries_mais_assistidas_com_rating &lt;- imdb_rating %&gt;% \n  left_join(series_mais_assistidas_total, by=c(\"series_name\"=\"series_title\"))\nseries_mais_assistidas_com_rating\n\nE se realizarmos um gráfico de dispersão?\n\nseries_mais_assistidas_com_rating %&gt;% \n  ggplot(aes(n, media))+\n  geom_point()+\n  geom_label(aes(label=series_name))"
  },
  {
    "objectID": "Lab5.html",
    "href": "Lab5.html",
    "title": "Lab 5 - Tidymodels - Regressão",
    "section": "",
    "text": "Carregando os dados\nEste webinar usará dados do Ikea, uma loja de móveis com filiais em várias partes do mundo.\nO próposito é prever o preço dos móveis vendidos na IKEA a partir de várias características destes produtos como a categoria e o tamanho do móvel, conforme aqui.\nVamos carregar os dados e ver as primeiras linhas\n\nlibrary(tidyverse)\n\nikea &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-11-03/ikea.csv\")\n\nhead(ikea)\n\n# A tibble: 6 × 14\n   ...1  item_id name             category price old_price sellable_online link \n  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;     &lt;lgl&gt;           &lt;chr&gt;\n1     0 90420332 FREKVENS         Bar fur…   265 No old p… TRUE            http…\n2     1   368814 NORDVIKEN        Bar fur…   995 No old p… FALSE           http…\n3     2  9333523 NORDVIKEN / NOR… Bar fur…  2095 No old p… FALSE           http…\n4     3 80155205 STIG             Bar fur…    69 No old p… TRUE            http…\n5     4 30180504 NORBERG          Bar fur…   225 No old p… TRUE            http…\n6     5 10122647 INGOLF           Bar fur…   345 No old p… TRUE            http…\n# ℹ 6 more variables: other_colors &lt;chr&gt;, short_description &lt;chr&gt;,\n#   designer &lt;chr&gt;, depth &lt;dbl&gt;, height &lt;dbl&gt;, width &lt;dbl&gt;\n\n\n\n\nLimpeza dos dados\nTambém vamos fazer uma revisão geral dos dados\n\nlibrary(skimr)\n\nskim(ikea)\n\n\nData summary\n\n\nName\nikea\n\n\nNumber of rows\n3694\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\nlogical\n1\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1\n3\n27\n0\n607\n0\n\n\ncategory\n0\n1\n4\n36\n0\n17\n0\n\n\nold_price\n0\n1\n4\n13\n0\n365\n0\n\n\nlink\n0\n1\n52\n163\n0\n2962\n0\n\n\nother_colors\n0\n1\n2\n3\n0\n2\n0\n\n\nshort_description\n0\n1\n3\n63\n0\n1706\n0\n\n\ndesigner\n0\n1\n3\n1261\n0\n381\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nsellable_online\n0\n1\n0.99\nTRU: 3666, FAL: 28\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\n…1\n0\n1.00\n1846.50\n1066.51\n0\n923.25\n1846.5\n2769.75\n3693\n▇▇▇▇▇\n\n\nitem_id\n0\n1.00\n48632396.79\n28887094.10\n58487\n20390574.00\n49288078.0\n70403572.75\n99932615\n▇▇▇▇▇\n\n\nprice\n0\n1.00\n1078.21\n1374.65\n3\n180.90\n544.7\n1429.50\n9585\n▇▁▁▁▁\n\n\ndepth\n1463\n0.60\n54.38\n29.96\n1\n38.00\n47.0\n60.00\n257\n▇▃▁▁▁\n\n\nheight\n988\n0.73\n101.68\n61.10\n1\n67.00\n83.0\n124.00\n700\n▇▂▁▁▁\n\n\nwidth\n589\n0.84\n104.47\n71.13\n1\n60.00\n80.0\n140.00\n420\n▇▅▂▁▁\n\n\n\n\n\nConforme visto no output do skim() há várias colunas em formato de “character” e uma coluna com nome X1 que apenas é um id de cada linha. Precisamos limpar o dataset para deixá-lo mais adequado ao modelo de machine learning.\n\nikea_df &lt;- ikea %&gt;% \n  select(-1,-2, -link, -short_description, -old_price,\n         -sellable_online) %&gt;% \n  mutate_if(is.character, as.factor)\n\n\n\nAnálise exploratória de dados - EDA\n\nikea_df %&gt;%\n  select(price, depth:width) %&gt;%\n  pivot_longer(depth:width, names_to = \"dim\") %&gt;%\n  ggplot(aes(value, price, color = dim)) +\n  geom_point(alpha = 0.4, show.legend = FALSE) +\n  geom_smooth(method = \"lm\", se = FALSE)+\n  scale_y_log10() +\n  facet_wrap(~dim, scales = \"free_x\") +\n  labs(x = NULL)\n\n\n\n\n\n\n\n\nVamos deixar o preço com log(10)\n\nikea_df &lt;- ikea_df %&gt;% \n  mutate(price=log10(price),\n         category=fct_lump(category, prop = 0.05),\n         designer=fct_lump(designer, prop=0.05),\n         name=fct_lump(name, prop=0.02))\n\nskim(ikea_df)\n\n\nData summary\n\n\nName\nikea_df\n\n\nNumber of rows\n3694\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nname\n0\n1\nFALSE\n6\nOth: 3179, BES: 173, PAX: 111, GRÖ: 83\n\n\ncategory\n0\n1\nFALSE\n10\nTab: 612, Boo: 548, Oth: 483, Cha: 481\n\n\nother_colors\n0\n1\nFALSE\n2\nNo: 2182, Yes: 1512\n\n\ndesigner\n0\n1\nFALSE\n2\nOth: 2866, IKE: 828\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nprice\n0\n1.00\n2.67\n0.65\n0.48\n2.26\n2.74\n3.16\n3.98\n▁▂▅▇▃\n\n\ndepth\n1463\n0.60\n54.38\n29.96\n1.00\n38.00\n47.00\n60.00\n257.00\n▇▃▁▁▁\n\n\nheight\n988\n0.73\n101.68\n61.10\n1.00\n67.00\n83.00\n124.00\n700.00\n▇▂▁▁▁\n\n\nwidth\n589\n0.84\n104.47\n71.13\n1.00\n60.00\n80.00\n140.00\n420.00\n▇▅▂▁▁\n\n\n\n\n\nVamos fazer a análise das correlações\n\nlibrary(GGally)\n\nikea_df %&gt;%\n  select(\n    price, other_colors, depth, height, width,\n    category, designer, name\n  ) %&gt;%\n  ggpairs()\n\n\n\n\n\n\n\n\n\n\nModelagem supervisionada usando Tidymodels\nO pacote tidymodels é uma evolução do caret e procura facilitar a construção de modelos de machine learning, seguindo um padrão que independe do modelo a ser construído (regressão linear, árvores de decisão, etc.).\nVamos dividir o dataset em treino e teste:\n\nlibrary(tidymodels)\n\nset.seed(1234)\nikea_split &lt;- initial_split(ikea_df, strata=price)\n\nikea_train &lt;- training(ikea_split)\nikea_test &lt;- testing(ikea_split)\n\nPara aprimorar a robustez das nossas estimações, iremos usar um procedimento denominado k-fold cross validation ou validação cruzada.\n\nikea_fold &lt;- vfold_cv(ikea_train)\n\nA recipe que iremos usar servirá para todos os modelos.\n\nikea_rec &lt;- recipe(price ~ ., data = ikea_train) %&gt;%\n  step_dummy(all_nominal()) %&gt;%\n  step_impute_knn(depth, height, width)\n\nikea_wf &lt;- workflow() %&gt;% \n  add_recipe(ikea_rec)\n\nVamos treinar um modelo de Regressão Linear (usando lm), logo um de Árvore de Decisão (usando rpart) e finalmente um de Random Forest (usando ranger).\n\nlm_spec &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\")\n\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\nrf_spec &lt;- rand_forest(trees=1000) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\")\n\nSegue os resultados do modelo da Regressão Linear:\n\ndoParallel::registerDoParallel()\n\nlm_rs &lt;- ikea_wf %&gt;% \n  add_model(lm_spec) %&gt;% \n  fit_resamples(resamples=ikea_fold,\n                metrics=metric_set(rmse, rsq, mae),\n                control=control_resamples(save_pred=TRUE))\n\ncollect_metrics(lm_rs)\n\n# A tibble: 3 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.340    10 0.00668 Preprocessor1_Model1\n2 rmse    standard   0.453    10 0.00778 Preprocessor1_Model1\n3 rsq     standard   0.506    10 0.0105  Preprocessor1_Model1\n\n\nOs resultados da árvore de decisão:\n\ntree_rs &lt;- ikea_wf %&gt;% \n  add_model(tree_spec) %&gt;% \n  fit_resamples(resamples=ikea_fold,\n                metrics=metric_set(rmse, rsq, mae),\n                control=control_resamples(save_pred=TRUE))\n\ncollect_metrics(tree_rs)\n\n# A tibble: 3 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.341    10 0.00499 Preprocessor1_Model1\n2 rmse    standard   0.453    10 0.00632 Preprocessor1_Model1\n3 rsq     standard   0.506    10 0.0145  Preprocessor1_Model1\n\n\nOs resultados do Random Forest:\n\nrf_rs &lt;- ikea_wf %&gt;% \n  add_model(rf_spec) %&gt;% \n  fit_resamples(resamples=ikea_fold,\n                metrics=metric_set(rmse, rsq, mae),\n                control=control_resamples(save_pred=TRUE))\n\ncollect_metrics(rf_rs)\n\n# A tibble: 3 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   0.241    10 0.00551 Preprocessor1_Model1\n2 rmse    standard   0.331    10 0.00801 Preprocessor1_Model1\n3 rsq     standard   0.751    10 0.00765 Preprocessor1_Model1\n\n\nJuntando os três resultados e comparando:\n\ncollect_metrics(lm_rs) %&gt;% mutate(modelo=\"lm\") %&gt;% rbind(collect_metrics(tree_rs) %&gt;% mutate(modelo=\"tree\")) %&gt;% rbind(collect_metrics(rf_rs) %&gt;% mutate(modelo=\"rf\")) %&gt;% \n  ggplot(aes(modelo, mean, fill=modelo))+\n  geom_col() +\n  facet_wrap(vars(.metric\n                  ), scales = \"free_y\")+\n  scale_fill_viridis_d()+\n  theme(axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nIremos escolher o modelo de Random Forest (menor MAE, menor RMSE e maior R2)\n\nmodelo_final &lt;- ikea_wf %&gt;% \n  add_model(rf_spec) %&gt;% \n  last_fit(ikea_split)\n\ncollect_metrics(modelo_final,\n                metrics = metric_set(rsq, rmse, mae))\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.353 Preprocessor1_Model1\n2 rsq     standard       0.711 Preprocessor1_Model1\n\n\nPor fim, se compararmos os valores estimados e os valores reais:\n\ncollect_predictions(modelo_final) %&gt;%\n  ggplot(aes(price, .pred)) +\n  geom_abline(lty = 2, color = \"gray50\") +\n  geom_point(alpha = 0.5, color = \"#e32d91\") +\n  coord_fixed()\n\n\n\n\n\n\n\n\nPor fim, podemos usar uma função da library(vip) para identificar os atributos mais importantes.\n\nlibrary(vip)\n\nimp_spec &lt;- rf_spec %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\")\n\nikea_wf %&gt;% \n  add_model(imp_spec) %&gt;%\n  fit(ikea_train) %&gt;%\n  pull_workflow_fit() %&gt;%\n  vip(aesthetics = list(alpha = 0.8, fill = \"midnightblue\"))"
  },
  {
    "objectID": "Tutorial4reg.html",
    "href": "Tutorial4reg.html",
    "title": "Tutorial 5 - Métodos de Regressão",
    "section": "",
    "text": "A regressão linear é talvez o método mais conhecido para realizar previsões quando o comportamento dos dados remete a uma forma linear. Para mostrar a utilidade deste método iremos usar três datasets, Salary.csv, GPA.csv e real_estate_price_size.\n\n\nVocê pode baixar os dados aqui.\n\n\n\nlibrary(tidyverse)\nlibrary(broom) #para manipular objetos de regressão linear\nlibrary(tidymodels) #para facilitar a manipulação de modelos de ML\n\nset.seed(1234)\n\n\n\nVamos ajustar a pasta com que iremos trabalhar\nImportando os dados:\n\nsal_exp &lt;- read.csv('data4/Salary_Data.csv')\n\nsal_exp %&gt;% \n  ggplot(aes(YearsExperience, Salary))+\n  geom_point()\n\n\n\n\n\n\n\n\nPara criar uma regressão linear no R, basta usar a função lm().\n\nmodel_sal_exp &lt;- lm(formula = Salary ~ YearsExperience, \n              data = sal_exp)\n\nPara ver as informações sobre a regressão, podemos utilizar a função summary() no objeto de modelo, neste caso, em model_sal_exp.\n\nsummary(model_sal_exp) \n\n\nCall:\nlm(formula = Salary ~ YearsExperience, data = sal_exp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7958.0 -4088.5  -459.9  3372.6 11448.0 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      25792.2     2273.1   11.35 5.51e-12 ***\nYearsExperience   9450.0      378.8   24.95  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5788 on 28 degrees of freedom\nMultiple R-squared:  0.957, Adjusted R-squared:  0.9554 \nF-statistic: 622.5 on 1 and 28 DF,  p-value: &lt; 2.2e-16\n\n\nE também podemos utilizar as funções glance(), augment() e tidy() todas do pacote broom\n\nglance(model_sal_exp)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.957         0.955 5788.      623. 1.14e-20     1  -301.  609.  613.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\naugment(model_sal_exp)\n\n# A tibble: 30 × 8\n   Salary YearsExperience .fitted .resid   .hat .sigma .cooksd .std.resid\n    &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1  39343             1.1  36187.  3156. 0.109   5859. 0.0205       0.578\n 2  46205             1.3  38077.  8128. 0.102   5659. 0.125        1.48 \n 3  37731             1.5  39967. -2236. 0.0956  5877. 0.00872     -0.406\n 4  43525             2    44692. -1167. 0.0803  5890. 0.00193     -0.210\n 5  39891             2.2  46582. -6691. 0.0748  5740. 0.0584      -1.20 \n 6  56642             2.9  53197.  3445. 0.0583  5855. 0.0116       0.613\n 7  60150             3    54142.  6008. 0.0562  5773. 0.0340       1.07 \n 8  54445             3.2  56032. -1587. 0.0525  5886. 0.00220     -0.282\n 9  64445             3.2  56032.  8413. 0.0525  5655. 0.0617       1.49 \n10  57189             3.7  60757. -3568. 0.0445  5853. 0.00926     -0.631\n# ℹ 20 more rows\n\n\n\ntidy(model_sal_exp)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)       25792.     2273.      11.3 5.51e-12\n2 YearsExperience    9450.      379.      25.0 1.14e-20\n\n\nFinalmente, podemos incluir as predições dentro do nosso dataframe, utilizando a função predict() dentro de um mutate(). Note que o resultado da coluna pred é igual à coluna .fitted quando aplicamos augment(model_sal_exp) acima.\n\nsal_exp %&gt;%\nmutate(pred = predict(model_sal_exp, sal_exp))\n\n   YearsExperience Salary      pred\n1              1.1  39343  36187.16\n2              1.3  46205  38077.15\n3              1.5  37731  39967.14\n4              2.0  43525  44692.12\n5              2.2  39891  46582.12\n6              2.9  56642  53197.09\n7              3.0  60150  54142.09\n8              3.2  54445  56032.08\n9              3.2  64445  56032.08\n10             3.7  57189  60757.06\n11             3.9  63218  62647.05\n12             4.0  55794  63592.05\n13             4.0  56957  63592.05\n14             4.1  57081  64537.05\n15             4.5  61111  68317.03\n16             4.9  67938  72097.02\n17             5.1  66029  73987.01\n18             5.3  83088  75877.00\n19             5.9  81363  81546.98\n20             6.0  93940  82491.97\n21             6.8  91738  90051.94\n22             7.1  98273  92886.93\n23             7.9 101302 100446.90\n24             8.2 113812 103281.89\n25             8.7 109431 108006.87\n26             9.0 105582 110841.86\n27             9.5 116969 115566.84\n28             9.6 112635 116511.84\n29            10.3 122391 123126.81\n30            10.5 121872 125016.80\n\n\nVamos incluir ao código anterior, a camada de ggplot para fazer um gráfico que inclua tanto salary (real) e pred (predição)\n\nsal_exp %&gt;%\nmutate(pred = predict(model_sal_exp, sal_exp))%&gt;%\nggplot(aes(YearsExperience, Salary))+\ngeom_point()+\ngeom_line(aes(x = YearsExperience, y = pred,\n             colour = 'pred'),\n             size=1) +\n  labs(title = 'Salary vs Experience',\n       x = 'Years of Experience',\n       y= 'Salary')\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeste segundo exemplo queremos prever a nota do GPA de estudantes universitários com base nas notas obtidas por eles no exame SAT.\n\ngpa &lt;- read.csv(\"data4/GPA.csv\")\n\nggplot(gpa, aes(SAT, GPA))+\n  geom_point()\n\n\n\n\n\n\n\n\nO procedimento é o mesmo para realizar a regressão linear.\n\nmodel_gpa &lt;- lm(_____~_____, data=_____)\n\nVamos utilizar as funções glance(), augment() e tidy() todas do pacote broom\n\nglance(______)\naugment(_____)\ntidy(______)\n\nPor fim, vamos fazer a predição dos valores, utilizando o modelo model_gpa e inclui-las no dataframe gpa para poder plotar o real (SAT) e a predição (pred).\n\ngpa %&gt;%\nmutate(pred = predict(______, ______))%&gt;%\nggplot(aes(SAT, GPA))+\ngeom_point()+\ngeom_line(aes(______) +\n  labs(title = 'GPA vs SAT',\n       x = 'SAT',\n       y= 'GPA')\n\n\n\n\n\n\n\n\nreal_estate &lt;- read.csv(\"data4/real_estate_price_size.csv\")\n\nRodando o modelo e graficando\n\nmodel_real_estate &lt;- lm(__________, ____________)\n\nreal_estate %&gt;%\nmutate(pred = predict(________,____________))%&gt;%\nggplot(aes(size, price))+\ngeom_point()+\ngeom_line(aes(_________________) +\n  labs(title = 'Size vs Price',\n       x = 'Size',\n       y= 'Price')"
  },
  {
    "objectID": "Tutorial4reg.html#salário-vs-experiência",
    "href": "Tutorial4reg.html#salário-vs-experiência",
    "title": "Tutorial 5 - Métodos de Regressão",
    "section": "",
    "text": "Vamos ajustar a pasta com que iremos trabalhar\nImportando os dados:\n\nsal_exp &lt;- read.csv('data4/Salary_Data.csv')\n\nsal_exp %&gt;% \n  ggplot(aes(YearsExperience, Salary))+\n  geom_point()\n\n\n\n\n\n\n\n\nPara criar uma regressão linear no R, basta usar a função lm().\n\nmodel_sal_exp &lt;- lm(formula = Salary ~ YearsExperience, \n              data = sal_exp)\n\nPara ver as informações sobre a regressão, podemos utilizar a função summary() no objeto de modelo, neste caso, em model_sal_exp.\n\nsummary(model_sal_exp) \n\n\nCall:\nlm(formula = Salary ~ YearsExperience, data = sal_exp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7958.0 -4088.5  -459.9  3372.6 11448.0 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      25792.2     2273.1   11.35 5.51e-12 ***\nYearsExperience   9450.0      378.8   24.95  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5788 on 28 degrees of freedom\nMultiple R-squared:  0.957, Adjusted R-squared:  0.9554 \nF-statistic: 622.5 on 1 and 28 DF,  p-value: &lt; 2.2e-16\n\n\nE também podemos utilizar as funções glance(), augment() e tidy() todas do pacote broom\n\nglance(model_sal_exp)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.957         0.955 5788.      623. 1.14e-20     1  -301.  609.  613.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\naugment(model_sal_exp)\n\n# A tibble: 30 × 8\n   Salary YearsExperience .fitted .resid   .hat .sigma .cooksd .std.resid\n    &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1  39343             1.1  36187.  3156. 0.109   5859. 0.0205       0.578\n 2  46205             1.3  38077.  8128. 0.102   5659. 0.125        1.48 \n 3  37731             1.5  39967. -2236. 0.0956  5877. 0.00872     -0.406\n 4  43525             2    44692. -1167. 0.0803  5890. 0.00193     -0.210\n 5  39891             2.2  46582. -6691. 0.0748  5740. 0.0584      -1.20 \n 6  56642             2.9  53197.  3445. 0.0583  5855. 0.0116       0.613\n 7  60150             3    54142.  6008. 0.0562  5773. 0.0340       1.07 \n 8  54445             3.2  56032. -1587. 0.0525  5886. 0.00220     -0.282\n 9  64445             3.2  56032.  8413. 0.0525  5655. 0.0617       1.49 \n10  57189             3.7  60757. -3568. 0.0445  5853. 0.00926     -0.631\n# ℹ 20 more rows\n\n\n\ntidy(model_sal_exp)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)       25792.     2273.      11.3 5.51e-12\n2 YearsExperience    9450.      379.      25.0 1.14e-20\n\n\nFinalmente, podemos incluir as predições dentro do nosso dataframe, utilizando a função predict() dentro de um mutate(). Note que o resultado da coluna pred é igual à coluna .fitted quando aplicamos augment(model_sal_exp) acima.\n\nsal_exp %&gt;%\nmutate(pred = predict(model_sal_exp, sal_exp))\n\n   YearsExperience Salary      pred\n1              1.1  39343  36187.16\n2              1.3  46205  38077.15\n3              1.5  37731  39967.14\n4              2.0  43525  44692.12\n5              2.2  39891  46582.12\n6              2.9  56642  53197.09\n7              3.0  60150  54142.09\n8              3.2  54445  56032.08\n9              3.2  64445  56032.08\n10             3.7  57189  60757.06\n11             3.9  63218  62647.05\n12             4.0  55794  63592.05\n13             4.0  56957  63592.05\n14             4.1  57081  64537.05\n15             4.5  61111  68317.03\n16             4.9  67938  72097.02\n17             5.1  66029  73987.01\n18             5.3  83088  75877.00\n19             5.9  81363  81546.98\n20             6.0  93940  82491.97\n21             6.8  91738  90051.94\n22             7.1  98273  92886.93\n23             7.9 101302 100446.90\n24             8.2 113812 103281.89\n25             8.7 109431 108006.87\n26             9.0 105582 110841.86\n27             9.5 116969 115566.84\n28             9.6 112635 116511.84\n29            10.3 122391 123126.81\n30            10.5 121872 125016.80\n\n\nVamos incluir ao código anterior, a camada de ggplot para fazer um gráfico que inclua tanto salary (real) e pred (predição)\n\nsal_exp %&gt;%\nmutate(pred = predict(model_sal_exp, sal_exp))%&gt;%\nggplot(aes(YearsExperience, Salary))+\ngeom_point()+\ngeom_line(aes(x = YearsExperience, y = pred,\n             colour = 'pred'),\n             size=1) +\n  labs(title = 'Salary vs Experience',\n       x = 'Years of Experience',\n       y= 'Salary')"
  },
  {
    "objectID": "Tutorial4reg.html#gpa",
    "href": "Tutorial4reg.html#gpa",
    "title": "Tutorial 5 - Métodos de Regressão",
    "section": "",
    "text": "Neste segundo exemplo queremos prever a nota do GPA de estudantes universitários com base nas notas obtidas por eles no exame SAT.\n\ngpa &lt;- read.csv(\"data4/GPA.csv\")\n\nggplot(gpa, aes(SAT, GPA))+\n  geom_point()\n\n\n\n\n\n\n\n\nO procedimento é o mesmo para realizar a regressão linear.\n\nmodel_gpa &lt;- lm(_____~_____, data=_____)\n\nVamos utilizar as funções glance(), augment() e tidy() todas do pacote broom\n\nglance(______)\naugment(_____)\ntidy(______)\n\nPor fim, vamos fazer a predição dos valores, utilizando o modelo model_gpa e inclui-las no dataframe gpa para poder plotar o real (SAT) e a predição (pred).\n\ngpa %&gt;%\nmutate(pred = predict(______, ______))%&gt;%\nggplot(aes(SAT, GPA))+\ngeom_point()+\ngeom_line(aes(______) +\n  labs(title = 'GPA vs SAT',\n       x = 'SAT',\n       y= 'GPA')"
  },
  {
    "objectID": "Tutorial4reg.html#real-state",
    "href": "Tutorial4reg.html#real-state",
    "title": "Tutorial 5 - Métodos de Regressão",
    "section": "",
    "text": "real_estate &lt;- read.csv(\"data4/real_estate_price_size.csv\")\n\nRodando o modelo e graficando\n\nmodel_real_estate &lt;- lm(__________, ____________)\n\nreal_estate %&gt;%\nmutate(pred = predict(________,____________))%&gt;%\nggplot(aes(size, price))+\ngeom_point()+\ngeom_line(aes(_________________) +\n  labs(title = 'Size vs Price',\n       x = 'Size',\n       y= 'Price')"
  },
  {
    "objectID": "Tutorial4reg.html#treinar-o-modelo",
    "href": "Tutorial4reg.html#treinar-o-modelo",
    "title": "Tutorial 5 - Métodos de Regressão",
    "section": "Treinar o modelo",
    "text": "Treinar o modelo\nAgora, rodamos o modelo como fizemos com os casos anterior, utilizando a função lm() porém apenas utilizando os dados de treino.\n\nstr(train)\n\n'data.frame':   6131 obs. of  14 variables:\n $ Rented.Bike.Count        : int  107 181 167 79 117 32 13 22 85 152 ...\n $ Hour                     : Factor w/ 24 levels \"0\",\"1\",\"2\",\"3\",..: 4 7 4 6 4 6 7 8 9 10 ...\n $ Temperature.C.           : num  -6.2 -6.6 -3.5 -4 3.4 3.9 3.4 3.1 3.2 3.5 ...\n $ Humidity...              : int  40 35 81 79 71 75 86 91 92 91 ...\n $ Wind.speed..m.s.         : num  0.9 1.3 2.2 1.5 1.6 1.9 2.5 1.2 1.8 1.8 ...\n $ Visibility..10m.         : int  2000 2000 1221 1202 1011 914 278 129 244 231 ...\n $ Dew.point.temperature..C.: num  -17.6 -19.5 -6.2 -7.1 -1.3 -0.1 1.2 1.7 2 2.1 ...\n $ Solar.Radiation..MJ.m2.  : num  0 0 0 0 0 0 0 0 0 0.02 ...\n $ Rainfall.mm.             : num  0 0 0 0 0.5 0 1 0 0 2.5 ...\n $ Snowfall..cm.            : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Holiday                  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Month                    : Factor w/ 12 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 2 3 3 3 3 3 3 ...\n $ Week                     : Factor w/ 52 levels \"1\",\"2\",\"3\",\"4\",..: 2 2 6 6 10 10 10 10 10 10 ...\n $ Dayofweek                : Factor w/ 7 levels \"0\",\"1\",\"2\",\"3\",..: 4 4 7 7 7 7 7 7 7 7 ...\n\n\n\nbike_model &lt;- lm(Rented.Bike.Count ~ ., data = train)\n\n\nsummary(bike_model)\n\n\nCall:\nlm(formula = Rented.Bike.Count ~ ., data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1642.44  -211.57     3.27   210.50  1565.22 \n\nCoefficients: (1 not defined because of singularities)\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                561.49285  108.21697   5.189 2.19e-07 ***\nHour1                     -120.69786   34.00126  -3.550 0.000388 ***\nHour2                     -216.57466   33.62341  -6.441 1.28e-10 ***\nHour3                     -286.04320   33.63545  -8.504  &lt; 2e-16 ***\nHour4                     -348.13562   34.38300 -10.125  &lt; 2e-16 ***\nHour5                     -332.33267   33.62620  -9.883  &lt; 2e-16 ***\nHour6                     -166.03320   33.32154  -4.983 6.44e-07 ***\nHour7                      124.66193   33.77818   3.691 0.000226 ***\nHour8                      534.75513   34.01577  15.721  &lt; 2e-16 ***\nHour9                       19.11326   34.97723   0.546 0.584778    \nHour10                    -180.97584   36.03952  -5.022 5.27e-07 ***\nHour11                    -206.68915   38.01518  -5.437 5.63e-08 ***\nHour12                    -166.22513   38.75394  -4.289 1.82e-05 ***\nHour13                    -196.38134   39.29393  -4.998 5.96e-07 ***\nHour14                    -179.21978   38.72286  -4.628 3.76e-06 ***\nHour15                    -125.55303   37.87188  -3.315 0.000921 ***\nHour16                      -2.85281   36.04687  -0.079 0.936922    \nHour17                     293.67016   35.43320   8.288  &lt; 2e-16 ***\nHour18                     710.56262   34.38963  20.662  &lt; 2e-16 ***\nHour19                     477.56693   34.43185  13.870  &lt; 2e-16 ***\nHour20                     429.74577   34.57360  12.430  &lt; 2e-16 ***\nHour21                     443.40577   34.16954  12.977  &lt; 2e-16 ***\nHour22                     313.91031   33.42865   9.390  &lt; 2e-16 ***\nHour23                     110.54761   33.91050   3.260 0.001120 ** \nTemperature.C.              27.43341    3.96541   6.918 5.05e-12 ***\nHumidity...                 -6.67531    1.09339  -6.105 1.09e-09 ***\nWind.speed..m.s.             3.59675    5.82720   0.617 0.537102    \nVisibility..10m.             0.05099    0.01184   4.308 1.67e-05 ***\nDew.point.temperature..C.    1.64759    4.11461   0.400 0.688857    \nSolar.Radiation..MJ.m2.     53.98632   12.43170   4.343 1.43e-05 ***\nRainfall.mm.               -59.79234    4.49014 -13.316  &lt; 2e-16 ***\nSnowfall..cm.               36.91562   13.98499   2.640 0.008320 ** \nHoliday1                   -43.28930   27.06962  -1.599 0.109832    \nMonth2                      57.24836   74.03923   0.773 0.439425    \nMonth3                     113.69767  102.92388   1.105 0.269344    \nMonth4                     121.33827  144.94740   0.837 0.402559    \nMonth5                    -156.88730  179.42260  -0.874 0.381935    \nMonth6                    -472.53313  198.50718  -2.380 0.017323 *  \nMonth7                    -739.29159  231.21765  -3.197 0.001394 ** \nMonth8                    -420.85275  253.03061  -1.663 0.096315 .  \nMonth9                    -384.11543  273.35868  -1.405 0.160022    \nMonth10                    644.68937  120.31789   5.358 8.72e-08 ***\nMonth11                    114.02756   96.54652   1.181 0.237624    \nMonth12                    -74.93480   50.94211  -1.471 0.141349    \nWeek2                       70.02441   51.73916   1.353 0.175975    \nWeek3                      -69.61830   51.16608  -1.361 0.173680    \nWeek4                      -32.79547   51.75307  -0.634 0.526306    \nWeek5                      -60.71425   68.28194  -0.889 0.373947    \nWeek6                     -185.58072   88.31000  -2.101 0.035641 *  \nWeek7                     -283.22509   91.79266  -3.085 0.002041 ** \nWeek8                     -156.75681   90.90400  -1.724 0.084683 .  \nWeek9                     -134.51699   98.63190  -1.364 0.172672    \nWeek10                    -137.45910  113.26600  -1.214 0.224950    \nWeek11                    -148.13537  114.96392  -1.289 0.197608    \nWeek12                    -120.67558  115.04109  -1.049 0.294230    \nWeek13                     -71.90026  115.74723  -0.621 0.534502    \nWeek14                     -73.74247  153.13173  -0.482 0.630134    \nWeek15                    -184.84545  153.44642  -1.205 0.228395    \nWeek16                    -141.83000  153.94025  -0.921 0.356914    \nWeek17                      46.99740  153.52653   0.306 0.759525    \nWeek18                     167.02467  177.68243   0.940 0.347246    \nWeek19                     239.54639  186.54253   1.284 0.199143    \nWeek20                     245.31530  187.30730   1.310 0.190349    \nWeek21                     324.61848  187.09481   1.735 0.082783 .  \nWeek22                     431.71109  190.71128   2.264 0.023629 *  \nWeek23                     624.93312  205.74205   3.037 0.002396 ** \nWeek24                     759.00123  206.12317   3.682 0.000233 ***\nWeek25                     759.63323  207.14867   3.667 0.000247 ***\nWeek26                     637.74218  208.49584   3.059 0.002232 ** \nWeek27                     823.28516  236.86238   3.476 0.000513 ***\nWeek28                     910.78057  238.50004   3.819 0.000135 ***\nWeek29                     634.80700  240.29923   2.642 0.008270 ** \nWeek30                     511.67589  240.60476   2.127 0.033492 *  \nWeek31                     368.32112  249.66476   1.475 0.140194    \nWeek32                     489.51231  259.72253   1.885 0.059511 .  \nWeek33                     219.03075  261.64387   0.837 0.402551    \nWeek34                     262.28664  261.47721   1.003 0.315855    \nWeek35                     352.55744  262.91159   1.341 0.179980    \nWeek36                     383.93474  279.74281   1.372 0.169973    \nWeek37                     260.09699  279.31023   0.931 0.351781    \nWeek38                     179.23099  280.48565   0.639 0.522846    \nWeek39                     275.00871  279.35325   0.984 0.324935    \nWeek40                    -734.34043  121.13084  -6.062 1.42e-09 ***\nWeek41                    -467.02788  120.58795  -3.873 0.000109 ***\nWeek42                    -286.07788  120.62316  -2.372 0.017739 *  \nWeek43                    -402.85384  120.97141  -3.330 0.000873 ***\nWeek44                    -346.87749  102.93371  -3.370 0.000757 ***\nWeek45                      -0.30865   96.61576  -0.003 0.997451    \nWeek46                     161.53730   98.12794   1.646 0.099777 .  \nWeek47                      19.63716   97.95441   0.200 0.841118    \nWeek48                     101.62138   79.43207   1.279 0.200823    \nWeek49                     113.00746   54.47186   2.075 0.038066 *  \nWeek50                     188.85906   49.56404   3.810 0.000140 ***\nWeek51                      51.25974   52.72105   0.972 0.330949    \nWeek52                            NA         NA      NA       NA    \nDayofweek1                 -15.64334   18.80469  -0.832 0.405507    \nDayofweek2                  38.81875   18.92059   2.052 0.040245 *  \nDayofweek3                  36.67689   19.01342   1.929 0.053778 .  \nDayofweek4                  38.61802   19.22112   2.009 0.044566 *  \nDayofweek5                  25.01000   19.44627   1.286 0.198455    \nDayofweek6                 -60.62579   19.48796  -3.111 0.001874 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 383.9 on 6031 degrees of freedom\nMultiple R-squared:  0.6519,    Adjusted R-squared:  0.6462 \nF-statistic: 114.1 on 99 and 6031 DF,  p-value: &lt; 2.2e-16\n\n\nAgora que rodamos o modelo, podemos calcular alguns indicadores de acurácia, os mais indicados são: \\(R^2\\), o RMSE (root mean squared error) e o MAE (mean absolute error). Para isso, precisamos utilizar o modelo treinado nos dados novos isto é, no dataset de teste.\nVamos utilizar a função predict() para que o modelo gere valores estimados e logo as funções rmse_vec e rsq_vec para gerar os valores de RMSE e \\(R^2\\)."
  },
  {
    "objectID": "Tutorial4reg.html#avaliar-o-modelo",
    "href": "Tutorial4reg.html#avaliar-o-modelo",
    "title": "Tutorial 5 - Métodos de Regressão",
    "section": "Avaliar o modelo",
    "text": "Avaliar o modelo\n\nbike_lm &lt;- test %&gt;%\n    mutate(pred_lm = as.numeric(predict(bike_model, newdata = test)),\n           Rented.Bike.Count = as.numeric(Rented.Bike.Count))%&gt;%\n    select(Rented.Bike.Count, pred_lm) %&gt;%\n    summarize(\n             R2 = rsq_vec(Rented.Bike.Count, pred_lm),\n             RMSE = rmse_vec(Rented.Bike.Count, pred_lm),\n             MAE = mae_vec(Rented.Bike.Count, pred_lm))\n\nbike_lm\n\n         R2     RMSE     MAE\n1 0.6054272 404.7653 303.263"
  },
  {
    "objectID": "Tutorial4reg.html#treinar-o-modelo-1",
    "href": "Tutorial4reg.html#treinar-o-modelo-1",
    "title": "Tutorial 5 - Métodos de Regressão",
    "section": "Treinar o modelo",
    "text": "Treinar o modelo\n\n# Treinar um modelo utilizando rpart\n\nbike_tree_model &lt;- rpart(Rented.Bike.Count ~ ., data = train, method = \"anova\")"
  },
  {
    "objectID": "Tutorial4reg.html#plotar-o-modelo",
    "href": "Tutorial4reg.html#plotar-o-modelo",
    "title": "Tutorial 5 - Métodos de Regressão",
    "section": "Plotar o modelo",
    "text": "Plotar o modelo\n\n# usar a fun fancyRpartPlot() do pacote rattle para plotar a árvore\n\nfancyRpartPlot(bike_tree_model)"
  },
  {
    "objectID": "Tutorial4reg.html#avaliar-o-modelo-1",
    "href": "Tutorial4reg.html#avaliar-o-modelo-1",
    "title": "Tutorial 5 - Métodos de Regressão",
    "section": "Avaliar o modelo",
    "text": "Avaliar o modelo\nPara fazer a avaliação dos indicadores de acurácia, procedemos da mesma forma que antes.\n\nbike_tree &lt;- test %&gt;%\n    mutate(pred_tree = as.numeric(predict(bike_tree_model, newdata = test)),\n           Rented.Bike.Count = as.numeric(Rented.Bike.Count))%&gt;%\n    select(Rented.Bike.Count, pred_tree) %&gt;%\n    summarize(\n             R2 = rsq_vec(Rented.Bike.Count, pred_tree),\n             RMSE = rmse_vec(Rented.Bike.Count, pred_tree),\n             MAE = mae_vec(Rented.Bike.Count, pred_tree))\n\nbike_tree\n\n         R2     RMSE      MAE\n1 0.6230404 395.9126 275.3184"
  },
  {
    "objectID": "Tutorial4reg.html#treinar-o-modelo-2",
    "href": "Tutorial4reg.html#treinar-o-modelo-2",
    "title": "Tutorial 5 - Métodos de Regressão",
    "section": "Treinar o modelo",
    "text": "Treinar o modelo\n\nlibrary(ranger)\n\n\n# treinar um modelo usando o pacote `ranger`\n\nbike_rf_model &lt;- ranger(Rented.Bike.Count ~ ., \n                              data = train,\n                              num.trees = 300)\n\nUma vez que o modelo foi treinado, vamos calcular o indicadores de acurácia utilizando os procedimentos já vistos. Porém, há um passo intermediário que serve para extrair as predições do objeto criado pelo predict() do ranger.\n\nrf_predict &lt;- predict(bike_rf_model, data = test, type = \"response\")$predictions\n\nAgora sim, podemos utilizar o código dos modelos anteriores, adaptando-o ligeiramente para incluir o vetor criado acima com os valores preditos."
  },
  {
    "objectID": "Tutorial4reg.html#avaliar-o-modelo-2",
    "href": "Tutorial4reg.html#avaliar-o-modelo-2",
    "title": "Tutorial 5 - Métodos de Regressão",
    "section": "Avaliar o modelo",
    "text": "Avaliar o modelo\n\nbike_rf &lt;- test %&gt;%\n    mutate(pred_rf = as.numeric(rf_predict),\n           Rented.Bike.Count = as.numeric(Rented.Bike.Count))%&gt;%\n    select(Rented.Bike.Count, pred_rf) %&gt;%\n    summarize(\n             R2 = rsq_vec(Rented.Bike.Count, pred_rf),\n             RMSE = rmse_vec(Rented.Bike.Count, pred_rf),\n             MAE = mae_vec(Rented.Bike.Count, pred_rf))\n\nbike_rf\n\n         R2     RMSE      MAE\n1 0.8268875 274.4973 170.6298\n\n\nPara facilitar a comparação, vamos juntar todos os três grupos de resultados."
  },
  {
    "objectID": "Lab2.html",
    "href": "Lab2.html",
    "title": "Lab 2 - Dashboards em R",
    "section": "",
    "text": "O dataset utiliza as informações de anúncios do Airbnb e informações sobre restaurantes e crimes de rua na cidade de Nova Iorque em 2018 e 2019. Os dados podem ser localizados em http://insideairbnb.com/, https://www.kaggle.com/datasets/popoandrew/restaurant-week-2018 e em https://www.kaggle.com/code/adamschroeder/crime-in-new-york-city/. Você pode carregá-los a partir da pasta anexa."
  },
  {
    "objectID": "Lab2.html#sobre-o-dataset",
    "href": "Lab2.html#sobre-o-dataset",
    "title": "Lab 2 - Dashboards em R",
    "section": "",
    "text": "O dataset utiliza as informações de anúncios do Airbnb e informações sobre restaurantes e crimes de rua na cidade de Nova Iorque em 2018 e 2019. Os dados podem ser localizados em http://insideairbnb.com/, https://www.kaggle.com/datasets/popoandrew/restaurant-week-2018 e em https://www.kaggle.com/code/adamschroeder/crime-in-new-york-city/. Você pode carregá-los a partir da pasta anexa."
  },
  {
    "objectID": "Lab2.html#o-que-faremos",
    "href": "Lab2.html#o-que-faremos",
    "title": "Lab 2 - Dashboards em R",
    "section": "O que faremos",
    "text": "O que faremos\nImagine que você está realizando uma consultoria de dados para uma agência de viagens. O interesse do cliente é conhecer melhor as opções de hospedagem alternativo e restaurantes em bairros mais seguros na cidade de NY.\nO mercado-alvo são viajantes que visitam NY por lazer e o cliente deseja conhecer melhor as diferenças existentes entre os bairros, levando em consideração por exemplo as avaliações dos Airbnbs e dos restaurantes, bem como os preços praticados e pouco risco de crimes além de estimativas de custos totais aproximados de moradia e alimentação para uma viagem de 7 dias em média.\nUtilize seus conhecimentos de tidyverse e ggplot para realizar análises e visualizações interessantes."
  },
  {
    "objectID": "Lab2.html#instruções",
    "href": "Lab2.html#instruções",
    "title": "Lab 2 - Dashboards em R",
    "section": "Instruções",
    "text": "Instruções\nUma vez que você tenha escolhidos as melhores visualizações, deverá criar um dashboard utilizando o pacote flexdashboard e subi-lo à plataforma shinyapps.io. O link deverá ser compartilhado com o Professor junto com o arquivo .Rmd do dashboard via moodle. Nota: o arquivo .Rmd deve ser exatamente o mesmo que gerou o dashboard, em outras palavras, ele deverá rodar também localmente.\nPara poder configurar o shinyapps.io deve seguir as instruções nesta página."
  },
  {
    "objectID": "Lab2.html#opções-para-o-sidebar",
    "href": "Lab2.html#opções-para-o-sidebar",
    "title": "Lab 2 - Dashboards em R",
    "section": "Opções para o Sidebar",
    "text": "Opções para o Sidebar\n\nCriar uma coluna sidebar usando o comando: `## Column{data-width=200 .sidebar}`\n\n#| eval: false\n\n# Menu desplegável\n\n\nselectInput(\"nome\",\n            label = \"Selecione ...\",\n            choices = c(____),\n            selected = ____)\n\n# Slider\n\nsliderInput(\"nome2\",\n            label = \"Escolha ....\",\n            min = \"___\", max = \"____\", value = \"___\", step = '___', \n            dragRange = TRUE)\n\n# Data\n\ndateInput(inputid = \"data1\", \n          label = \"____\", \n          value = \"YYYY-MM-DD\", #informar a data\n          format = \"mm/dd/yy\",\n          language = \"pt\")\n\n# Intervalo de datas - produzirá um objeto com dois valores\n\ndateRangeInput(\"data2\", label = \"______\",\n                 start  = \"_____\", #informar a data de início\n                 end    = \"_____\", #informar a data de finalização\n                 format = \"mm/dd/yy\",\n                 language = \"pt\",\n                 separator = \" - \")"
  },
  {
    "objectID": "Lab2.html#opções-para-o-corpo",
    "href": "Lab2.html#opções-para-o-corpo",
    "title": "Lab 2 - Dashboards em R",
    "section": "Opções para o corpo",
    "text": "Opções para o corpo\n\n#| eval: false\n\n# KPIs\n\nrenderValueBox({\n\nvalueBox(prettyNum(______, big.mark = ','), \n         icon = 'fa-ship', caption = \"_____\",\n         color=\"#9b5de5\")\n}\n)\n\n# Requer library(plotly)\n\nrenderPlotly({  \ndf &lt;- _____\nggplotly(df)\n  })\n\n# Gauges - velocímetros\n\nrenderGauge({\n  df &lt;- _____\n  gauge(____, min = ___, max = ___, symbol = '%', gaugeSectors(\n    success = c(____, ___), warning = c(____, ___ ), danger = c(____, ___)\n  ))\n})\n\n\n# Tabelas\n\nrenderTable({  \ndf %&gt;% \n  mutate(____) %&gt;% \n  filter(_____) %&gt;% \n  group_by(____) %&gt;% \n  count() %&gt;% \n  arrange(desc(n)) \n})"
  },
  {
    "objectID": "Lab1.html",
    "href": "Lab1.html",
    "title": "Lab 1 - EDA",
    "section": "",
    "text": "Carregando os dados\nNeste Lab iremos explorar os dados sobre seriados de TV, usando dois datasets, um do Netflix (informações do viewing history) e outro do IMDb, banco de dados com informações sobre o rating de seriados e filmes.\n\n\nVocê pode baixar os dados aqui.\n\n\nInicialmente vamos carregar os dados. Os dados do Netflix vem do site onde o usuário pode fazer download do seu histórico após o login. Já os dados o IMDb usam um script que encontra-se em: https://github.com/nazareno/imdb-series para fazer o web scrapping do site oficial do IMDb.\nCarregando os dados do IMDb e mostrando os primeiras 10 linhas:\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nCarregando os dados do Netflix e mostrando as primeiras 10 linhas:\n\n#carregue os dados\n\nObserve-se que os dados de Date não estão no formato certo, pois eles aparecem como character o que pode causar problemas posteriores. Portanto antes de continuar, precisamos arrumar o formato para “data” com os comandos a seguir:\n\n#configure a coluna 'Date'\n\nComo pode se ver, a estrutura dos dados do IMDb é mais complexa, vamos ver mais de perto os dados, usando a função str():\n\n#utilize str()\n\nNos dados do IMDb, as últimas 10 colunas representam a proporção de usuários que votou o capítulo específico com a nota n, sendo que, por exemplo, a coluna r9 representa a proporção de usuários que votou com a nota 9.\n\n\nLimpando os dados do Netflix\nPrimeiro, vamos explorar mais os dados do Netflix, que conta com apenas duas colunas: o Título (Title) e a data de visualização (Date). Para isto, vamos filtrar o dataset original para ficarmos apenas com os seriados, usando os comandos do Tidyverse: filter , str_detect e separate.\n\n#utilize filter(), str_detect() e separate()\n\nCom os comandos anteriores conseguimos ter uma base limpa, onde apenas consideramos seriados e temos as informações separadas para cada capítulo e temporada.\nPorém, pode ser que o nosso processo de filtragem com str_detect() não tenha sido perfeito.\nVamos fazer um teste verificando se existem registros com o campo episode com NA:\n\n#utilize is.na()\n\nOk, identificamos (x) registros que apresentam NA, vamos explorá-los com mais cuidado para confirmar se de fato se trata de um problema do nosso filtrado inicial. E se de fato encontrarmos registros mal filtrados, faremos a limpeza manual.\n\n#utilize filter() e is.na() para explorar os dados 'NA'\n\nNa verdade esses (x) registros são de fato filmes com subtítulos (por isso o nosso filtro com str_detect() os pegou). Devemos retirá-los da nossa base de seriados.\n\n#utilize str_detect() para eliminar os registros com 'NA'\n\n\n\nExplorando as séries de Netflix\nAgora, podemos iniciar algumas visualizações, por exemplo, podemos fazer uma tabela que nos indique o número de capítulos assistidos por cada seriado. Para isto, vamos usar os comandos do dplyr:\n\n#utilize ggplot()\n\nUma outra forma de fazer esta mesma análise seria filtrar por seriados onde mais de x capítulos foram assistidos, por exemplo, seriados com mais de 20 capítulos assistidos:\n\n#utilize ggplot()\n\nTambém podemos fazer uma análise do número de capítulos assistidos em um ano específico, por exemplo, em 2020:\n\n#utilize ggplot()\n\nPodemos ver que os seriados mais assistidos foram [liste aqui os seriados mais assistidos].\nTambém podemos ver o número de capítulos totais assistidos por ano:\n\n#utilize ggplot()\n\nE também ver a distribuição dos seriados por ano, por exemplo para o seriado “Bates Motel”:\n\n#utilize ggplot()\n\nOu ver a distribuição por data:\n\n#utilize ggplot()\n\nFinalmente, podemos ver a distribuição dos seriados num ano específico:\n\n#utilize ggplot()\n\n\n\nExplorando os dados do IMDb\nAgora vamos iniciar a análise dos dados do IMDB. Primeiro vamos contabilizar o número de capítulos por seriado e vamos ordenar usando a função arrange\n\n#utilize arrange()\n\nClaramente têm seriados com muitos capítulos e outros com poucos, pode ser por causa do número de temporadas? vamos conferir plotando o total de temporadas por seriado no eixo \\(x\\) e o total de capítulos por seriado no eixo \\(y\\) utilizando geom_point()\n\n#utilize ggplot()\n\nSerá que há relação entre o número de capítulos e o número de temporadas? ou seja, quanto maior o número de temporadas, maior o número de capítulos? utilize como base o gráfico anterior e inclua uma linha de tendência com geom_smooth(\"lm\")\n\n#utilize ggplot()"
  },
  {
    "objectID": "pyTutorial3.html",
    "href": "pyTutorial3.html",
    "title": "Tutorial 3 - Matplotlib",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "pyTutorial3.html#carregando-os-pacotes",
    "href": "pyTutorial3.html#carregando-os-pacotes",
    "title": "Tutorial 3 - Matplotlib",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "pyTutorial3.html#lendo-os-dados",
    "href": "pyTutorial3.html#lendo-os-dados",
    "title": "Tutorial 3 - Matplotlib",
    "section": "Lendo os dados",
    "text": "Lendo os dados\n\n\nVocê pode baixar os dados aqui.imdb_filmes.csv\n\n\n\nimdb_filmes = pd.read_csv(\"pydata3/imdb_filmes.csv\")\n\n\nimdb_filmes.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 11340 entries, 0 to 11339\nData columns (total 20 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   id_filme              11340 non-null  object \n 1   titulo                11340 non-null  object \n 2   ano                   11339 non-null  float64\n 3   data_lancamento       11340 non-null  object \n 4   generos               11340 non-null  object \n 5   duracao               11340 non-null  int64  \n 6   pais                  11340 non-null  object \n 7   idioma                11261 non-null  object \n 8   orcamento             6389 non-null   float64\n 9   receita               6366 non-null   float64\n 10  receita_eua           5997 non-null   float64\n 11  nota_imdb             11340 non-null  float64\n 12  num_avaliacoes        11340 non-null  int64  \n 13  direcao               11340 non-null  object \n 14  roteiro               11325 non-null  object \n 15  producao              11220 non-null  object \n 16  elenco                11336 non-null  object \n 17  descricao             11326 non-null  object \n 18  num_criticas_publico  11330 non-null  float64\n 19  num_criticas_critica  11300 non-null  float64\ndtypes: float64(7), int64(2), object(11)\nmemory usage: 1.7+ MB\n\n\n\nimdb_filmes.describe()\n\n\n\n\n\n\n\n\nano\nduracao\norcamento\nreceita\nreceita_eua\nnota_imdb\nnum_avaliacoes\nnum_criticas_publico\nnum_criticas_critica\n\n\n\n\ncount\n11339.000000\n11340.000000\n6.389000e+03\n6.366000e+03\n5.997000e+03\n11340.000000\n1.134000e+04\n11330.000000\n11300.000000\n\n\nmean\n1989.452244\n99.691975\n1.903052e+07\n5.468264e+07\n3.147534e+07\n6.102487\n3.591951e+04\n138.221977\n65.660885\n\n\nstd\n25.548495\n17.649559\n3.238295e+07\n1.433980e+08\n6.033264e+07\n1.128953\n1.051027e+05\n303.109281\n85.890610\n\n\nmin\n1914.000000\n45.000000\n0.000000e+00\n1.600000e+01\n2.520000e+02\n1.300000\n1.001000e+03\n1.000000\n1.000000\n\n\n25%\n1973.000000\n89.000000\n1.500000e+06\n5.030680e+05\n1.007583e+06\n5.500000\n1.877000e+03\n31.000000\n17.000000\n\n\n50%\n1997.000000\n97.000000\n6.500000e+06\n9.521381e+06\n1.067257e+07\n6.300000\n4.736500e+03\n55.000000\n35.000000\n\n\n75%\n2011.000000\n108.000000\n2.200000e+07\n4.309351e+07\n3.600150e+07\n6.900000\n2.106250e+04\n127.000000\n78.000000\n\n\nmax\n2020.000000\n271.000000\n3.560000e+08\n2.797801e+09\n9.366622e+08\n9.300000\n2.278845e+06\n8869.000000\n909.000000\n\n\n\n\n\n\n\n\nimdb_filmes.sample(8)\n\n\n\n\n\n\n\n\nid_filme\ntitulo\nano\ndata_lancamento\ngeneros\nduracao\npais\nidioma\norcamento\nreceita\nreceita_eua\nnota_imdb\nnum_avaliacoes\ndirecao\nroteiro\nproducao\nelenco\ndescricao\nnum_criticas_publico\nnum_criticas_critica\n\n\n\n\n4965\ntt0048260\nKismet\n1955.0\n1955-12-23\nAdventure, Musical, Fantasy\n113\nUSA\nEnglish\n2692960.0\nNaN\nNaN\n6.3\n1329\nVincente Minnelli, Stanley Donen\nCharles Lederer, Luther Davis\nMetro-Goldwyn-Mayer (MGM)\nHoward Keel, Ann Blyth, Dolores Gray, Vic Damo...\nA roguish poet is given the run of the schemin...\n42.0\n23.0\n\n\n5526\ntt0049552\nNightfall\n1956.0\n1958-06-20\nCrime, Drama, Film-Noir\n78\nUSA\nEnglish, Spanish, Italian\nNaN\nNaN\nNaN\n7.2\n3204\nJacques Tourneur\nStirling Silliphant, David Goodis\nCopa Productions\nAldo Ray, Brian Keith, Anne Bancroft, Jocelyn ...\nThrough a series of bizarre coincidences, an a...\n51.0\n39.0\n\n\n6362\ntt0041822\nRope of Sand\n1949.0\n1949-08-03\nAdventure, Film-Noir\n104\nUSA\nEnglish, Afrikaans\nNaN\nNaN\nNaN\n6.8\n1196\nWilliam Dieterle\nWalter Doniger, John Paxton\nWallis-Hazen\nBurt Lancaster, Paul Henreid, Claude Rains, Pe...\nA man abused by a sadistic mining company cop ...\n21.0\n13.0\n\n\n4728\ntt5179598\nBillionaire Boys Club\n2018.0\n2018-07-17\nBiography, Drama, Thriller\n108\nUSA\nEnglish\n15000000.0\n2713955.0\n1349.0\n5.6\n10299\nJames Cox\nJames Cox, Captain Mauzner\nArmory Films\nAnsel Elgort, Kevin Spacey, Taron Egerton, Emm...\nA group of wealthy boys in Los Angeles during ...\n70.0\n41.0\n\n\n597\ntt0029843\nThe Adventures of Robin Hood\n1938.0\n1939-01-26\nAction, Adventure, Romance\n102\nUSA\nEnglish\n1900000.0\nNaN\nNaN\n7.9\n46382\nMichael Curtiz, William Keighley\nNorman Reilly Raine, Seton I. Miller\nWarner Bros.\nErrol Flynn, Olivia de Havilland, Basil Rathbo...\nWhen Prince John and the Norman Lords begin op...\n279.0\n107.0\n\n\n914\ntt0093800\nPsychos in Love\n1987.0\n1987-05-01\nComedy, Horror\n88\nUSA\nEnglish\n75000.0\nNaN\nNaN\n5.9\n1071\nGorman Bechard\nGorman Bechard, Carmine Capobianco\nBeyond Infinity\nCarmine Capobianco, Patti Chambers, Carla Brag...\nA strip-joint owner and a manicurist find that...\n39.0\n38.0\n\n\n10697\ntt0461770\nEnchanted\n2007.0\n2007-12-07\nAnimation, Comedy, Family\n107\nUSA\nEnglish\n85000000.0\n340487652.0\n127807262.0\n7.0\n173962\nKevin Lima\nBill Kelly\nWalt Disney Pictures\nAmy Adams, Patrick Dempsey, James Marsden, Tim...\nA young maiden in a land called Andalasia, who...\n421.0\n231.0\n\n\n5402\ntt0096874\nBack to the Future Part II\n1989.0\n1989-12-22\nAdventure, Comedy, Sci-Fi\n108\nUSA\nEnglish\n40000000.0\n335973020.0\n119000002.0\n7.8\n467662\nRobert Zemeckis\nRobert Zemeckis, Bob Gale\nUniversal Pictures\nMichael J. Fox, Christopher Lloyd, Lea Thompso...\nAfter visiting 2015, Marty McFly must repeat h...\n421.0\n149.0"
  },
  {
    "objectID": "pyTutorial3.html#limpeza-dos-dados",
    "href": "pyTutorial3.html#limpeza-dos-dados",
    "title": "Tutorial 3 - Matplotlib",
    "section": "Limpeza dos dados",
    "text": "Limpeza dos dados\nVamos converter a coluna data_lancamento para formato data.\n\n# Get all unique non-date values from `data_lancamento`\nnon_date_values = imdb_filmes[pd.to_datetime(imdb_filmes['data_lancamento'], errors='coerce').isna()]['data_lancamento'].unique() \n\nif (len(non_date_values) &gt; 20):\n  # Sample 20 of them if there are too many unique values\n  print(f\"Non-date values in `data_lancamento`: {np.random.choice(non_date_values, 20, replace=False)}\")\nelse:\n  # Otherwise print all unique non-date values from `data_lancamento`\n  print(f\"Non-date values in `data_lancamento`: {non_date_values}\")\n\nNon-date values in `data_lancamento`: ['1989' '2010' '1990' '1970' '1974' '1967' '1973' '2006' '1971' '1983'\n '1966' '1999' '1956' '2005' '1927' '1948' '1943' '1961' '1997' '2009']\n\n\n\nimdb_filmes['data_lancamento']=pd.to_datetime(imdb_filmes['data_lancamento'], format='%Y-%m-%d',\nerrors='coerce')"
  },
  {
    "objectID": "Tutorial2_gabarito.html",
    "href": "Tutorial2_gabarito.html",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "",
    "text": "Neste tutorial vamos aprender um pouco mais sobre o ggplot em conjunto com os outros pacotes do tidyverse.\nPara praticar, vamos usar o dataset netflix_series_limpo.xlsx, o dataset NetflixViewingHistory.csv e o dataset imdb_series.xlsx e o dataset imdb.rds de filmes. Portanto, o primeiro passo será carregar os datasets:"
  },
  {
    "objectID": "Tutorial2_gabarito.html#exemplo-base",
    "href": "Tutorial2_gabarito.html#exemplo-base",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nPara realizar gráficos de dispersão, precisamos passar a função geom_point() como uma nova camada do ggplot. As coordenadas x e y devem ser numéricas necessariamente. Vamos usar como exemplo, o dataframe imdb_filmes.\nNo exemplo, a posição do ponto no eixo x pode ser dada pela coluna orcamento e a posição do ponto no eixo y pela coluna receita.\n\nimdb_filmes %&gt;% \n  ggplot(aes(orcamento/1000000,receita/1000000000))+\n  geom_point()+\n  scale_x_continuous(labels = scales::dollar,\n                     breaks = c(0, 50, 100,150,               200,250,300,350))+\n  scale_y_continuous(labels=scales::dollar,\n                     n.breaks = 8)+\n  labs(x=\"Orçamento em Milhões de $\",\n       y=\"Receita em Bilhões de $\")+\n  theme(panel.background = element_rect(fill = \"#fbeca5\",                 color = \"black\",\n                                        size = 2),\n        panel.grid = element_line(color= \"white\"))+\n  coord_cartesian(ylim = c(0,6),\n                  xlim = c(0,450))\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nWarning: Removed 7004 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "Tutorial2_gabarito.html#adicionando-cores",
    "href": "Tutorial2_gabarito.html#adicionando-cores",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Adicionando cores",
    "text": "Adicionando cores\nPodemos adicionar cores grupalmente para todos os pontos ou podemos usar alguma outra variável para criar cores em gradiente\n\nimdb_filmes %&gt;% \n  ggplot(aes(orcamento,receita))+\n  geom_point(color=\"#e38421\", size=5)\n\nWarning: Removed 7004 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nPara ver mais detalhamento da função geom_point() recomenda-se a leitura do Capítulo 8 do Livro “Curso-R”."
  },
  {
    "objectID": "Tutorial2_gabarito.html#exemplo-base-1",
    "href": "Tutorial2_gabarito.html#exemplo-base-1",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nPara realizar gráficos de barras usamos geom_col(). Vamos usar o dataframe imdb_filmes para exemplificar, ordenando as linhas por ordem decrescente de UserRating. Primeiro vamos gerar o gráfico com as configuração padrão.\n\nimdb_filmes %&gt;% \n  arrange(desc(nota_imdb)) %&gt;% \n  head(10) %&gt;%\n  ggplot(aes(titulo, nota_imdb,label = nota_imdb))+\n  geom_col()+\n  geom_label()+\n  theme_minimal()+\n  labs(x=\"\",y=\"\",\n       title = \"Filmes com maior nota no imdb\")+\n  scale_y_continuous(\n    breaks = c(0,2,4,6,8,10),\n    limits=c(0,10))+\n  coord_flip()"
  },
  {
    "objectID": "Tutorial2_gabarito.html#mudando-os-eixos-e-adicionando-cores",
    "href": "Tutorial2_gabarito.html#mudando-os-eixos-e-adicionando-cores",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Mudando os eixos e adicionando cores",
    "text": "Mudando os eixos e adicionando cores\nPodemos mudar os eixos (trocar de eixo) usando a função coord_flip()\n\nimdb_filmes %&gt;% \n  arrange(desc(nota_imdb)) %&gt;% \n  head(10) %&gt;% \n  ggplot(aes(reorder(titulo,nota_imdb), nota_imdb))+\n  geom_col(fill=\"#650871\")+\n  coord_flip()\n\n\n\n\n\n\n\nimdb_filmes %&gt;% \n  arrange(desc(nota_imdb)) %&gt;% \n  head(10) %&gt;% \n  ggplot(aes(reorder(titulo,nota_imdb),\n             nota_imdb, \n             fill=titulo,\n             label=nota_imdb))+\n  geom_col()+\n  geom_label()+\n  coord_flip()+\n  theme_minimal()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nVamos aplicar tudo o aprendido num segundo exemplo, usando o dataframe imdb para mostrar os seriados com maior número de votos UserVotes.\n\nimdb %&gt;% \n  group_by(series_name) %&gt;% \n  summarize(Votos = sum(UserVotes)) %&gt;% \n  arrange(desc(Votos)) %&gt;% \n  ggplot(aes(reorder(series_name,\n                     Votos), \n             Votos))+\n  geom_col(fill=\"#c53a48\")+\n  coord_flip()\n\n\n\n\n\n\n\n\nVocê pode ver mais exemplos no Capítulo 8 do livro Curso-R"
  },
  {
    "objectID": "Tutorial2_gabarito.html#exemplo-base-2",
    "href": "Tutorial2_gabarito.html#exemplo-base-2",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nVamos fazer um gráfico de linha usando a função geom_line() como camada do ggplot usando o dataset netflix_filmes para mostrar o total de capítulos assistidos por dia.\n\nnetflix_filmes %&gt;%  #colocar código ggplot\n  group_by(Date) %&gt;% \n  summarize(filmes=n()) %&gt;% \n  ggplot(aes(Date,filmes))+\n  geom_point(size=5, color=\"black\")+\n  geom_line(size=1.2, color=\"#c53a48\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nPodemos agrupar contagens usando a função floor_date do pacote lubridate para meses, trimestres, etc. Adicionalmente, podemos incluir cores.\n\nlibrary(lubridate)\n\nnetflix_filmes %&gt;% \n  count(mes = floor_date(Date, \"month\")) %&gt;% \n  ggplot(aes(mes,n))+\n  geom_line(size=1.2, color=\"#502846\")"
  },
  {
    "objectID": "Tutorial2_gabarito.html#adicionando-mais-de-uma-linha",
    "href": "Tutorial2_gabarito.html#adicionando-mais-de-uma-linha",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Adicionando mais de uma linha",
    "text": "Adicionando mais de uma linha\nVamos usar o dataset imdb_filmes para comparar o desempenho dos filmes (nota imdb) em função de ter lucro ou não. Vamos adicionar também uma camada extra para nos mostrar a tendência de ambas curvas, usando geom_smooth().\n\nimdb_filmes %&gt;% \n  mutate(lucro = receita - orcamento,\n         lucro_factor = ifelse(lucro &gt; 0, \"Sim\",\"Não\")) %&gt;% \n  filter(!is.na(lucro)) %&gt;% \n  group_by(lucro_factor,ano) %&gt;% \n  summarise(nota_media=mean(nota_imdb,na.rm=TRUE)) %&gt;% \n  ggplot(aes(ano,nota_media,color=lucro_factor))+\n  geom_line(size=1.5)+\n  geom_smooth()\n\n`summarise()` has grouped output by 'lucro_factor'. You can override using the\n`.groups` argument.\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nVocê pode ver mais exemplos do geom_line() no livro Curso-R"
  },
  {
    "objectID": "Tutorial2_gabarito.html#exemplo-base-3",
    "href": "Tutorial2_gabarito.html#exemplo-base-3",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nBoxplots servem para comparar a distribuição dos dados em diferentes categorias. Vamos usar os dados do dataset imdb para comparar as notas de UserRating em cada temporada do seriado The Walking Dead.\n\nseriado_escolhido &lt;- \"The Walking Dead\"\n\nimdb %&gt;% \n  filter(series_name == seriado_escolhido) %&gt;% \n  select(1,2,4,7) %&gt;% \n  ggplot(aes(factor(season), UserRating))+\n  geom_boxplot()"
  },
  {
    "objectID": "Tutorial2_gabarito.html#adicionando-cores-por-meio-de-paletas-pré-definidas",
    "href": "Tutorial2_gabarito.html#adicionando-cores-por-meio-de-paletas-pré-definidas",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Adicionando cores por meio de paletas pré-definidas",
    "text": "Adicionando cores por meio de paletas pré-definidas\nVamos adicionar cores para cada temporada, para isto precisamos incluir o argumento fill dentro da camada de estetica aes. Podemos também escolher as cores de paletas pré-definidas como: scale_fill_viridis_d() e scale_fill_brewer()\n\nimdb %&gt;% \n  filter(series_name == seriado_escolhido) %&gt;% \n  select(1,2,4,7) %&gt;% \n  ggplot(aes(factor(season), UserRating, fill=factor(season)))+\n  geom_boxplot() + \n  scale_fill_viridis_d(alpha=0.6)"
  },
  {
    "objectID": "Tutorial2_gabarito.html#exemplo-base-4",
    "href": "Tutorial2_gabarito.html#exemplo-base-4",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nVamos continuar com os dados sobre UserRating usados anteriormente para graficar Boxplots. Neste caso, vamos graficar a distribuição do UserRating considerando apenas a contagem. Podemos usar o argumento bins ou binwidth para ajustar melhor o resultado.\n\nimdb %&gt;% \n  filter(series_name == seriado_escolhido) %&gt;% \n  select(1,2,4,7) %&gt;% \n  ggplot(aes(UserRating))+\n  geom_histogram(binwidth = 0.5)\n\n\n\n\n\n\n\n\nAgora, vamos usar o dataset imdb_filmes para verificar quão lucrativo é um determinado genero.\n\ngenero_escolhido &lt;- \"Comedy\"\n\nimdb_filmes %&gt;% \n  #filter(str_detect(string = generos, pattern = genero_escolhido)) %&gt;% \n  filter(generos == genero_escolhido) %&gt;% \n  mutate(lucro = receita - orcamento) %&gt;% \n  ggplot(aes(lucro))+\n  geom_histogram(bins = 10)\n\nWarning: Removed 345 rows containing non-finite values (`stat_bin()`)."
  },
  {
    "objectID": "Tutorial2_gabarito.html#adicionando-cores-1",
    "href": "Tutorial2_gabarito.html#adicionando-cores-1",
    "title": "Tutorial 2 - Ggplot e Tidyverse",
    "section": "Adicionando cores",
    "text": "Adicionando cores\nPodemos melhorar a apresentação, escolhendo cores diferentes adicionando o argumento fill dentro de geom_histogram().\n\nimdb_filmes %&gt;% \n  #filter(str_detect(string = generos, pattern = genero_escolhido)) %&gt;% \n  filter(generos == genero_escolhido) %&gt;% \n  mutate(lucro = receita - orcamento) %&gt;% \n  ggplot(aes(lucro))+\n  geom_histogram(bins = 50, fill=\"#cde55a\")\n\nWarning: Removed 345 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\nVocê pode ver mais exemplos do Histogramas e Boxplots no Cap. 8 do livro Curso-R\nPodemos usar alternativamente um outro tipo de visualização muito similar, que é a geom_density(). Vamos usar os mesmos exemplos anteriores.\n\nimdb_filmes %&gt;% \n  #filter(str_detect(string = generos, pattern = genero_escolhido)) %&gt;% \n  filter(generos == genero_escolhido) %&gt;% \n  mutate(lucro = receita - orcamento) %&gt;% \n  ggplot(aes(lucro))+\n  geom_density()\n\nWarning: Removed 345 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\n\n\nPodemos também customizar a cor da linha e da área abaixo da densidade usando os argumentos fill e color dentro do geom_density() bem como a transparência usando alpha.\n\nimdb_filmes %&gt;% \n  #filter(str_detect(string = generos, pattern = genero_escolhido)) %&gt;% \n  filter(generos == genero_escolhido) %&gt;% \n  mutate(lucro = receita - orcamento) %&gt;% \n  ggplot(aes(lucro))+\n  geom_density(fill=\"#e99d4e\", alpha = 0.9,\n               color = \"#e99d4e\")\n\nWarning: Removed 345 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\n\n\nE por fim, podemos comparar duas ou mais categorias. Vamos fazer isso, comparando os generos “Comedy” e “Drama” para saber se há diferenças significativas em termos de lucro. Para isto, vamos incluir o argumento group dentro da camada aes além de fill e color. Adicionalmente podemos incluir alpha dentro de geom_density().\n\nimdb_filmes %&gt;% \n  #filter(str_detect(string = generos, pattern = genero_escolhido)) %&gt;% \n  filter(generos %in% c(\"Comedy\",\"Drama\")) %&gt;% \n  mutate(lucro = receita - orcamento) %&gt;% \n  ggplot(aes(lucro, group=generos, fill=generos, color=generos))+\n  geom_density(alpha=0.5)\n\nWarning: Removed 783 rows containing non-finite values (`stat_density()`)."
  },
  {
    "objectID": "Lab4enunciado.html",
    "href": "Lab4enunciado.html",
    "title": "Lab 4 - Tidymodels - Classificação",
    "section": "",
    "text": "Carregamento dos dados\nNeste Lab usaremos dados sobre reservas de Hotéis. O objetivo é prever, dadas certas características (atributos) se uma determinada reserva inclui ou não crianças. Os dados fazem parte deste estudo.\nVamos carregar os dados e incluir apenas as reservas que não foram canceladas uma vez que mais informações são coletadas do hóspede na hora do check-in. Neste caso, os dados são automaticamente baixados do site do tidytuesday.\nVamos considerar apenas as reservas efetivadas (e não as canceladas), portanto precisamos filtrá-las. Também vamos considerar bebês e crianças dentro da mesma categoria, então iremos criar uma nova coluna ‘children’.\n\n\n# A tibble: 6 × 29\n  hotel    lead_time arrival_date_year arrival_date_month arrival_date_week_nu…¹\n  &lt;chr&gt;        &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;                               &lt;dbl&gt;\n1 Resort …       342              2015 July                                   27\n2 Resort …       737              2015 July                                   27\n3 Resort …         7              2015 July                                   27\n4 Resort …        13              2015 July                                   27\n5 Resort …        14              2015 July                                   27\n6 Resort …        14              2015 July                                   27\n# ℹ abbreviated name: ¹​arrival_date_week_number\n# ℹ 24 more variables: arrival_date_day_of_month &lt;dbl&gt;,\n#   stays_in_weekend_nights &lt;dbl&gt;, stays_in_week_nights &lt;dbl&gt;, adults &lt;dbl&gt;,\n#   children &lt;chr&gt;, meal &lt;chr&gt;, country &lt;chr&gt;, market_segment &lt;chr&gt;,\n#   distribution_channel &lt;chr&gt;, is_repeated_guest &lt;dbl&gt;,\n#   previous_cancellations &lt;dbl&gt;, previous_bookings_not_canceled &lt;dbl&gt;,\n#   reserved_room_type &lt;chr&gt;, assigned_room_type &lt;chr&gt;, …\n\n\n\n\nInstruções\nA entrega deverá incluir uma análise exploratória de dados, prévia à construção dos modelos de machine learning. Deverão ser utilizados os três modelos aprendidos: regressão logística, árvores de decisão e random forests.\nAdicionalmente, será avaliada a estrutura e as etapas de machine learning para a resolução dos problemas, incluindo 1) a separação em dados de treino e teste, 2) o uso de indicadores de acurácia específicos para classificação, 3) a escolha das variáveis a serem utilizadas para o treino e a justificativa de inclusão de cada variável, 4) uso de validação cruzada.\nO documento deverá ser entregue em formato PDF pelo moodle, ou seja, deverá ser realizado no RStudio e deverá conter, além dos scripts utilizados e dos resultados gráficos e numéricos, uma descrição das atividades realizadas (comentários que ajudem à compreensão do que foi feito)."
  },
  {
    "objectID": "pyTutorial4.html",
    "href": "pyTutorial4.html",
    "title": "Tutorial 4 - Seaborn e GeoPandas",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "pyTutorial4.html#carregando-os-pacotes",
    "href": "pyTutorial4.html#carregando-os-pacotes",
    "title": "Tutorial 4 - Seaborn e GeoPandas",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "pyTutorial4.html#lendo-os-dados",
    "href": "pyTutorial4.html#lendo-os-dados",
    "title": "Tutorial 4 - Seaborn e GeoPandas",
    "section": "Lendo os dados",
    "text": "Lendo os dados\n\n\nVocê pode baixar os dados aqui.imdb_filmes.csv\n\n\n\nimdb_filmes = pd.read_csv(\"pydata3/imdb_filmes.csv\")\n\n\nimdb_filmes.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 11340 entries, 0 to 11339\nData columns (total 20 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   id_filme              11340 non-null  object \n 1   titulo                11340 non-null  object \n 2   ano                   11339 non-null  float64\n 3   data_lancamento       11340 non-null  object \n 4   generos               11340 non-null  object \n 5   duracao               11340 non-null  int64  \n 6   pais                  11340 non-null  object \n 7   idioma                11261 non-null  object \n 8   orcamento             6389 non-null   float64\n 9   receita               6366 non-null   float64\n 10  receita_eua           5997 non-null   float64\n 11  nota_imdb             11340 non-null  float64\n 12  num_avaliacoes        11340 non-null  int64  \n 13  direcao               11340 non-null  object \n 14  roteiro               11325 non-null  object \n 15  producao              11220 non-null  object \n 16  elenco                11336 non-null  object \n 17  descricao             11326 non-null  object \n 18  num_criticas_publico  11330 non-null  float64\n 19  num_criticas_critica  11300 non-null  float64\ndtypes: float64(7), int64(2), object(11)\nmemory usage: 1.7+ MB\n\n\n\nimdb_filmes.describe()\n\n\n\n\n\n\n\n\nano\nduracao\norcamento\nreceita\nreceita_eua\nnota_imdb\nnum_avaliacoes\nnum_criticas_publico\nnum_criticas_critica\n\n\n\n\ncount\n11339.000000\n11340.000000\n6.389000e+03\n6.366000e+03\n5.997000e+03\n11340.000000\n1.134000e+04\n11330.000000\n11300.000000\n\n\nmean\n1989.452244\n99.691975\n1.903052e+07\n5.468264e+07\n3.147534e+07\n6.102487\n3.591951e+04\n138.221977\n65.660885\n\n\nstd\n25.548495\n17.649559\n3.238295e+07\n1.433980e+08\n6.033264e+07\n1.128953\n1.051027e+05\n303.109281\n85.890610\n\n\nmin\n1914.000000\n45.000000\n0.000000e+00\n1.600000e+01\n2.520000e+02\n1.300000\n1.001000e+03\n1.000000\n1.000000\n\n\n25%\n1973.000000\n89.000000\n1.500000e+06\n5.030680e+05\n1.007583e+06\n5.500000\n1.877000e+03\n31.000000\n17.000000\n\n\n50%\n1997.000000\n97.000000\n6.500000e+06\n9.521381e+06\n1.067257e+07\n6.300000\n4.736500e+03\n55.000000\n35.000000\n\n\n75%\n2011.000000\n108.000000\n2.200000e+07\n4.309351e+07\n3.600150e+07\n6.900000\n2.106250e+04\n127.000000\n78.000000\n\n\nmax\n2020.000000\n271.000000\n3.560000e+08\n2.797801e+09\n9.366622e+08\n9.300000\n2.278845e+06\n8869.000000\n909.000000\n\n\n\n\n\n\n\n\nimdb_filmes.sample(8)\n\n\n\n\n\n\n\n\nid_filme\ntitulo\nano\ndata_lancamento\ngeneros\nduracao\npais\nidioma\norcamento\nreceita\nreceita_eua\nnota_imdb\nnum_avaliacoes\ndirecao\nroteiro\nproducao\nelenco\ndescricao\nnum_criticas_publico\nnum_criticas_critica\n\n\n\n\n9250\ntt0119223\nGreat Expectations\n1998.0\n1998-03-02\nDrama, Romance\n111\nUSA\nEnglish, French, Portuguese\n25000000.0\n55494066.0\n26420672.0\n6.9\n50455\nAlfonso Cuarón\nCharles Dickens, Mitch Glazer\nArt Linson Productions\nEthan Hawke, Gwyneth Paltrow, Hank Azaria, Chr...\nModernization of Charles Dickens classic story...\n208.0\n81.0\n\n\n6397\ntt0145529\nTime Chasers\n1994.0\n1994-03-17\nSci-Fi\n89\nUSA\nEnglish\n150000.0\nNaN\nNaN\n2.4\n3041\nDavid Giancola\nDavid Giancola\nEdgewood Studios\nMatthew Bruch, Bonnie Pritchard, Peter Harring...\nAn inventor comes up with a time machine, but ...\n89.0\n7.0\n\n\n7101\ntt0058371\nThe Moon-Spinners\n1964.0\n1964-07-08\nAdventure, Family, Mystery\n118\nUSA\nEnglish, Greek, Spanish\n5000000.0\nNaN\nNaN\n6.7\n2137\nJames Neilson\nMichael Dyne, Mary Stewart\nWalt Disney Productions\nHayley Mills, Eli Wallach, Peter McEnery, Joan...\nA teenager encounters romance, intrigue and a ...\n42.0\n10.0\n\n\n10576\ntt0198781\nMonsters, Inc.\n2001.0\n2002-03-15\nAnimation, Adventure, Comedy\n92\nUSA\nEnglish\n115000000.0\n578981070.0\n289916256.0\n8.0\n794964\nPete Docter, David Silverman\nPete Docter, Jill Culton\nPixar Animation Studios\nJohn Goodman, Billy Crystal, Mary Gibbs, Steve...\nIn order to power the city, monsters have to s...\n687.0\n264.0\n\n\n10292\ntt1713476\nThe Bay\n2012.0\n2013-06-06\nHorror, Sci-Fi, Thriller\n84\nUSA\nEnglish\nNaN\n1581252.0\n30668.0\n5.6\n24893\nBarry Levinson\nMichael Wallach, Barry Levinson\nAutomatik Entertainment\nNansi Aluka, Christopher Denham, Stephen Kunke...\nChaos breaks out in a small Maryland town afte...\n152.0\n223.0\n\n\n1260\ntt3203620\nThe Dinner\n2017.0\n2017-05-18\nCrime, Drama, Thriller\n120\nUSA\nEnglish\nNaN\n2544921.0\n1323312.0\n4.5\n7518\nOren Moverman\nHerman Koch, Oren Moverman\nChubbCo Film\nMichael Chernus, Taylor Rae Almonte, Steve Coo...\nTwo sets of wealthy parents meet for dinner to...\n193.0\n120.0\n\n\n1126\ntt0087795\nNight Patrol\n1984.0\n1984-11-16\nComedy\n85\nUSA\nFrench, English\n4500000.0\nNaN\nNaN\n4.9\n1314\nJackie Kong\nMurray Langston, William A. Levey\nRSL\nLinda Blair, Pat Paulsen, Jaye P. Morgan, Jack...\nA hapless police officer is transferred to the...\n34.0\n16.0\n\n\n117\ntt0064683\nMondo Trasho\n1969.0\n1969-03-14\nComedy\n95\nUSA\nEnglish\n2100.0\nNaN\nNaN\n6.2\n1169\nJohn Waters\nJohn Waters\nDreamland\nMary Vivian Pearce, Divine, David Lochary, Min...\nA day in the lives of a hit-and-run driver and...\n22.0\n18.0"
  },
  {
    "objectID": "pyTutorial4.html#limpeza-dos-dados",
    "href": "pyTutorial4.html#limpeza-dos-dados",
    "title": "Tutorial 4 - Seaborn e GeoPandas",
    "section": "Limpeza dos dados",
    "text": "Limpeza dos dados\nVamos converter a coluna data_lancamento para formato data.\n\n# Get all unique non-date values from `data_lancamento`\nnon_date_values = imdb_filmes[pd.to_datetime(imdb_filmes['data_lancamento'], errors='coerce').isna()]['data_lancamento'].unique() \n\nif (len(non_date_values) &gt; 20):\n  # Sample 20 of them if there are too many unique values\n  print(f\"Non-date values in `data_lancamento`: {np.random.choice(non_date_values, 20, replace=False)}\")\nelse:\n  # Otherwise print all unique non-date values from `data_lancamento`\n  print(f\"Non-date values in `data_lancamento`: {non_date_values}\")\n\nNon-date values in `data_lancamento`: ['1934' '1988' '1922' '1996' '1950' '1974' '1972' '1977' '1978' '1936'\n '1998' '2018' '1959' '1997' '1966' '1938' '1954' '2006' '2011' '1951']\n\n\n\nimdb_filmes['data_lancamento']=pd.to_datetime(imdb_filmes['data_lancamento'], format='%Y-%m-%d',\nerrors='coerce')"
  },
  {
    "objectID": "pyTutorial4.html#gráfico-de-dispersão",
    "href": "pyTutorial4.html#gráfico-de-dispersão",
    "title": "Tutorial 4 - Seaborn e GeoPandas",
    "section": "Gráfico de Dispersão",
    "text": "Gráfico de Dispersão\nQuando há necessidade de encontrar correlações, são utilizados os gráficos de dispersão. Se existir um conjunto de dados XY, então um gráfico de dispersão é utilizado para encontrar a relação entre as variáveis X e Y.\nAs coordenadas x e y devem ser numéricas necessariamente. Vamos usar como exemplo, o dataframe imdb_filmes.\nNo exemplo, a posição do ponto no eixo x pode ser dada pela coluna orcamento e a posição do ponto no eixo y pela coluna receita.\n\nsns.scatterplot(data=imdb_filmes, x='orcamento', y='receita')\n\n\n\n\n\n\n\n\nDeixando o gráfico mais informativo:\n\nsns.scatterplot(data=imdb_filmes, x='orcamento', y='receita', hue='ano')\n\nplt.title('Gráfico de Dispersão entre Orçamento x Receita')\nplt.xlabel('Orçamento (x 100 Mi UDS)')\nplt.ylabel('Receita (Bi USD)')\nplt.show()"
  },
  {
    "objectID": "pyTutorial4.html#gráfico-de-barras",
    "href": "pyTutorial4.html#gráfico-de-barras",
    "title": "Tutorial 4 - Seaborn e GeoPandas",
    "section": "Gráfico de Barras",
    "text": "Gráfico de Barras\nUm gráfico de barras também exibe tendências ao longo do tempo. No caso de múltiplas variáveis, um gráfico de barras pode facilitar a comparação dos dados para cada variável em todos os momentos no tempo. Por exemplo, um gráfico de barras pode ser utilizado para comparar o crescimento da empresa ano a ano.\nVamos usar o dataframe imdb_filmes para exemplificar, ordenando as linhas por ordem decrescente de nota_imdb. Primeiro vamos gerar o gráfico com as configuração padrão.\n\nimdb_summary = imdb_filmes.sort_values('nota_imdb',\nascending=False).head(8)\n\n\nsns.barplot(imdb_summary, x='titulo', y='nota_imdb')\n\n\n\n\n\n\n\n\nMas os nomes dos filmes estão sobrepostos, então neste caso é recomendável realizar um gráfico de barras horizontais.\n\nsns.barplot(imdb_summary, x='nota_imdb', y='titulo')\nplt.title('Top 10 Filmes por Nota IMDB')\nplt.xlabel('Nota IMDB')\nplt.show()"
  },
  {
    "objectID": "pyTutorial4.html#gráfico-de-linha",
    "href": "pyTutorial4.html#gráfico-de-linha",
    "title": "Tutorial 4 - Seaborn e GeoPandas",
    "section": "Gráfico de linha",
    "text": "Gráfico de linha\nGráficos de linhas são utilizados para exibir tendências ao longo do tempo. O eixo X é geralmente utilizado para representar um período, enquanto o eixo Y é utilizado para representar a quantidade associada ao período de tempo no eixo X. Por exemplo, um gráfico de linhas pode ilustrar o horário de pico de visitas em um shopping durante o dia, dividido por dias da semana e horas.\nVamos fazer um gráfico utilizando a coluna data_lancamento\n\nimdb_lancamento = imdb_filmes.groupby('data_lancamento').agg({'titulo':\"count\",'nota_imdb':'mean','duracao':'mean'})\n\nimdb_lancamento.head(3)\n\n\n\n\n\n\n\n\ntitulo\nnota_imdb\nduracao\n\n\ndata_lancamento\n\n\n\n\n\n\n\n1914-03-08\n1\n6.1\n61.0\n\n\n1914-08-24\n1\n6.4\n78.0\n\n\n1914-12-21\n1\n6.3\n82.0\n\n\n\n\n\n\n\nVamos agrupar os filmes por ano.\n\nimdb_anual = imdb_filmes.groupby(pd.Grouper(key='data_lancamento', freq='Y'))['nota_imdb'].mean()\n\nimdb_anual.head(3)\n\ndata_lancamento\n1914-12-31    6.266667\n1915-12-31    6.566667\n1916-12-31    6.200000\nName: nota_imdb, dtype: float64\n\n\n\nimdb_anual_df = pd.DataFrame(imdb_anual).reset_index()\n\nimdb_anual_df['data_lancamento'] = pd.to_datetime(imdb_anual_df['data_lancamento'], format='%Y-%m-%d')\n\nsns.lineplot(x='data_lancamento', y='nota_imdb', data=imdb_anual_df)\n\n\n\n\n\n\n\n\nTambém podemos plotar duas ou mais linhas. Vamos calcular intervalo interquartil e inclui-la no gráfico anterior."
  },
  {
    "objectID": "pyTutorial4.html#gráfico-de-boxplot",
    "href": "pyTutorial4.html#gráfico-de-boxplot",
    "title": "Tutorial 4 - Seaborn e GeoPandas",
    "section": "Gráfico de Boxplot",
    "text": "Gráfico de Boxplot\nUm gráfico de boxplot, também conhecido como diagrama de caixa, é uma ferramenta de visualização estatística que fornece uma representação compacta e informativa da distribuição de um conjunto de dados. Consiste em um retângulo (“caixa”) que se estende de um quartil ao outro, com uma linha vertical (ou “whisker”) estendendo-se de cada extremidade da caixa para representar a amplitude dos dados fora do intervalo interquartil.\n\nimdb_filmes['ano'] = imdb_filmes['data_lancamento'].dt.year\n\nimdb_filmes['decada'] = imdb_filmes['ano'] // 10 * 10\n\nimdb_filmes['decada'].astype('category')\n\n0        1980.0\n1        1940.0\n2        2000.0\n3        1940.0\n4        2000.0\n          ...  \n11335       NaN\n11336    2010.0\n11337    1950.0\n11338    2010.0\n11339    2000.0\nName: decada, Length: 11340, dtype: category\nCategories (12, float64): [1910.0, 1920.0, 1930.0, 1940.0, ..., 1990.0, 2000.0, 2010.0, 2020.0]\n\n\n\nsns.boxplot(x='decada', y='nota_imdb', data=imdb_filmes, showmeans=True)\nplt.title('Distribuição de notas IMDB por Década')\nplt.ylabel('IMDb Rating')\nplt.show()\n\n\n\n\n\n\n\n\nO gráfico ficou ruim de visualizar, porém, é possível alterar as dimensões de qualquer gráfico matplotlib, utilizando a opção: plt.figure(figsize=(15, 8)). Também podemos mudar a cor de cada caixa.\n\nplt.figure(figsize=(15, 8))\nsns.boxplot(x='decada', y='nota_imdb', hue='decada', data=imdb_filmes, showmeans=True)\nplt.title('Distribuição de notas IMDB por Década')\nplt.ylabel('IMDb Rating')\nplt.show()"
  },
  {
    "objectID": "pyTutorial4.html#gráfico-de-histograma",
    "href": "pyTutorial4.html#gráfico-de-histograma",
    "title": "Tutorial 4 - Seaborn e GeoPandas",
    "section": "Gráfico de Histograma",
    "text": "Gráfico de Histograma\nUm histograma representa dados usando barras de alturas diferentes. Geralmente, cada barra agrupa números em intervalos em um histograma. Quanto mais alta as barras, mais dados se encaixam nesse intervalo. É usado para exibir a forma e a dispersão de amostras de um conjunto de dados contínuos. Por exemplo, podemos usar um histograma para medir as frequências de resposta da variável nota_imdb .\nO Histograma serve para observar a distribuição dos dados e eventualmente serve para comparar mais de uma distribuição.\n\nsns.histplot(data=imdb_filmes, x='nota_imdb', \nbins=30, color='darkblue')\nplt.xlabel('Nota IMDb')\nplt.show()"
  },
  {
    "objectID": "pyTutorial4.html#pacote-geobr-para-mapas-do-brasil",
    "href": "pyTutorial4.html#pacote-geobr-para-mapas-do-brasil",
    "title": "Tutorial 4 - Seaborn e GeoPandas",
    "section": "Pacote geobr para mapas do Brasil",
    "text": "Pacote geobr para mapas do Brasil\nVamos baixar os dados do Brasil por Estados.\n\nimport numpy as np\nimport geobr\nimport geopandas as gpd\n\nstates = geobr.read_state(year=2019)\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/geopandas/array.py:93: ShapelyDeprecationWarning:\n\n__len__ for multi-part geometries is deprecated and will be removed in Shapely 2.0. Check the length of the `geoms` property instead to get the  number of parts of a multi-part geometry.\n\n\n\n\nfig, ax = plt.subplots(figsize=(15, 15), dpi=300)\nstates.plot(facecolor=\"#2D3E50\", edgecolor=\"#FEBF57\", ax=ax)\nax.set_title(\"Estados do Brasil\", fontsize=20)\nplt.show()\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/geopandas/plotting.py:33: ShapelyDeprecationWarning:\n\nIteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning:\n\nThe array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/descartes/patch.py:64: ShapelyDeprecationWarning:\n\nThe array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n\n\n\n\n\n\n\n\n\n\nOutra forma de trabalhar manualmente o mapa é utilizando a library geopandas e importando diretamente os dados geográficos.\n\n\nVocê pode baixar os dados aqui..DS_Storeindia.zipbr_states_lifexpect2017.csvimportIN.xlsxbcim_2016_21_11_2018.gpkgindia\n\n\nVamos baixar os dados do Brasil do seguinte endereço:\nwww.ibge.gov.br –&gt; geociências –&gt; downloads –&gt; cartas e mapas –&gt; bases cartográficas contínuas –&gt; bcim –&gt; versao 2016 –&gt; geopackage\n\nmapa = gpd.read_file('pydata4/bcim_2016_21_11_2018.gpkg', layer='lim_unidade_federacao_a')\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/geopandas/array.py:93: ShapelyDeprecationWarning:\n\n__len__ for multi-part geometries is deprecated and will be removed in Shapely 2.0. Check the length of the `geoms` property instead to get the  number of parts of a multi-part geometry.\n\n\n\n\nmapa.head()\n\n\n\n\n\n\n\n\nnome\nnomeabrev\ngeometriaaproximada\nsigla\ngeocodigo\nid_produtor\nid_elementoprodutor\ncd_insumo_orgao\nnr_insumo_mes\nnr_insumo_ano\ntx_insumo_documento\ngeometry\n\n\n\n\n0\nGoiás\nNone\nSim\nGO\n52\n1000001\nNone\nNaN\nNone\nNone\nNone\nMULTIPOLYGON (((-50.15876 -12.41581, -50.15743...\n\n\n1\nMato Grosso do Sul\nNone\nSim\nMS\n50\n1000001\nNone\nNaN\nNone\nNone\nNone\nMULTIPOLYGON (((-56.09815 -17.17220, -56.09159...\n\n\n2\nParaná\nNone\nSim\nPR\n41\n1000001\nNone\nNaN\nNone\nNone\nNone\nMULTIPOLYGON (((-52.08090 -22.52893, -52.04903...\n\n\n3\nMinas Gerais\nNone\nSim\nMG\n31\n1000001\nNone\nNaN\nNone\nNone\nNone\nMULTIPOLYGON (((-44.21152 -14.22955, -44.20750...\n\n\n4\nSergipe\nNone\nSim\nSE\n28\n1000001\nNone\nNaN\nNone\nNone\nNone\nMULTIPOLYGON (((-38.00366 -9.51544, -38.00052 ...\n\n\n\n\n\n\n\nVamos a plotar o mapa do Brasil\n\nmapa.plot()\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/geopandas/plotting.py:33: ShapelyDeprecationWarning:\n\nIteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning:\n\nThe array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/descartes/patch.py:64: ShapelyDeprecationWarning:\n\nThe array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n\n\n\n\n\n\n\n\n\n\nAgora queremos mapear alguma informação. Como exemplo, vamos pegar dados da expectativa de vida para os estados brasileiros:\n\nvida = pd.read_csv('pydata4/br_states_lifexpect2017.csv')\nvida.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 27 entries, 0 to 26\nData columns (total 3 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   X            27 non-null     int64  \n 1   uf           27 non-null     object \n 2   ESPVIDA2017  27 non-null     float64\ndtypes: float64(1), int64(1), object(1)\nmemory usage: 776.0+ bytes\n\n\nVamos unir as informações dos dois dataframes.\n\nbr = pd.merge(mapa, vida, left_on='nome',right_on='uf', how='left')\nbr= br.fillna(73)\n\nFinalmente, vamos plotar o mapa:\n\nvmax = np.max(br['ESPVIDA2017'])\nvmin=np.min(br['ESPVIDA2017'])\n\nbr.plot(column='ESPVIDA2017',\n        legend=True,\n        cmap='Reds',\n        vmax=vmax,\n        vmin=70)\nplt.show()\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/geopandas/plotting.py:33: ShapelyDeprecationWarning:\n\nIteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning:\n\nThe array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/descartes/patch.py:64: ShapelyDeprecationWarning:\n\nThe array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead."
  },
  {
    "objectID": "pyTutorial4.html#exemplo-com-dados-de-outro-pais",
    "href": "pyTutorial4.html#exemplo-com-dados-de-outro-pais",
    "title": "Tutorial 4 - Seaborn e GeoPandas",
    "section": "Exemplo com dados de outro pais",
    "text": "Exemplo com dados de outro pais\n\nmapa_india = gpd.read_file('pydata4/india/india-polygon.shp')\n\nmapa_india.head(3)\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/geopandas/array.py:93: ShapelyDeprecationWarning:\n\n__len__ for multi-part geometries is deprecated and will be removed in Shapely 2.0. Check the length of the `geoms` property instead to get the  number of parts of a multi-part geometry.\n\n\n\n\n\n\n\n\n\n\nid\nst_nm\ngeometry\n\n\n\n\n0\nNone\nAndaman and Nicobar Islands\nMULTIPOLYGON (((93.84831 7.24028, 93.92705 7.0...\n\n\n1\nNone\nArunachal Pradesh\nPOLYGON ((95.23643 26.68105, 95.19594 27.03612...\n\n\n2\nNone\nAssam\nPOLYGON ((95.19594 27.03612, 95.08795 26.94578...\n\n\n\n\n\n\n\n\nmapa_india.plot()\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/geopandas/plotting.py:33: ShapelyDeprecationWarning:\n\nIteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning:\n\nThe array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/descartes/patch.py:64: ShapelyDeprecationWarning:\n\nThe array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n\n\n\n\n\n\n\n\n\n\nVamos mapear os dados provenientes de:\n\ndados_india = pd.read_excel('pydata4/importIN.xlsx')\ndados_india.head(3)\n\ndados_india2023 = dados_india[['Year',2023]]\n\nJuntando os dados\n\nmapa_india_2023 = pd.merge(mapa_india, dados_india2023,\nleft_on = 'st_nm', right_on='Year', how='left')\n\nmapa_india_2023.head(3)\n\n\n\n\n\n\n\n\nid\nst_nm\ngeometry\nYear\n2023\n\n\n\n\n0\nNone\nAndaman and Nicobar Islands\nMULTIPOLYGON (((93.84831 7.24028, 93.92705 7.0...\nNaN\nNaN\n\n\n1\nNone\nArunachal Pradesh\nPOLYGON ((95.23643 26.68105, 95.19594 27.03612...\nArunachal Pradesh\n11.64\n\n\n2\nNone\nAssam\nPOLYGON ((95.19594 27.03612, 95.08795 26.94578...\nAssam\n147.92\n\n\n\n\n\n\n\nAgora vamos plotar:\n\nnp.min(mapa_india_2023[2023])\n\n3.04\n\n\n\nnp.min(mapa_india_2023[2023])\n\nmapa_india_2023.plot(column=2023,\n        missing_kwds={'color':'lightgrey'},\n        legend=True,\n        cmap='YlOrRd',\n        linewidth=0.2,\n        edgecolor='0',\n        vmin=0).set_axis_off()\nplt.title('Capacidade Instalada Solar na India em 2023 (em MW)')\nplt.show()\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/geopandas/plotting.py:33: ShapelyDeprecationWarning:\n\nIteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning:\n\nThe array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/descartes/patch.py:64: ShapelyDeprecationWarning:\n\nThe array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/geopandas/plotting.py:33: ShapelyDeprecationWarning:\n\nIteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning:\n\nThe array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead."
  },
  {
    "objectID": "pyTutorial5b.html",
    "href": "pyTutorial5b.html",
    "title": "Tutorial 5b - Mapas Interativos",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport json\nimport ipyleaflet\nimport os\nimport requests\nfrom ipywidgets import link, FloatSlider\nfrom branca.colormap import linear\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020"
  },
  {
    "objectID": "pyTutorial5b.html#carregando-os-pacotes",
    "href": "pyTutorial5b.html#carregando-os-pacotes",
    "title": "Tutorial 5b - Mapas Interativos",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport json\nimport ipyleaflet\nimport os\nimport requests\nfrom ipywidgets import link, FloatSlider\nfrom branca.colormap import linear\n\n/Users/mauricio/Library/Python/3.8/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning:\n\nurllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020"
  },
  {
    "objectID": "pyTutorial7.html",
    "href": "pyTutorial7.html",
    "title": "Tutorial 7 - Métodos de Regressão",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import LabelEncoder"
  },
  {
    "objectID": "pyTutorial7.html#carregando-os-pacotes",
    "href": "pyTutorial7.html#carregando-os-pacotes",
    "title": "Tutorial 7 - Métodos de Regressão",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import LabelEncoder"
  },
  {
    "objectID": "pyTutorial7.html#lendo-os-dados",
    "href": "pyTutorial7.html#lendo-os-dados",
    "title": "Tutorial 7 - Métodos de Regressão",
    "section": "Lendo os dados",
    "text": "Lendo os dados\nUsaremos dados do Ikea, uma loja de móveis com filiais em várias partes do mundo.\nO próposito é prever o preço dos móveis vendidos na IKEA a partir de várias características destes produtos como a categoria e o tamanho do móvel, conforme aqui.\nVamos carregar os dados e ver as primeiras linhas.\n\ndf = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-11-03/ikea.csv')\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3694 entries, 0 to 3693\nData columns (total 14 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Unnamed: 0         3694 non-null   int64  \n 1   item_id            3694 non-null   int64  \n 2   name               3694 non-null   object \n 3   category           3694 non-null   object \n 4   price              3694 non-null   float64\n 5   old_price          3694 non-null   object \n 6   sellable_online    3694 non-null   bool   \n 7   link               3694 non-null   object \n 8   other_colors       3694 non-null   object \n 9   short_description  3694 non-null   object \n 10  designer           3694 non-null   object \n 11  depth              2231 non-null   float64\n 12  height             2706 non-null   float64\n 13  width              3105 non-null   float64\ndtypes: bool(1), float64(4), int64(2), object(7)\nmemory usage: 378.9+ KB\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nitem_id\nname\ncategory\nprice\nold_price\nsellable_online\nlink\nother_colors\nshort_description\ndesigner\ndepth\nheight\nwidth\n\n\n\n\n0\n0\n90420332\nFREKVENS\nBar furniture\n265.0\nNo old price\nTrue\nhttps://www.ikea.com/sa/en/p/frekvens-bar-tabl...\nNo\nBar table, in/outdoor, 51x51 cm\nNicholai Wiig Hansen\nNaN\n99.0\n51.0\n\n\n1\n1\n368814\nNORDVIKEN\nBar furniture\n995.0\nNo old price\nFalse\nhttps://www.ikea.com/sa/en/p/nordviken-bar-tab...\nNo\nBar table, 140x80 cm\nFrancis Cayouette\nNaN\n105.0\n80.0\n\n\n2\n2\n9333523\nNORDVIKEN / NORDVIKEN\nBar furniture\n2095.0\nNo old price\nFalse\nhttps://www.ikea.com/sa/en/p/nordviken-nordvik...\nNo\nBar table and 4 bar stools\nFrancis Cayouette\nNaN\nNaN\nNaN\n\n\n3\n3\n80155205\nSTIG\nBar furniture\n69.0\nNo old price\nTrue\nhttps://www.ikea.com/sa/en/p/stig-bar-stool-wi...\nYes\nBar stool with backrest, 74 cm\nHenrik Preutz\n50.0\n100.0\n60.0\n\n\n4\n4\n30180504\nNORBERG\nBar furniture\n225.0\nNo old price\nTrue\nhttps://www.ikea.com/sa/en/p/norberg-wall-moun...\nNo\nWall-mounted drop-leaf table, ...\nMarcus Arvonen\n60.0\n43.0\n74.0"
  },
  {
    "objectID": "pyTutorial7.html#limpeza-dos-dados",
    "href": "pyTutorial7.html#limpeza-dos-dados",
    "title": "Tutorial 7 - Métodos de Regressão",
    "section": "Limpeza dos dados",
    "text": "Limpeza dos dados\nTambém vamos fazer uma revisão geral dos dados.\nConforme visto no output do info() há várias colunas em formato de string e algumas colunas que apenas são um id de cada linha, além de termos valores NULL. Precisamos limpar o dataset para deixá-lo mais adequado ao modelo de machine learning.\n\nikea = df[['price','category','sellable_online','depth','height','width']].dropna()\n\nikea['category'] = ikea['category'].astype('category')\n\nikea.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1899 entries, 3 to 3688\nData columns (total 6 columns):\n #   Column           Non-Null Count  Dtype   \n---  ------           --------------  -----   \n 0   price            1899 non-null   float64 \n 1   category         1899 non-null   category\n 2   sellable_online  1899 non-null   bool    \n 3   depth            1899 non-null   float64 \n 4   height           1899 non-null   float64 \n 5   width            1899 non-null   float64 \ndtypes: bool(1), category(1), float64(4)\nmemory usage: 78.6 KB"
  },
  {
    "objectID": "pyTutorial7.html#análise-exploratória-de-dados",
    "href": "pyTutorial7.html#análise-exploratória-de-dados",
    "title": "Tutorial 7 - Métodos de Regressão",
    "section": "Análise Exploratória de Dados",
    "text": "Análise Exploratória de Dados\nAnálise de Correlação.\n\nnum_cols = ikea.select_dtypes(include=['int64', 'float64'])\ncorr_matrix = num_cols.corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\nplt.title('Heatmap de Correlações')\nplt.show()\n\n\n\n\n\n\n\n\nGráficos de Dispersão com o preço\n\nsns.scatterplot(data=ikea, x='depth', y='price')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=ikea, x='width', y='price')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=ikea, x='height', y='price')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=ikea, x='sellable_online', y='price')\nplt.show()\n\n\n\n\n\n\n\n\ncomo praticamente todos os produtos se vendem online, vamos eliminar essa coluna.\n\ncounts = ikea['sellable_online'].value_counts()\n\nsns.countplot(x='sellable_online', data=ikea)\nplt.show()\n\n\n\n\n\n\n\n\n\nikea = ikea.drop(['sellable_online'], axis=1)\n\nikea.head()\n\n\n\n\n\n\n\n\nprice\ncategory\ndepth\nheight\nwidth\n\n\n\n\n3\n69.0\nBar furniture\n50.0\n100.0\n60.0\n\n\n4\n225.0\nBar furniture\n60.0\n43.0\n74.0\n\n\n5\n345.0\nBar furniture\n45.0\n91.0\n40.0\n\n\n6\n129.0\nBar furniture\n44.0\n95.0\n50.0\n\n\n8\n129.0\nBar furniture\n44.0\n95.0\n50.0\n\n\n\n\n\n\n\nVamos ver o preço:\n\nsns.histplot(data=ikea, x='price', \nbins=30, color='darkblue')\nplt.xlabel('Preço')\nplt.show()\n\n\n\n\n\n\n\n\nComo o preço está muito agregaado aos valores mais baixos, é necessário transformar a coluna, p.ex. utilizando log10.\n\nikea['log_price'] = np.log10(ikea['price'])\n\nikea = ikea.drop(['price'], axis=1)\n\nikea.head()\n\n\n\n\n\n\n\n\ncategory\ndepth\nheight\nwidth\nlog_price\n\n\n\n\n3\nBar furniture\n50.0\n100.0\n60.0\n1.838849\n\n\n4\nBar furniture\n60.0\n43.0\n74.0\n2.352183\n\n\n5\nBar furniture\n45.0\n91.0\n40.0\n2.537819\n\n\n6\nBar furniture\n44.0\n95.0\n50.0\n2.110590\n\n\n8\nBar furniture\n44.0\n95.0\n50.0\n2.110590\n\n\n\n\n\n\n\n\nsns.histplot(data=ikea, x='log_price', \nbins=30, color='darkblue')\nplt.xlabel('Log10 Preço')\nplt.show()\n\n\n\n\n\n\n\n\nAntes de iniciar a modelagem, precisamos codificar a(s) coluna(s) categóricas, neste caso, temos uma única coluna, chamada category.\n\nlabel_encoder = LabelEncoder()\nikea['category_encoded'] = label_encoder.fit_transform(ikea['category'])\n\n\nikea = ikea.drop('category', axis=1)"
  },
  {
    "objectID": "pyTutorial7.html#modelagem-supervisionada",
    "href": "pyTutorial7.html#modelagem-supervisionada",
    "title": "Tutorial 7 - Métodos de Regressão",
    "section": "Modelagem Supervisionada",
    "text": "Modelagem Supervisionada\nVamos dividir o dataset em treino e teste:\n\nX = ikea.drop('log_price', axis=1)  # Features \ny = ikea['log_price']  # Target variable\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n\n\nÁrvores de Decisão (Decision Trees)\nVamos treinar o modelo de árvore de decisão.\n\nfrom sklearn.tree import DecisionTreeRegressor\n\n\ntree_model = DecisionTreeRegressor(random_state=42)  \ntree_model.fit(X_train, y_train)\n\nDecisionTreeRegressor(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(random_state=42)\n\n\nVamos criar uma nova coluna com os resultados da predição:\n\ny_pred = tree_model.predict(X_test)\n\nPor fim, vamos avaliar o modelo de árvore que treinamos, para isso vamos usar os dados de teste.\n\nmse_tree = mean_squared_error(y_test, y_pred)\nr2_tree = r2_score(y_test, y_pred)\nrmse_tree = np.sqrt(mse_tree)\n\nprint(f'Mean Squared Error: {mse_tree}')\nprint(f'R-squared: {r2_tree}')\nprint(f'RMSE: {rmse_tree}')\n\nMean Squared Error: 0.06556498700053338\nR-squared: 0.7911592645549473\nRMSE: 0.25605660897647886\n\n\nOs resultados parecem satisfatórios, vamos fazer um gráfico para visualizar o resultado do modelo.\n\nplt.scatter(y_test, y_pred, alpha=0.5)  \nplt.xlabel(\"Log Preço verdadeiro (y_test)\")\nplt.ylabel(\"Log Preço predito (y_pred)\")\nplt.title(\"Precisão do modelo para Árvore de Decisão\")\n\n# Addicionar linha diagonal\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n\n\nplt.show()\n\n\n\n\n\n\n\n\nvamos criar um dataframe com os indicadores para compará-los com os outros modelos.\n\nindicadores = pd.DataFrame({\n  'R2':[r2_tree],\n  'MSE':[mse_tree],\n  'RMSE':[rmse_tree],\n  'Modelo':['Decision Tree']\n  \n})\n\nindicadores\n\n\n\n\n\n\n\n\nR2\nMSE\nRMSE\nModelo\n\n\n\n\n0\n0.791159\n0.065565\n0.256057\nDecision Tree\n\n\n\n\n\n\n\n\n\nRandom Forests\nAgora vamos treinar um modelo de RF.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf_model = RandomForestRegressor(random_state=42) \nrf_model.fit(X_train, y_train)\n\nRandomForestRegressor(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\nVamos calcular a precisão com os dados de teste.\n\ny_pred_rf = rf_model.predict(X_test)\n\nmse_rf = mean_squared_error(y_test, y_pred_rf)\nr2_rf = r2_score(y_test, y_pred_rf)\nrmse_rf = np.sqrt(mse_rf)\n\nprint(f'Random Forest Mean Squared Error: {mse_rf}')\nprint(f'Random Forest R-squared: {r2_rf}')\nprint(f'Random Forest RMSE: {rmse_rf}')\n\nRandom Forest Mean Squared Error: 0.04567220496347414\nRandom Forest R-squared: 0.8545227062442409\nRandom Forest RMSE: 0.21371056352804402\n\n\nNovamente vamos fazer um gráfico para visualizar o desempenho do modelo RF.\n\nplt.scatter(y_test, y_pred_rf, alpha=0.5)  \nplt.xlabel(\"Log Preço verdadeiro (y_test)\")\nplt.ylabel(\"Log Preço predito (y_pred)\")\nplt.title(\"Precisão do modelo para Random Forest\")\n\n# Addicionar linha diagonal\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n\n\nplt.show()\n\n\n\n\n\n\n\n\nE vamos incluir os indicadores do RF no nosso dataframe indicadores.\n\nrf_df = pd.DataFrame({\n  'R2':[r2_rf],\n  'MSE':[mse_rf],\n  'RMSE':[rmse_rf],\n  'Modelo':['Random Forest']\n  \n})\n\nindicadores = pd.concat([indicadores, rf_df], ignore_index=True)\n\nindicadores\n\n\n\n\n\n\n\n\nR2\nMSE\nRMSE\nModelo\n\n\n\n\n0\n0.791159\n0.065565\n0.256057\nDecision Tree\n\n\n1\n0.854523\n0.045672\n0.213711\nRandom Forest\n\n\n\n\n\n\n\n\n\nSupport Vector Regression (SVR)\nPrimeiro precisamos normalizar os valores dos atributos.\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nAgora vamos treinar um modelo SVR.\n\nfrom sklearn.svm import SVR\n\nAgora podemos treinar o modelo. Têm três tipos de modelos SVR: ‘linear’ é como se fosse uma regressão linear; ‘poly’ é como se fosse uma regressão polinomial; ‘rbf’ uma regressão não-linear.\n\nsvr_model = SVR(kernel = 'rbf')\n\nsvr_model.fit(X_train, y_train)\n\nSVR()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVRSVR()\n\n\nVamos calcular a precisão com os dados de teste.\n\ny_pred_svr = svr_model.predict(X_test)\n\nmse_svr = mean_squared_error(y_test, y_pred_svr)\nr2_svr = r2_score(y_test, y_pred_svr)\nrmse_svr = np.sqrt(mse_svr)\n\nprint(f'SVR Mean Squared Error: {mse_svr}')\nprint(f'SVR R-squared: {r2_svr}')\nprint(f'SVR RMSE: {rmse_svr}')\n\nSVR Mean Squared Error: 0.06872190022184728\nSVR R-squared: 0.7811037134287031\nSVR RMSE: 0.2621486223916641\n\n\nNovamente vamos fazer um gráfico para visualizar o desempenho do modelo SVR.\n\nplt.scatter(y_test, y_pred_svr, alpha=0.5)  \nplt.xlabel(\"Log Preço verdadeiro (y_test)\")\nplt.ylabel(\"Log Preço predito (y_pred)\")\nplt.title(\"Precisão do modelo para Support Vector Regression\")\n\n# Addicionar linha diagonal\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n\n\nplt.show()\n\n\n\n\n\n\n\n\nE vamos incluir os valores no nosso dataframe de indicadores.\n\nnew_row = pd.DataFrame({\n    'R2': [r2_svr],\n    'MSE': [mse_svr],\n    'RMSE':[rmse_svr],\n    'Modelo': ['SVR']\n})\nindicadores = pd.concat([indicadores, new_row], ignore_index=True)\n\nindicadores\n\n\n\n\n\n\n\n\nR2\nMSE\nRMSE\nModelo\n\n\n\n\n0\n0.791159\n0.065565\n0.256057\nDecision Tree\n\n\n1\n0.854523\n0.045672\n0.213711\nRandom Forest\n\n\n2\n0.781104\n0.068722\n0.262149\nSVR"
  },
  {
    "objectID": "pyTutorial7.html#gráfico-comparativo",
    "href": "pyTutorial7.html#gráfico-comparativo",
    "title": "Tutorial 7 - Métodos de Regressão",
    "section": "Gráfico comparativo",
    "text": "Gráfico comparativo\n\nfig = sns.scatterplot(data=indicadores, x='R2', y='RMSE', hue='Modelo')\nfig.set_xlim(0.5, 1)\nfig.set_ylim(0,0.5)\nplt.title('Gráfico comparativo entre os indicadores RMSE x R2')\nplt.show()"
  },
  {
    "objectID": "pyTutorial6.html",
    "href": "pyTutorial6.html",
    "title": "Tutorial 6 - Métodos não supervisionados",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "pyTutorial6.html#carregando-os-pacotes",
    "href": "pyTutorial6.html#carregando-os-pacotes",
    "title": "Tutorial 6 - Métodos não supervisionados",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "pyTutorial6.html#lendo-os-dados",
    "href": "pyTutorial6.html#lendo-os-dados",
    "title": "Tutorial 6 - Métodos não supervisionados",
    "section": "Lendo os dados",
    "text": "Lendo os dados\n\n\nVocê pode baixar os dados aqui.pythonQMD.zipcard_transdata.csvpenguins.csv\n\n\nNeste tutorial vamos aprender 2 métodos de machine learning que servem para problemas de agrupamento. Neste tipo de problemas, não temos uma variável de saída conhecida, portanto não podemos usar os métodos supervisionados já conhecidos.\n\npenguins = pd.read_csv('pydata5/penguins.csv')\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMALE\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n\n\n\n\n\n\n\ncolunas:\n\npenguins.info()\npenguins.isna().sum()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\n\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\nquantidades:\n\nsns.countplot(x = \"species\", data = penguins)\nplt.show()\n\n\n\n\n\n\n\n\nboxplots:\n\nfig,axs = plt.subplots(ncols = 2)\nfig.tight_layout()\n\nsns.boxplot(y= 'bill_length_mm', x = 'species', data = penguins, ax= axs[0])\nsns.boxplot(y= 'bill_depth_mm', x = 'species', data = penguins, ax= axs[1])"
  },
  {
    "objectID": "pyTutorial6.html#k-means",
    "href": "pyTutorial6.html#k-means",
    "title": "Tutorial 6 - Métodos não supervisionados",
    "section": "K-means",
    "text": "K-means\nVamos aprender inicialmente o método conhecido como ‘k-means’ que basicamente agrupa as observações por uma métrica de similaridade e distância e logo cria os grupos a partir da aproximação entre elas, ou seja, aquelas observações com distância pequena entre elas farão parte do mesmo grupo.\n\nVamos plotar os dados:\n\nsns.scatterplot(x='bill_length_mm', y='bill_depth_mm', data = penguins)\n\n\n\n\n\n\n\n\nAgora podemos começar o tratamento dos dados.\n\ndf = penguins.dropna(inplace=True)\ndf = penguins.drop(columns=['species', 'island', 'sex'])\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 333 entries, 0 to 343\nData columns (total 4 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   bill_length_mm     333 non-null    float64\n 1   bill_depth_mm      333 non-null    float64\n 2   flipper_length_mm  333 non-null    float64\n 3   body_mass_g        333 non-null    float64\ndtypes: float64(4)\nmemory usage: 13.0 KB\n\n\nUtilizando o algoritmo de padronização:\n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df)\n\nReconvertendo o dataset scaled_data para dataframe para facilitar o manuseio.\n\ndf = pd.DataFrame(scaled_data, columns=df.columns)\n\ndf.head()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\n0\n-0.896042\n0.780732\n-1.426752\n-0.568475\n\n\n1\n-0.822788\n0.119584\n-1.069474\n-0.506286\n\n\n2\n-0.676280\n0.424729\n-0.426373\n-1.190361\n\n\n3\n-1.335566\n1.085877\n-0.569284\n-0.941606\n\n\n4\n-0.859415\n1.747026\n-0.783651\n-0.692852\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 333 entries, 0 to 332\nData columns (total 4 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   bill_length_mm     333 non-null    float64\n 1   bill_depth_mm      333 non-null    float64\n 2   flipper_length_mm  333 non-null    float64\n 3   body_mass_g        333 non-null    float64\ndtypes: float64(4)\nmemory usage: 10.5 KB\n\n\nRodando o algoritmo k-means:\n\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n\n\nkmeans.fit(df)\n\nKMeans(n_clusters=3, n_init=10, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=3, n_init=10, random_state=42)\n\n\nincluindo a coluna cluster:\n\ndf['cluster'] = kmeans.labels_\n\ndf.head()\n\ndf.tail()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\ncluster\n\n\n\n\n328\n0.587352\n-1.762145\n0.931283\n0.892957\n1\n\n\n329\n0.514098\n-1.457000\n1.002739\n0.799674\n1\n\n\n330\n1.173384\n-0.744994\n1.502928\n1.919069\n1\n\n\n331\n0.221082\n-1.202712\n0.788372\n1.234995\n1\n\n\n332\n1.081817\n-0.541564\n0.859828\n1.483749\n1\n\n\n\n\n\n\n\n\nsns.scatterplot(x='bill_length_mm', y='bill_depth_mm', hue = 'cluster', data = df)\n\n\n\n\n\n\n\n\n\nEscolhendo o número ideal de clusters\n\ninertia = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, random_state=i, n_init=10)\n    kmeans.fit(df)\n    inertia.append(kmeans.inertia_)\n\nCriar um dataframe com todos os valores de k :\n\ninertia_df = pd.DataFrame({'k': range(1, 11), 'inertia': inertia})\n\ninertia_df.head()\n\n\n\n\n\n\n\n\nk\ninertia\n\n\n\n\n0\n1\n1540.186186\n\n\n1\n2\n757.624293\n\n\n2\n3\n370.766144\n\n\n3\n4\n293.904751\n\n\n4\n5\n249.413429\n\n\n\n\n\n\n\nEscolhe-se o ponto onde o ‘cotovelo’ é mais visível, neste caso, 3 clusters:\n\nplt.plot(inertia_df['k'], inertia_df['inertia'])\nplt.show()"
  },
  {
    "objectID": "pyTutorial6.html#cluster-hierárquico",
    "href": "pyTutorial6.html#cluster-hierárquico",
    "title": "Tutorial 6 - Métodos não supervisionados",
    "section": "Cluster Hierárquico",
    "text": "Cluster Hierárquico\nA diferença com o método anterior é que para o cluster hierárquico não é necessário pré-definir o número de clusters, pois o método agrupa as observações de forma sequencial, facilitando a identificação do número ideal de cluster.\n\nO ideal é que os dados estejam normalizados, pelo menos as colunas numéricas, pois este método aceita também colunas (atributos) categóricos.\n\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\naplicando o algoritmo:\n\nh_clustering = linkage(scaled_data, method=\"ward\", metric=\"euclidean\")\n\ngraficando o dendograma:\n\ndendrogram(h_clustering)\nplt.show()\n\n\n\n\n\n\n\n\nsalvando os resultados no dataframe:\n\nfrom sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')\ndf['hcluster'] = cluster.fit_predict(df)\n\ngraficando os resultados:\n\nsns.scatterplot(x='bill_length_mm', y='bill_depth_mm', hue = 'hcluster', data = df)"
  },
  {
    "objectID": "pyTutorialPython.html",
    "href": "pyTutorialPython.html",
    "title": "Tutorial de Instalação do Python",
    "section": "",
    "text": "Windows\nPara instalar o python em Windows, siga os passos abaixo.\n\nFaça o download do Python em: python.org (a versão de 64-bit).\nAo clicar duas vezes no instalador, a tela abaixo deve aparecer.\n\nImportante: marque as duas opções na parte de baixo (Use admin...) e (Add python.exe to PATH)\nClique em “Install now” e siga os passos.\nSe a instalação foi bem-sucedida, pesquise no menu iniciar por cmd e clique duas vezes para abri-lo\n\nDigite o seguinte comando python --version e a tela deve retornar a versão instalada: Python 3.nn.x\nPor fim, verifique a instalação do pip (o gerenciador que permite adicionar novas libraries ao Python): pip --version\n\nO último comando deve retornar a versão do pip instalada na sua máquina.\n\n\n\nMac OS\n\nFaça o download do Python em: python.org e siga os passos do instalador.\n\n\n\nInstalação de pacotes\nPara instalar os principais pacotes que iremos usar, abra o cmd e digite:\n\npip install numpy pandas matplotlib seaborn plotly jupyterlab\n\n\n\nInstalação do quarto\nPara instalar o quarto visite quarto.org, escolha a versão do seu sistema: Windows, Mac, etc. e faça do download do instalador e siga os passos do mesmo."
  },
  {
    "objectID": "pyTutorial5c.html",
    "href": "pyTutorial5c.html",
    "title": "Tutorial 5c - Dashboards Interativos",
    "section": "",
    "text": "Carregando os pacotes\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nInstalar pacotes\nPrimeiro precisamos instalar o shiny e suas dependências:\n\npip install --upgrade shiny shinywidgets\n\n\n\nHello Shiny\nVamos começar com um dashboard reativo simples.\n\n\nVocê pode baixar os dados aqui.pythonQMD.zipcard_transdata.csvpenguins.csv\n\n\n\nUsaremos como base este código (que também está disponível nos arquivos da pasta.\n\nPara poder renderizar o .qmd o procedimento é similar aos arquivos ipynb, pois é necessário digitar os comandos no Terminal:\n\nquarto preview &lt;filename&gt;.qmd\n\nAlternativamente, sugere-se utilizar um IDE como o RStudio, o VSCode ou o Positron.\n\n\nHello Shiny 2\nAgora vamos criar um dashboard mais complexo:\n\nExistem vários elementos que podemos incluir no dashboard para obter reatividade e podem ser encontrados aqui."
  },
  {
    "objectID": "pyTutorial1.html",
    "href": "pyTutorial1.html",
    "title": "Tutorial 1 - Pandas",
    "section": "",
    "text": "O Pandas é uma biblioteca Python fundamental para análise de dados. Ela fornece estruturas de dados eficientes e ferramentas para manipulação, limpeza e análise de dados.\n\n\nVocê pode baixar os dados aqui.imdb_series.xlsxnetflix_series_limpo.xlsx\n\n\n\nimport pandas as pd\n\n\nCriando um dataframe\n\ndata = {'name': ['Alice', 'Bob', 'Charlie', 'Alice', 'David', 'Bob','Camille'],\n        'age': [25, 30, 35, 25, 40, 30,20],\n        'title':['Sherlock','The Walking Dead','Dark',\n        'Friends','Orange Is the New Black',\n        'The Walking Dead','Narcos']}\ndf = pd.DataFrame(data)\n\n\nprint(df)\n\n      name  age                    title\n0    Alice   25                 Sherlock\n1      Bob   30         The Walking Dead\n2  Charlie   35                     Dark\n3    Alice   25                  Friends\n4    David   40  Orange Is the New Black\n5      Bob   30         The Walking Dead\n6  Camille   20                   Narcos\n\n\n\n\nCarregando os dados\n\nnetflix = pd.read_excel(\"data1/netflix_series_limpo.xlsx\")\nimdb = pd.read_excel(\"data1/imdb_series.xlsx\")\n\n\nprint(netflix.head)\n\n&lt;bound method NDFrame.head of      series_title     season               episode       Date\n0            Away   Season 1                  Home 2020-10-06\n1            Away   Season 1                Spektr 2020-10-06\n2            Away   Season 1           Vital Signs 2020-10-05\n3            Away   Season 1        Goodnight Mars 2020-10-05\n4            Away   Season 1        A Little Faith 2020-10-05\n..            ...        ...                   ...        ...\n918  Orphan Black   Season 1     Natural Selection 2015-08-21\n919   Bates Motel   Season 2   The Immutable Truth 2015-08-15\n920   Bates Motel   Season 2               The Box 2015-08-15\n921   Bates Motel   Season 2              Meltdown 2015-08-14\n922   Bates Motel   Season 2     Presumed Innocent 2015-08-13\n\n[923 rows x 4 columns]&gt;\n\n\nTambém podemos ver rapidamente a estrutura de cada dataset, utilizando a função dtypes e describe().\n\nnetflix.describe()\n\n\n\n\n\n\n\n\nDate\n\n\n\n\ncount\n923\n\n\nmean\n2018-05-29 23:58:26.392199424\n\n\nmin\n2015-08-13 00:00:00\n\n\n25%\n2017-01-06 00:00:00\n\n\n50%\n2018-09-10 00:00:00\n\n\n75%\n2019-07-30 00:00:00\n\n\nmax\n2020-10-06 00:00:00\n\n\n\n\n\n\n\n\nnetflix.dtypes\n\nseries_title            object\nseason                  object\nepisode                 object\nDate            datetime64[ns]\ndtype: object\n\n\nVamos converter a coluna season em variável categórica.\n\nnetflix['season'].astype('category')\n\n0       Season 1\n1       Season 1\n2       Season 1\n3       Season 1\n4       Season 1\n         ...    \n918     Season 1\n919     Season 2\n920     Season 2\n921     Season 2\n922     Season 2\nName: season, Length: 923, dtype: category\nCategories (32, object): [' 1ª temporada', ' Back in Business', ' Berry Bitty Adventures', ' Chapter Eight', ..., ' Turma da Mônica', ' Volume 1', ' Volume 2', ' Welcome to Ever After High']\n\n\nPodemos ver as dimensões do nosso dataframe:\n\nnetflix.shape\n\n(923, 4)\n\n\n\n\nSelecionando colunas\nPara selecionar colunas utilizamos a forma df[['coluna']] onde df é o nome do dataframe e ‘coluna’ é o nome da coluna ou colunas que queremos selecionar.\n\nnetflix[['series_title','season']]\n\n\n\n\n\n\n\n\nseries_title\nseason\n\n\n\n\n0\nAway\nSeason 1\n\n\n1\nAway\nSeason 1\n\n\n2\nAway\nSeason 1\n\n\n3\nAway\nSeason 1\n\n\n4\nAway\nSeason 1\n\n\n...\n...\n...\n\n\n918\nOrphan Black\nSeason 1\n\n\n919\nBates Motel\nSeason 2\n\n\n920\nBates Motel\nSeason 2\n\n\n921\nBates Motel\nSeason 2\n\n\n922\nBates Motel\nSeason 2\n\n\n\n\n923 rows × 2 columns\n\n\n\n\n\nOrdenando os dados\nPara ordenar as linhas, podemos utilizar a função sort_values() de forma a termos, por exemplo, uma lista de maior a menor de um determinado valor. Vamos usar o dataframe imdb para exemplificar, ordenando as linhas por ordem crescente de UserRating:\n\nimdb.head()\n\n\n\n\n\n\n\n\nseries_name\nEpisode\nseries_ep\nseason\nseason_ep\nurl\nUserRating\nUserVotes\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8\nr9\nr10\n\n\n\n\n0\n13 Reasons Why\nTape 1, Side A\n1\n1\n1\nhttp://www.imdb.com/title/tt5174246/?ref_=ttep...\n8.3\n7016\n0.055445\n0.004418\n0.004704\n0.005844\n0.014681\n0.032640\n0.105188\n0.237030\n0.246579\n0.293472\n\n\n1\n13 Reasons Why\nTape 1, Side B\n2\n1\n2\nhttp://www.imdb.com/title/tt5174248/?ref_=ttep...\n8.0\n5859\n0.056665\n0.004438\n0.005803\n0.008022\n0.016726\n0.045400\n0.138420\n0.311145\n0.171872\n0.241509\n\n\n2\n13 Reasons Why\nTape 2, Side A\n3\n1\n3\nhttp://www.imdb.com/title/tt5174250/?ref_=ttep...\n7.9\n5509\n0.058813\n0.003993\n0.005627\n0.009984\n0.022509\n0.051734\n0.160828\n0.302414\n0.140860\n0.243238\n\n\n3\n13 Reasons Why\nTape 2, Side B\n4\n1\n4\nhttp://www.imdb.com/title/tt5174252/?ref_=ttep...\n8.1\n5309\n0.063477\n0.003956\n0.005086\n0.007911\n0.019213\n0.045960\n0.129215\n0.298550\n0.171219\n0.255415\n\n\n4\n13 Reasons Why\nTape 3, Side A\n5\n1\n5\nhttp://www.imdb.com/title/tt5174254/?ref_=ttep...\n8.2\n5252\n0.066832\n0.002666\n0.004570\n0.008378\n0.018850\n0.037129\n0.114242\n0.257997\n0.203542\n0.285796\n\n\n\n\n\n\n\n\nimdb = imdb.sort_values('UserRating',\n                ascending=False)\n\nAgora vamos ver como ficou:\n\nimdb[['series_name','UserRating']]\n\n\n\n\n\n\n\n\nseries_name\nUserRating\n\n\n\n\n2359\nLúcifer\n9.8\n\n\n134\nDark\n9.7\n\n\n1404\nThe Walking Dead\n9.7\n\n\n1183\nFriends\n9.7\n\n\n2371\nLúcifer\n9.7\n\n\n...\n...\n...\n\n\n773\nPânico: A Série de TV\n5.5\n\n\n774\nPânico: A Série de TV\n5.2\n\n\n1908\nDracula\n5.2\n\n\n42\n13 Reasons Why\n5.2\n\n\n137\nDracula\n5.2\n\n\n\n\n2597 rows × 2 columns\n\n\n\n\n\nFiltrando Linhas\nPodemos filtrar linhas utilizando o método df.query() onde df é o nome do dataframe.\n\nimdb.query('UserVotes &gt; 10000')\n\n\n\n\n\n\n\n\nseries_name\nEpisode\nseries_ep\nseason\nseason_ep\nurl\nUserRating\nUserVotes\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8\nr9\nr10\n\n\n\n\n2359\nLúcifer\nA Devil of My Word\n55\n3\n24\nhttp://www.imdb.com/title/tt8253126/?ref_=ttep...\n9.8\n10712\n0.021004\n0.001587\n0.000840\n0.000747\n0.002054\n0.002801\n0.007468\n0.018764\n0.063107\n0.881628\n\n\n134\nDark\nThe Paradise\n26\n3\n8\nhttp://www.imdb.com/title/tt12557704/?ref_=tte...\n9.7\n17468\n0.020151\n0.002633\n0.003263\n0.002805\n0.004351\n0.006698\n0.012308\n0.022556\n0.059824\n0.865411\n\n\n1404\nThe Walking Dead\nNo Way Out\n57\n6\n9\nhttp://www.imdb.com/title/tt4575388/?ref_=ttep...\n9.7\n24358\n0.027424\n0.003490\n0.002094\n0.002669\n0.005665\n0.007390\n0.015519\n0.033008\n0.101979\n0.800764\n\n\n1371\nThe Walking Dead\nToo Far Gone\n24\n4\n8\nhttp://www.imdb.com/title/tt2948638/?ref_=ttep...\n9.7\n22061\n0.044876\n0.002584\n0.001541\n0.001496\n0.004624\n0.006029\n0.013100\n0.039255\n0.126286\n0.760210\n\n\n985\nThe Walking Dead\nNo Way Out\n57\n6\n9\nhttp://www.imdb.com/title/tt4575388/?ref_=ttep...\n9.7\n24358\n0.027424\n0.003490\n0.002094\n0.002669\n0.005665\n0.007390\n0.015519\n0.033008\n0.101979\n0.800764\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1411\nThe Walking Dead\nLast Day on Earth\n64\n6\n16\nhttp://www.imdb.com/title/tt4589574/?ref_=ttep...\n6.6\n26981\n0.263667\n0.036878\n0.037285\n0.037211\n0.052259\n0.049590\n0.068826\n0.090693\n0.104221\n0.259368\n\n\n992\nThe Walking Dead\nLast Day on Earth\n64\n6\n16\nhttp://www.imdb.com/title/tt4589574/?ref_=ttep...\n6.6\n26981\n0.263667\n0.036878\n0.037285\n0.037211\n0.052259\n0.049590\n0.068826\n0.090693\n0.104221\n0.259368\n\n\n1337\nStranger Things\nChapter Seven: The Lost Sister\n15\n2\n7\nhttp://www.imdb.com/title/tt6020810/?ref_=ttep...\n6.1\n21371\n0.166955\n0.045716\n0.047869\n0.059239\n0.100791\n0.118198\n0.157784\n0.116794\n0.058958\n0.127696\n\n\n1417\nThe Walking Dead\nSwear\n70\n7\n6\nhttp://www.imdb.com/title/tt5207734/?ref_=ttep...\n5.6\n11971\n0.197978\n0.042686\n0.047615\n0.060647\n0.102164\n0.140506\n0.133823\n0.087127\n0.037591\n0.149862\n\n\n998\nThe Walking Dead\nSwear\n70\n7\n6\nhttp://www.imdb.com/title/tt5207734/?ref_=ttep...\n5.6\n11971\n0.197978\n0.042686\n0.047615\n0.060647\n0.102164\n0.140506\n0.133823\n0.087127\n0.037591\n0.149862\n\n\n\n\n124 rows × 18 columns\n\n\n\nSe quisermos combinar filtrado de linhas com seleção de colunas:\n\nimdb_10 = imdb.query('UserVotes &gt; 10000')[['series_name','Episode','UserRating','UserVotes']]\n\n\nimdb_10.head(10)\n\n\n\n\n\n\n\n\nseries_name\nEpisode\nUserRating\nUserVotes\n\n\n\n\n2359\nLúcifer\nA Devil of My Word\n9.8\n10712\n\n\n134\nDark\nThe Paradise\n9.7\n17468\n\n\n1404\nThe Walking Dead\nNo Way Out\n9.7\n24358\n\n\n1371\nThe Walking Dead\nToo Far Gone\n9.7\n22061\n\n\n985\nThe Walking Dead\nNo Way Out\n9.7\n24358\n\n\n1314\nSherlock\nThe Reichenbach Fall\n9.7\n33904\n\n\n1307\nFriends\nThe Last One\n9.7\n11634\n\n\n952\nThe Walking Dead\nToo Far Gone\n9.7\n22061\n\n\n961\nThe Walking Dead\nNo Sanctuary\n9.6\n23608\n\n\n133\nDark\nBetween the Time\n9.6\n13301\n\n\n\n\n\n\n\n\nimdb_10.shape\n\n(124, 4)\n\n\nvamos identificar se há duplicados.\n\nduplicates = imdb_10.duplicated()\nprint(duplicates)\n\n2359    False\n134     False\n1404    False\n1371    False\n985      True\n        ...  \n1411    False\n992      True\n1337    False\n1417    False\n998      True\nLength: 124, dtype: bool\n\n\nNotamos que existem duplicados, portanto precisamos removê-los utilizando o método drop_duplicates().\n\nimdb_10 = imdb_10.drop_duplicates(subset=['Episode'])\n\nVamos ver quantas linhas temos agora.\n\nimdb_10.shape\n\n(91, 4)\n\n\ntambém podemos filtrar por strings. Por exemplo, vamos filtrar pelo seriado “Dark”:\n\nimdb['series_name'].isin(['Dark'])\n\n2359    False\n134      True\n1404    False\n1183    False\n2371    False\n        ...  \n773     False\n774     False\n1908    False\n42      False\n137     False\nName: series_name, Length: 2597, dtype: bool\n\n\nVemos que o código acima retorna uma série com valores lógicos (True-False) para cada linha. Para efetivamente ver o resultado do seriado Dark devemos:\n\nimdb[imdb['series_name'].isin(['Dark'])]\n\n\n\n\n\n\n\n\nseries_name\nEpisode\nseries_ep\nseason\nseason_ep\nurl\nUserRating\nUserVotes\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8\nr9\nr10\n\n\n\n\n134\nDark\nThe Paradise\n26\n3\n8\nhttp://www.imdb.com/title/tt12557704/?ref_=tte...\n9.7\n17468\n0.020151\n0.002633\n0.003263\n0.002805\n0.004351\n0.006698\n0.012308\n0.022556\n0.059824\n0.865411\n\n\n133\nDark\nBetween the Time\n25\n3\n7\nhttp://www.imdb.com/title/tt12557700/?ref_=tte...\n9.6\n13301\n0.020374\n0.001880\n0.002556\n0.002707\n0.004285\n0.006842\n0.014811\n0.031577\n0.073904\n0.841065\n\n\n124\nDark\nAn Endless Cycle\n16\n2\n6\nhttp://www.imdb.com/title/tt10454734/?ref_=tte...\n9.6\n11149\n0.007983\n0.000807\n0.001256\n0.001435\n0.001884\n0.005292\n0.016055\n0.045834\n0.124765\n0.794690\n\n\n131\nDark\nLife and Death\n23\n3\n5\nhttp://www.imdb.com/title/tt12557688/?ref_=tte...\n9.5\n10359\n0.021238\n0.001834\n0.002510\n0.002993\n0.005502\n0.010908\n0.020948\n0.044985\n0.123661\n0.765421\n\n\n126\nDark\nEndings and Beginnings\n18\n2\n8\nhttp://www.imdb.com/title/tt10457654/?ref_=tte...\n9.5\n11379\n0.012040\n0.002109\n0.001846\n0.002373\n0.004482\n0.009667\n0.020301\n0.045698\n0.123473\n0.778012\n\n\n122\nDark\nThe Travelers\n14\n2\n4\nhttp://www.imdb.com/title/tt10454726/?ref_=tte...\n9.5\n9968\n0.007925\n0.000903\n0.001204\n0.001505\n0.002307\n0.006120\n0.019161\n0.056180\n0.166132\n0.738563\n\n\n123\nDark\nLost and Found\n15\n2\n5\nhttp://www.imdb.com/title/tt10454732/?ref_=tte...\n9.4\n9210\n0.007600\n0.000869\n0.001303\n0.001629\n0.002606\n0.007166\n0.023127\n0.075570\n0.197720\n0.682410\n\n\n125\nDark\nThe White Devil\n17\n2\n7\nhttp://www.imdb.com/title/tt10457652/?ref_=tte...\n9.3\n9050\n0.008066\n0.001657\n0.001878\n0.001768\n0.003757\n0.009392\n0.026851\n0.083315\n0.204972\n0.658343\n\n\n132\nDark\nLight and Shadow\n24\n3\n6\nhttp://www.imdb.com/title/tt12557694/?ref_=tte...\n9.3\n10060\n0.024453\n0.001889\n0.002783\n0.004175\n0.006362\n0.010934\n0.025547\n0.063221\n0.127137\n0.733499\n\n\n116\nDark\nAs You Sow, so You Shall Reap\n8\n1\n8\nhttp://www.imdb.com/title/tt7313316/?ref_=ttep...\n9.2\n9294\n0.007209\n0.001076\n0.000753\n0.001829\n0.003443\n0.011298\n0.030773\n0.101033\n0.257586\n0.585001\n\n\n118\nDark\nAlpha and Omega\n10\n1\n10\nhttp://www.imdb.com/title/tt7313322/?ref_=ttep...\n9.2\n9758\n0.010248\n0.001947\n0.001230\n0.003279\n0.005944\n0.011273\n0.032589\n0.086801\n0.229248\n0.617442\n\n\n121\nDark\nGhosts\n13\n2\n3\nhttp://www.imdb.com/title/tt10454722/?ref_=tte...\n9.2\n9138\n0.007004\n0.000766\n0.001970\n0.002079\n0.004487\n0.008645\n0.030422\n0.100897\n0.226855\n0.616875\n\n\n114\nDark\nSic Mundus Creatus Est\n6\n1\n6\nhttp://www.imdb.com/title/tt7313312/?ref_=ttep...\n9.1\n9434\n0.006996\n0.000742\n0.001272\n0.001272\n0.004240\n0.014204\n0.035616\n0.115222\n0.264151\n0.556286\n\n\n130\nDark\nThe Origin\n22\n3\n4\nhttp://www.imdb.com/title/tt12557686/?ref_=tte...\n9.1\n9529\n0.022248\n0.003883\n0.004722\n0.004198\n0.006611\n0.013118\n0.033582\n0.082695\n0.163816\n0.665128\n\n\n113\nDark\nTruths\n5\n1\n5\nhttp://www.imdb.com/title/tt7313308/?ref_=ttep...\n9.0\n9563\n0.005751\n0.000837\n0.001046\n0.002301\n0.004183\n0.014744\n0.045070\n0.134790\n0.278992\n0.512287\n\n\n120\nDark\nDark Matter\n12\n2\n2\nhttp://www.imdb.com/title/tt10454716/?ref_=tte...\n9.0\n9221\n0.006941\n0.001084\n0.001193\n0.002603\n0.003579\n0.010303\n0.034920\n0.122655\n0.247479\n0.569244\n\n\n127\nDark\nDeja-vu\n19\n3\n1\nhttp://www.imdb.com/title/tt10414808/?ref_=tte...\n9.0\n10609\n0.022622\n0.003205\n0.003959\n0.004430\n0.006221\n0.014704\n0.039872\n0.098407\n0.202658\n0.603921\n\n\n129\nDark\nAdam and Eva\n21\n3\n3\nhttp://www.imdb.com/title/tt12557682/?ref_=tte...\n8.9\n9364\n0.022747\n0.002777\n0.004806\n0.004058\n0.008330\n0.016339\n0.040474\n0.109141\n0.194682\n0.596647\n\n\n128\nDark\nThe Survivors\n20\n3\n2\nhttp://www.imdb.com/title/tt12557670/?ref_=tte...\n8.9\n9666\n0.022967\n0.003311\n0.004138\n0.004552\n0.006104\n0.017381\n0.045520\n0.119284\n0.202773\n0.573971\n\n\n119\nDark\nBeginnings and Endings\n11\n2\n1\nhttp://www.imdb.com/title/tt7787482/?ref_=ttep...\n8.9\n9969\n0.007122\n0.001505\n0.001404\n0.003611\n0.004213\n0.013442\n0.049955\n0.154780\n0.238239\n0.525730\n\n\n115\nDark\nCrossroads\n7\n1\n7\nhttp://www.imdb.com/title/tt7305824/?ref_=ttep...\n8.8\n8725\n0.005845\n0.001261\n0.001032\n0.002636\n0.005501\n0.015931\n0.051920\n0.194269\n0.275186\n0.446418\n\n\n117\nDark\nEverything Is Now\n9\n1\n9\nhttp://www.imdb.com/title/tt7313320/?ref_=ttep...\n8.8\n8599\n0.006978\n0.001744\n0.001512\n0.001279\n0.006280\n0.018025\n0.055472\n0.185719\n0.260147\n0.462845\n\n\n111\nDark\nPast and Present\n3\n1\n3\nhttp://www.imdb.com/title/tt7305820/?ref_=ttep...\n8.7\n9585\n0.006051\n0.001148\n0.001669\n0.002608\n0.006886\n0.023996\n0.067710\n0.208242\n0.255921\n0.425769\n\n\n109\nDark\nSecrets\n1\n1\n1\nhttp://www.imdb.com/title/tt6305578/?ref_=ttep...\n8.3\n11242\n0.007917\n0.001245\n0.003202\n0.003914\n0.011564\n0.030422\n0.106298\n0.274239\n0.194094\n0.367105\n\n\n112\nDark\nDouble Lives\n4\n1\n4\nhttp://www.imdb.com/title/tt7305818/?ref_=ttep...\n8.3\n9088\n0.005722\n0.001761\n0.001540\n0.003631\n0.009793\n0.029820\n0.102993\n0.281690\n0.180238\n0.382812\n\n\n110\nDark\nLies\n2\n1\n2\nhttp://www.imdb.com/title/tt7305776/?ref_=ttep...\n8.2\n9903\n0.006160\n0.001616\n0.003130\n0.002827\n0.011108\n0.034939\n0.119661\n0.293547\n0.165303\n0.361709\n\n\n\n\n\n\n\nvamos guardar este dataframe com o nome darkdf:\n\ndarkdf = imdb[imdb['series_name'].isin(['Dark'])]\n\n\n\nAgrupando dados por categorias\nVamos agrupar os dados de imdb e calcular algumas informações: a contagem de capítulos por seriado, a média do UserRating por seriado e a soma total dos votos.\n\nimdb_summary = imdb.groupby(['series_name']).agg({'series_name':\"count\",'UserRating':'mean','UserVotes':'sum'})\n\n\nimdb_summary\n\n\n\n\n\n\n\n\nseries_name\nUserRating\nUserVotes\n\n\nseries_name\n\n\n\n\n\n\n\n13 Reasons Why\n49\n7.248980\n151452\n\n\nArquivo X\n356\n7.988202\n969628\n\n\nAway\n10\n7.080000\n5388\n\n\nComo Defender um Assassino\n180\n8.568889\n228352\n\n\nDark\n26\n9.076923\n264631\n\n\nDracula\n6\n6.966667\n28858\n\n\nEra Uma Vez\n312\n8.266026\n397332\n\n\nFriends\n235\n8.451915\n853287\n\n\nGreenleaf\n60\n7.586667\n2777\n\n\nGrimm: Contos de Terror\n123\n8.435772\n104967\n\n\nHemlock Grove\n33\n7.490909\n12538\n\n\nLa Casa de Papel\n31\n8.209677\n161782\n\n\nLúcifer\n75\n8.744000\n298412\n\n\nMotel Bates\n50\n8.592000\n78773\n\n\nNarcos\n30\n8.760000\n122160\n\n\nO Bom Lugar\n50\n8.272000\n98562\n\n\nOrange Is the New Black\n91\n8.172527\n156565\n\n\nOrphan Black\n50\n8.528000\n60532\n\n\nOs Originais\n184\n8.828261\n179686\n\n\nPânico: A Série de TV\n29\n7.537931\n28078\n\n\nRatched\n8\n7.812500\n8234\n\n\nSherlock\n15\n8.800000\n373114\n\n\nSleepy Hollow\n62\n7.680645\n33079\n\n\nSobrenatural\n211\n8.389573\n499046\n\n\nStranger Things\n25\n8.676000\n391288\n\n\nThe Walking Dead\n288\n8.006944\n2394586\n\n\nThe Witcher\n8\n8.487500\n122656\n\n\n\n\n\n\n\nPodemos fazer o mesmo para o dataframe darkdf:\n\ndarkdf_grouped = darkdf.groupby(['series_name']).count()\n\ndarkdf_grouped\n\n\n\n\n\n\n\n\nEpisode\nseries_ep\nseason\nseason_ep\nurl\nUserRating\nUserVotes\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8\nr9\nr10\n\n\nseries_name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDark\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n\n\n\n\n\n\n\nUma boa prática é retornar o nome da coluna, neste caso series_name para uma nova coluna, e desta forma, deixar o index novamente numérico, para isto utilizamos a função reset_index.\n\ndarkdf_grouped.reset_index()\n\n\n\n\n\n\n\n\nseries_name\nEpisode\nseries_ep\nseason\nseason_ep\nurl\nUserRating\nUserVotes\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8\nr9\nr10\n\n\n\n\n0\nDark\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n26\n\n\n\n\n\n\n\nVamos fazer o mesmo para o imdb_summary:\n\nimdb_summary2 = imdb_summary.rename(columns={'series_name':'Episodes'})\nimdb_summary2.reset_index(inplace=True)\n\nimdb_summary2.head(7)\n\n\n\n\n\n\n\n\nseries_name\nEpisodes\nUserRating\nUserVotes\n\n\n\n\n0\n13 Reasons Why\n49\n7.248980\n151452\n\n\n1\nArquivo X\n356\n7.988202\n969628\n\n\n2\nAway\n10\n7.080000\n5388\n\n\n3\nComo Defender um Assassino\n180\n8.568889\n228352\n\n\n4\nDark\n26\n9.076923\n264631\n\n\n5\nDracula\n6\n6.966667\n28858\n\n\n6\nEra Uma Vez\n312\n8.266026\n397332\n\n\n\n\n\n\n\n\n\nModificando ou criando novas colunas\nSimilar à função mutate() no R, podemos usar o método df.assign em Python.\n\nimdb_rtotal = imdb.assign(r_total=imdb['r1']+imdb['r2']+imdb['r3']+imdb['r4']+imdb['r5']+imdb['r6']+imdb['r7']+imdb['r8']+imdb['r9']+imdb['r10'])\n\nAgora podemos selecionar apenas as colunas que nos interessam.\n\nimdb_rtotal[['series_name','Episode','r_total']]\n\n\n\n\n\n\n\n\nseries_name\nEpisode\nr_total\n\n\n\n\n2359\nLúcifer\nA Devil of My Word\n1.0\n\n\n134\nDark\nThe Paradise\n1.0\n\n\n1404\nThe Walking Dead\nNo Way Out\n1.0\n\n\n1183\nFriends\nThe One Where Everybody Finds Out\n1.0\n\n\n2371\nLúcifer\nWho's da New King of Hell?\n1.0\n\n\n...\n...\n...\n...\n\n\n773\nPânico: A Série de TV\nBlindspots\n1.0\n\n\n774\nPânico: A Série de TV\nEndgame\n1.0\n\n\n1908\nDracula\nThe Dark Compass\n1.0\n\n\n42\n13 Reasons Why\nSenior Camping Trip\n1.0\n\n\n137\nDracula\nThe Dark Compass\n1.0\n\n\n\n\n2597 rows × 3 columns\n\n\n\ntambém podemos criar novas colunas utilizando outra notação, vamos assumir que queremos criar uma nova coluna no dataframe imdb_summary :\n\nimdb_summary2['VotesPerEpisode']=imdb_summary2['UserVotes']/imdb_summary2['Episodes']\n\nimdb_summary2.head(8)\n\n\n\n\n\n\n\n\nseries_name\nEpisodes\nUserRating\nUserVotes\nVotesPerEpisode\n\n\n\n\n0\n13 Reasons Why\n49\n7.248980\n151452\n3090.857143\n\n\n1\nArquivo X\n356\n7.988202\n969628\n2723.674157\n\n\n2\nAway\n10\n7.080000\n5388\n538.800000\n\n\n3\nComo Defender um Assassino\n180\n8.568889\n228352\n1268.622222\n\n\n4\nDark\n26\n9.076923\n264631\n10178.115385\n\n\n5\nDracula\n6\n6.966667\n28858\n4809.666667\n\n\n6\nEra Uma Vez\n312\n8.266026\n397332\n1273.500000\n\n\n7\nFriends\n235\n8.451915\n853287\n3631.008511\n\n\n\n\n\n\n\ne agora vamos ordenar pela nova coluna VotesPerEpisode e arredondar para dois decimais:\n\nimdb_summary2.sort_values('VotesPerEpisode',\n                ascending=False).round(decimals=2)\n\n\n\n\n\n\n\n\nseries_name\nEpisodes\nUserRating\nUserVotes\nVotesPerEpisode\n\n\n\n\n21\nSherlock\n15\n8.80\n373114\n24874.27\n\n\n24\nStranger Things\n25\n8.68\n391288\n15651.52\n\n\n26\nThe Witcher\n8\n8.49\n122656\n15332.00\n\n\n4\nDark\n26\n9.08\n264631\n10178.12\n\n\n25\nThe Walking Dead\n288\n8.01\n2394586\n8314.53\n\n\n11\nLa Casa de Papel\n31\n8.21\n161782\n5218.77\n\n\n5\nDracula\n6\n6.97\n28858\n4809.67\n\n\n14\nNarcos\n30\n8.76\n122160\n4072.00\n\n\n12\nLúcifer\n75\n8.74\n298412\n3978.83\n\n\n7\nFriends\n235\n8.45\n853287\n3631.01\n\n\n0\n13 Reasons Why\n49\n7.25\n151452\n3090.86\n\n\n1\nArquivo X\n356\n7.99\n969628\n2723.67\n\n\n23\nSobrenatural\n211\n8.39\n499046\n2365.15\n\n\n15\nO Bom Lugar\n50\n8.27\n98562\n1971.24\n\n\n16\nOrange Is the New Black\n91\n8.17\n156565\n1720.49\n\n\n13\nMotel Bates\n50\n8.59\n78773\n1575.46\n\n\n6\nEra Uma Vez\n312\n8.27\n397332\n1273.50\n\n\n3\nComo Defender um Assassino\n180\n8.57\n228352\n1268.62\n\n\n17\nOrphan Black\n50\n8.53\n60532\n1210.64\n\n\n20\nRatched\n8\n7.81\n8234\n1029.25\n\n\n18\nOs Originais\n184\n8.83\n179686\n976.55\n\n\n19\nPânico: A Série de TV\n29\n7.54\n28078\n968.21\n\n\n9\nGrimm: Contos de Terror\n123\n8.44\n104967\n853.39\n\n\n2\nAway\n10\n7.08\n5388\n538.80\n\n\n22\nSleepy Hollow\n62\n7.68\n33079\n533.53\n\n\n10\nHemlock Grove\n33\n7.49\n12538\n379.94\n\n\n8\nGreenleaf\n60\n7.59\n2777\n46.28\n\n\n\n\n\n\n\n\n\nJuntando dois ou mais df\nmerge une dois DataFrames (df1 e df2) com base em colunas especificadas (on, left_on, right_on). O tipo de junção (how) define como as tabelas serão combinadas:\n\n'inner': Mantém apenas as linhas onde há correspondência nas colunas de junção (interseção).\n'left': Mantém todas as linhas do DataFrame esquerdo (df1) e as correspondentes do direito (df2).\n'right': Mantém todas as linhas do DataFrame direito (df2) e as correspondentes do esquerdo (df1).\n'outer': Mantém todas as linhas de ambos os DataFrames, preenchendo com NaN onde não há correspondência (união).\n\nUse left_on e right_on quando as colunas de junção têm nomes diferentes nos DataFrames. Vamos a usar o pequeno df que foi criado no início deste tutorial para verificar o funcionamento do merge.\n\ndf\n\n\n\n\n\n\n\n\nname\nage\ntitle\n\n\n\n\n0\nAlice\n25\nSherlock\n\n\n1\nBob\n30\nThe Walking Dead\n\n\n2\nCharlie\n35\nDark\n\n\n3\nAlice\n25\nFriends\n\n\n4\nDavid\n40\nOrange Is the New Black\n\n\n5\nBob\n30\nThe Walking Dead\n\n\n6\nCamille\n20\nNarcos\n\n\n\n\n\n\n\ne vamos criar um pequeno df com dados do Netflix:\n\nnetflix_5 = {'season': ['Season 1', 'Season 2', 'Season 3', 'Season 4', 'Season 5', 'Part 1','Part 2'],\n        'title': ['Sherlock', 'The Walking Dead', 'Dark', 'Friends', 'Orange Is the New Black', 'The Walking Dead','Narcos']}\n\nnetflix_5_df = pd.DataFrame(netflix_5)\n\nnetflix_5_df\n\n\n\n\n\n\n\n\nseason\ntitle\n\n\n\n\n0\nSeason 1\nSherlock\n\n\n1\nSeason 2\nThe Walking Dead\n\n\n2\nSeason 3\nDark\n\n\n3\nSeason 4\nFriends\n\n\n4\nSeason 5\nOrange Is the New Black\n\n\n5\nPart 1\nThe Walking Dead\n\n\n6\nPart 2\nNarcos\n\n\n\n\n\n\n\nVamos realizar o merge:\n\nnetflix_5_df.merge(df, on='title', how='left')\n\n\n\n\n\n\n\n\nseason\ntitle\nname\nage\n\n\n\n\n0\nSeason 1\nSherlock\nAlice\n25\n\n\n1\nSeason 2\nThe Walking Dead\nBob\n30\n\n\n2\nSeason 2\nThe Walking Dead\nBob\n30\n\n\n3\nSeason 3\nDark\nCharlie\n35\n\n\n4\nSeason 4\nFriends\nAlice\n25\n\n\n5\nSeason 5\nOrange Is the New Black\nDavid\n40\n\n\n6\nPart 1\nThe Walking Dead\nBob\n30\n\n\n7\nPart 1\nThe Walking Dead\nBob\n30\n\n\n8\nPart 2\nNarcos\nCamille\n20"
  },
  {
    "objectID": "pyTutorial2.html",
    "href": "pyTutorial2.html",
    "title": "Tutorial 2 - Pandas",
    "section": "",
    "text": "Agora vamos realizar algumas análises mais interessantes.\n\n\nVocê pode baixar os dados aqui.users.csvratings.csvmovies.csv\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nLendo os dados\n\nusers = pd.read_csv('pydata2/users.csv')\n\nusers.head()\n\n\n\n\n\n\n\n\nuser_id\nage\nsex\noccupation\nzip_code\n\n\n\n\n0\n1\n24\nM\ntechnician\n85711\n\n\n1\n2\n53\nF\nother\n94043\n\n\n2\n3\n23\nM\nwriter\n32067\n\n\n3\n4\n24\nM\ntechnician\n43537\n\n\n4\n5\n33\nF\nother\n15213\n\n\n\n\n\n\n\n\nusers.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 943 entries, 0 to 942\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   user_id     943 non-null    int64 \n 1   age         943 non-null    int64 \n 2   sex         943 non-null    object\n 3   occupation  943 non-null    object\n 4   zip_code    943 non-null    object\ndtypes: int64(2), object(3)\nmemory usage: 37.0+ KB\n\n\n\nmovies = pd.read_csv('pydata2/movies.csv')\nmovies.head()\n\n\n\n\n\n\n\n\nmovie_id\ntitle\nrelease_date\nvideo_release_date\nimdb_url\n\n\n\n\n0\n2\nGoldenEye (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?GoldenEye%20(...\n\n\n1\n3\nFour Rooms (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?Four%20Rooms%...\n\n\n2\n4\nGet Shorty (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?Get%20Shorty%...\n\n\n3\n5\nCopycat (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?Copycat%20(1995)\n\n\n4\n6\nShanghai Triad (Yao a yao yao dao waipo qiao) ...\n01-Jan-1995\nNaN\nhttp://us.imdb.com/Title?Yao+a+yao+yao+dao+wai...\n\n\n\n\n\n\n\n\nmovies.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1681 entries, 0 to 1680\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   movie_id            1681 non-null   int64  \n 1   title               1681 non-null   object \n 2   release_date        1680 non-null   object \n 3   video_release_date  0 non-null      float64\n 4   imdb_url            1678 non-null   object \ndtypes: float64(1), int64(1), object(3)\nmemory usage: 65.8+ KB\n\n\n\nratings = pd.read_csv('pydata2/ratings.csv')\nratings.head()\n\n\n\n\n\n\n\n\nuser_id\nmovie_id\nrating\nunix_timestamp\n\n\n\n\n0\n196\n242\n3\n881250949\n\n\n1\n186\n302\n3\n891717742\n\n\n2\n22\n377\n1\n878887116\n\n\n3\n244\n51\n2\n880606923\n\n\n4\n166\n346\n1\n886397596\n\n\n\n\n\n\n\n\nratings.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100000 entries, 0 to 99999\nData columns (total 4 columns):\n #   Column          Non-Null Count   Dtype\n---  ------          --------------   -----\n 0   user_id         100000 non-null  int64\n 1   movie_id        100000 non-null  int64\n 2   rating          100000 non-null  int64\n 3   unix_timestamp  100000 non-null  int64\ndtypes: int64(4)\nmemory usage: 3.1 MB\n\n\n\n\nJuntando os dados\n\nmovie_ratings = pd.merge(movies, ratings, how='right')\nlens = pd.merge(movie_ratings, users)\nlens.head()\n\n\n\n\n\n\n\n\nmovie_id\ntitle\nrelease_date\nvideo_release_date\nimdb_url\nuser_id\nrating\nunix_timestamp\nage\nsex\noccupation\nzip_code\n\n\n\n\n0\n242\nKolya (1996)\n24-Jan-1997\nNaN\nhttp://us.imdb.com/M/title-exact?Kolya%20(1996)\n196\n3\n881250949\n49\nM\nwriter\n55105\n\n\n1\n393\nMrs. Doubtfire (1993)\n01-Jan-1993\nNaN\nhttp://us.imdb.com/M/title-exact?Mrs.%20Doubtf...\n196\n4\n881251863\n49\nM\nwriter\n55105\n\n\n2\n381\nMuriel's Wedding (1994)\n01-Jan-1994\nNaN\nhttp://us.imdb.com/M/title-exact?Muriel's%20We...\n196\n4\n881251728\n49\nM\nwriter\n55105\n\n\n3\n251\nShall We Dance? (1996)\n11-Jul-1997\nNaN\nhttp://us.imdb.com/M/title-exact?Shall%20we%20...\n196\n3\n881251274\n49\nM\nwriter\n55105\n\n\n4\n655\nStand by Me (1986)\n01-Jan-1986\nNaN\nhttp://us.imdb.com/M/title-exact?Stand%20by%20...\n196\n5\n881251793\n49\nM\nwriter\n55105\n\n\n\n\n\n\n\n\nlens.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100000 entries, 0 to 99999\nData columns (total 12 columns):\n #   Column              Non-Null Count   Dtype  \n---  ------              --------------   -----  \n 0   movie_id            100000 non-null  int64  \n 1   title               99548 non-null   object \n 2   release_date        99539 non-null   object \n 3   video_release_date  0 non-null       float64\n 4   imdb_url            99535 non-null   object \n 5   user_id             100000 non-null  int64  \n 6   rating              100000 non-null  int64  \n 7   unix_timestamp      100000 non-null  int64  \n 8   age                 100000 non-null  int64  \n 9   sex                 100000 non-null  object \n 10  occupation          100000 non-null  object \n 11  zip_code            100000 non-null  object \ndtypes: float64(1), int64(5), object(6)\nmemory usage: 9.2+ MB\n\n\n\n\nAnálise Exploratória de Dados\n\nQuais são os 25 filmes mais avaliados?\n\nbest = lens.groupby('title').count().reset_index()[['title','movie_id']].rename(columns={'movie_id':'count'}).sort_values('count',\n                ascending=False)\n                \nbest.head(25)\n\n\n\n\n\n\n\n\ntitle\ncount\n\n\n\n\n1398\nStar Wars (1977)\n583\n\n\n333\nContact (1997)\n509\n\n\n498\nFargo (1996)\n508\n\n\n1234\nReturn of the Jedi (1983)\n507\n\n\n860\nLiar Liar (1997)\n485\n\n\n460\nEnglish Patient, The (1996)\n481\n\n\n1284\nScream (1996)\n478\n\n\n32\nAir Force One (1997)\n431\n\n\n744\nIndependence Day (ID4) (1996)\n429\n\n\n1205\nRaiders of the Lost Ark (1981)\n420\n\n\n612\nGodfather, The (1972)\n413\n\n\n1190\nPulp Fiction (1994)\n394\n\n\n1542\nTwelve Monkeys (1995)\n392\n\n\n1329\nSilence of the Lambs, The (1991)\n390\n\n\n780\nJerry Maguire (1996)\n384\n\n\n293\nChasing Amy (1997)\n379\n\n\n1251\nRock, The (1996)\n378\n\n\n456\nEmpire Strikes Back, The (1980)\n367\n\n\n1394\nStar Trek: First Contact (1996)\n365\n\n\n113\nBack to the Future (1985)\n350\n\n\n1500\nTitanic (1997)\n350\n\n\n987\nMission: Impossible (1996)\n344\n\n\n570\nFugitive, The (1993)\n336\n\n\n747\nIndiana Jones and the Last Crusade (1989)\n331\n\n\n1632\nWilly Wonka and the Chocolate Factory (1971)\n326\n\n\n\n\n\n\n\nOutra forma de fazer o mesmo (porém, neste caso retorna uma Pandas series:\n\nbest2 = lens.title.value_counts()[:25]\nbest2\n\ntitle\nStar Wars (1977)                                583\nContact (1997)                                  509\nFargo (1996)                                    508\nReturn of the Jedi (1983)                       507\nLiar Liar (1997)                                485\nEnglish Patient, The (1996)                     481\nScream (1996)                                   478\nAir Force One (1997)                            431\nIndependence Day (ID4) (1996)                   429\nRaiders of the Lost Ark (1981)                  420\nGodfather, The (1972)                           413\nPulp Fiction (1994)                             394\nTwelve Monkeys (1995)                           392\nSilence of the Lambs, The (1991)                390\nJerry Maguire (1996)                            384\nChasing Amy (1997)                              379\nRock, The (1996)                                378\nEmpire Strikes Back, The (1980)                 367\nStar Trek: First Contact (1996)                 365\nBack to the Future (1985)                       350\nTitanic (1997)                                  350\nMission: Impossible (1996)                      344\nFugitive, The (1993)                            336\nIndiana Jones and the Last Crusade (1989)       331\nWilly Wonka and the Chocolate Factory (1971)    326\nName: count, dtype: int64\n\n\n\n\nQuais são os filmes com as maiores notas?\n\nmovie_stats = lens.groupby('title').agg({'occupation':'count',\n'rating':'mean','age':'mean'}).sort_values('rating', ascending=False).rename(columns={'occupation':'count','age':'mean_age'}).round(decimals=2)\n\nmovie_stats\n\n\n\n\n\n\n\n\ncount\nrating\nmean_age\n\n\ntitle\n\n\n\n\n\n\n\nSanta with Muscles (1996)\n2\n5.0\n26.50\n\n\nSaint of Fort Washington, The (1993)\n2\n5.0\n24.00\n\n\nPrefontaine (1997)\n3\n5.0\n29.00\n\n\nMarlene Dietrich: Shadow and Light (1996)\n1\n5.0\n60.00\n\n\nSomeone Else's America (1995)\n1\n5.0\n27.00\n\n\n...\n...\n...\n...\n\n\nMan from Down Under, The (1943)\n1\n1.0\n22.00\n\n\nGood Morning (1971)\n1\n1.0\n26.00\n\n\nBird of Prey (1996)\n1\n1.0\n26.00\n\n\nGordy (1995)\n3\n1.0\n28.33\n\n\nPower 98 (1995)\n1\n1.0\n47.00\n\n\n\n\n1663 rows × 3 columns\n\n\n\nOs filmes acima são avaliados tão raramente, que nem podemos qualificá-los como top filmes. Que tal apenas olharmos os filmes que foram avaliados pelo menos 100 vezes.\n\nmovie_stats.query('count &gt;= 100')\n\n\n\n\n\n\n\n\ncount\nrating\nmean_age\n\n\ntitle\n\n\n\n\n\n\n\nClose Shave, A (1995)\n112\n4.49\n31.44\n\n\nSchindler's List (1993)\n298\n4.47\n33.45\n\n\nWrong Trousers, The (1993)\n118\n4.47\n31.55\n\n\nCasablanca (1942)\n243\n4.46\n35.90\n\n\nShawshank Redemption, The (1994)\n283\n4.45\n32.81\n\n\n...\n...\n...\n...\n\n\nSpawn (1997)\n143\n2.62\n28.29\n\n\nEvent Horizon (1997)\n127\n2.57\n31.08\n\n\nCrash (1996)\n128\n2.55\n32.45\n\n\nJungle2Jungle (1997)\n132\n2.44\n32.27\n\n\nCable Guy, The (1996)\n106\n2.34\n28.43\n\n\n\n\n337 rows × 3 columns\n\n\n\noutra forma de fazer o mesmo:\n\natleast_100 = movie_stats['count'] &gt;= 100\n\natleast_100_movies = movie_stats[atleast_100]\n\natleast_100_movies\n\n\n\n\n\n\n\n\ncount\nrating\nmean_age\n\n\ntitle\n\n\n\n\n\n\n\nClose Shave, A (1995)\n112\n4.49\n31.44\n\n\nSchindler's List (1993)\n298\n4.47\n33.45\n\n\nWrong Trousers, The (1993)\n118\n4.47\n31.55\n\n\nCasablanca (1942)\n243\n4.46\n35.90\n\n\nShawshank Redemption, The (1994)\n283\n4.45\n32.81\n\n\n...\n...\n...\n...\n\n\nSpawn (1997)\n143\n2.62\n28.29\n\n\nEvent Horizon (1997)\n127\n2.57\n31.08\n\n\nCrash (1996)\n128\n2.55\n32.45\n\n\nJungle2Jungle (1997)\n132\n2.44\n32.27\n\n\nCable Guy, The (1996)\n106\n2.34\n28.43\n\n\n\n\n337 rows × 3 columns\n\n\n\n\n\nComo se distribuem as idades?\nVamos criar um gráfico mostrando a distribuição das idades.\npandas tem um integração nativa com a biblioteca matplotlib que permite a plotagem de Series/DataFrames ficarem mais fáceis ainda. Neste caso apenas chamamos o método histo na coluna que queremos produzir o histograma. Nós podemos também chamar o matplotlib.pyplot para customizar o nosso gráfico um pouco. (sempre lembrar de rotular seus eixos!)\n\nusers.age.hist(bins=30)\n# Add title and labels\nplt.title('Distribuição das idades')\nplt.xlabel('Idades')\nplt.ylabel('Frequencia')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nEu não acho muito legal comparar nossas idades individuais, portanto vamos binarizar nossos usuários em groupos por idade usando o pd.cut.\n\nlabels = ['0-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70-79']\nlens['age_group'] = pd.cut(lens.age, range(0, 81, 10), right=False, labels=labels)\n\nlens[['title','age','age_group']]\n\n\n\n\n\n\n\n\ntitle\nage\nage_group\n\n\n\n\n0\nKolya (1996)\n49\n40-49\n\n\n1\nMrs. Doubtfire (1993)\n49\n40-49\n\n\n2\nMuriel's Wedding (1994)\n49\n40-49\n\n\n3\nShall We Dance? (1996)\n49\n40-49\n\n\n4\nStand by Me (1986)\n49\n40-49\n\n\n...\n...\n...\n...\n\n\n99995\nCity of Lost Children, The (1995)\n20\n20-29\n\n\n99996\nHeat (1995)\n20\n20-29\n\n\n99997\nNaN\n20\n20-29\n\n\n99998\nLiar Liar (1997)\n20\n20-29\n\n\n99999\nWaiting for Guffman (1996)\n20\n20-29\n\n\n\n\n100000 rows × 3 columns\n\n\n\nVimos que há uma valor ‘NaN’ no título, vamos conferior melhor:\n\nlens.isna().sum()\n\nmovie_id                   0\ntitle                    452\nrelease_date             461\nvideo_release_date    100000\nimdb_url                 465\nuser_id                    0\nrating                     0\nunix_timestamp             0\nage                        0\nsex                        0\noccupation                 0\nzip_code                   0\nage_group                  0\ndtype: int64\n\n\nVamos eliminar os ‘NaN’:\n\nlens = lens.dropna(subset=['title'])\nlens\n\n\n\n\n\n\n\n\nmovie_id\ntitle\nrelease_date\nvideo_release_date\nimdb_url\nuser_id\nrating\nunix_timestamp\nage\nsex\noccupation\nzip_code\nage_group\n\n\n\n\n0\n242\nKolya (1996)\n24-Jan-1997\nNaN\nhttp://us.imdb.com/M/title-exact?Kolya%20(1996)\n196\n3\n881250949\n49\nM\nwriter\n55105\n40-49\n\n\n1\n393\nMrs. Doubtfire (1993)\n01-Jan-1993\nNaN\nhttp://us.imdb.com/M/title-exact?Mrs.%20Doubtf...\n196\n4\n881251863\n49\nM\nwriter\n55105\n40-49\n\n\n2\n381\nMuriel's Wedding (1994)\n01-Jan-1994\nNaN\nhttp://us.imdb.com/M/title-exact?Muriel's%20We...\n196\n4\n881251728\n49\nM\nwriter\n55105\n40-49\n\n\n3\n251\nShall We Dance? (1996)\n11-Jul-1997\nNaN\nhttp://us.imdb.com/M/title-exact?Shall%20we%20...\n196\n3\n881251274\n49\nM\nwriter\n55105\n40-49\n\n\n4\n655\nStand by Me (1986)\n01-Jan-1986\nNaN\nhttp://us.imdb.com/M/title-exact?Stand%20by%20...\n196\n5\n881251793\n49\nM\nwriter\n55105\n40-49\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n99994\n300\nAir Force One (1997)\n01-Jan-1997\nNaN\nhttp://us.imdb.com/M/title-exact?Air+Force+One...\n941\n4\n875048495\n20\nM\nstudent\n97229\n20-29\n\n\n99995\n919\nCity of Lost Children, The (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/Title?Cit%E9+des+enfants+pe...\n941\n5\n875048887\n20\nM\nstudent\n97229\n20-29\n\n\n99996\n273\nHeat (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?Heat%20(1995)\n941\n3\n875049038\n20\nM\nstudent\n97229\n20-29\n\n\n99998\n294\nLiar Liar (1997)\n21-Mar-1997\nNaN\nhttp://us.imdb.com/Title?Liar+Liar+(1997)\n941\n4\n875048532\n20\nM\nstudent\n97229\n20-29\n\n\n99999\n1007\nWaiting for Guffman (1996)\n31-Jan-1997\nNaN\nhttp://us.imdb.com/M/title-exact?Waiting%20for...\n941\n4\n875049077\n20\nM\nstudent\n97229\n20-29\n\n\n\n\n99548 rows × 13 columns\n\n\n\nVamos verificar a contagem e o rating médio por cada faixa etária:\n\nlens_age = lens.groupby('age_group').agg({'rating': 'mean', 'title':'count'})\n\nlens_age\n\n\n\n\n\n\n\n\nrating\ntitle\n\n\nage_group\n\n\n\n\n\n\n0-9\n3.767442\n43\n\n\n10-19\n3.485511\n8144\n\n\n20-29\n3.465155\n39346\n\n\n30-39\n3.552180\n25575\n\n\n40-49\n3.591265\n14951\n\n\n50-59\n3.635389\n8675\n\n\n60-69\n3.649351\n2618\n\n\n70-79\n3.642857\n196\n\n\n\n\n\n\n\nJovens usuários tendem ser mais críticos que os outros grupos por faixa etária.\nE vamos fazer um gráfico com estas informações.\n\nplt.bar(lens_age.index,lens_age['rating'])\nplt.title(\"Rating médio por faixa etária\")\nplt.show()\n\n\n\n\n\n\n\n\nVamos olhar apenas para os 50 filmes mais avaliados e ver como eles são vistos por cada grupo.\n\nbest50 = atleast_100_movies.head(50)\nbest50.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 50 entries, Close Shave, A (1995) to Apt Pupil (1998)\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   count     50 non-null     int64  \n 1   rating    50 non-null     float64\n 2   mean_age  50 non-null     float64\ndtypes: float64(2), int64(1)\nmemory usage: 1.6+ KB\n\n\n\nbest50.head(5)\n\n\n\n\n\n\n\n\ncount\nrating\nmean_age\n\n\ntitle\n\n\n\n\n\n\n\nClose Shave, A (1995)\n112\n4.49\n31.44\n\n\nSchindler's List (1993)\n298\n4.47\n33.45\n\n\nWrong Trousers, The (1993)\n118\n4.47\n31.55\n\n\nCasablanca (1942)\n243\n4.46\n35.90\n\n\nShawshank Redemption, The (1994)\n283\n4.45\n32.81\n\n\n\n\n\n\n\n\nlens50 = lens.set_index('title')\nlens50.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 99548 entries, Kolya (1996) to Waiting for Guffman (1996)\nData columns (total 12 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   movie_id            99548 non-null  int64   \n 1   release_date        99539 non-null  object  \n 2   video_release_date  0 non-null      float64 \n 3   imdb_url            99535 non-null  object  \n 4   user_id             99548 non-null  int64   \n 5   rating              99548 non-null  int64   \n 6   unix_timestamp      99548 non-null  int64   \n 7   age                 99548 non-null  int64   \n 8   sex                 99548 non-null  object  \n 9   occupation          99548 non-null  object  \n 10  zip_code            99548 non-null  object  \n 11  age_group           99548 non-null  category\ndtypes: category(1), float64(1), int64(5), object(5)\nmemory usage: 9.2+ MB\n\n\n\nby_age = lens50.loc[best50.index]\n\nby_age.reset_index()\n\n\n\n\n\n\n\n\ntitle\nmovie_id\nrelease_date\nvideo_release_date\nimdb_url\nuser_id\nrating\nunix_timestamp\nage\nsex\noccupation\nzip_code\nage_group\n\n\n\n\n0\nClose Shave, A (1995)\n408\n28-Apr-1996\nNaN\nhttp://us.imdb.com/M/title-exact?Close%20Shave...\n305\n5\n886323189\n23\nM\nprogrammer\n94086\n20-29\n\n\n1\nClose Shave, A (1995)\n408\n28-Apr-1996\nNaN\nhttp://us.imdb.com/M/title-exact?Close%20Shave...\n6\n4\n883599075\n42\nM\nexecutive\n98101\n40-49\n\n\n2\nClose Shave, A (1995)\n408\n28-Apr-1996\nNaN\nhttp://us.imdb.com/M/title-exact?Close%20Shave...\n286\n4\n875806800\n27\nM\nstudent\n15217\n20-29\n\n\n3\nClose Shave, A (1995)\n408\n28-Apr-1996\nNaN\nhttp://us.imdb.com/M/title-exact?Close%20Shave...\n303\n4\n879467035\n19\nM\nstudent\n14853\n10-19\n\n\n4\nClose Shave, A (1995)\n408\n28-Apr-1996\nNaN\nhttp://us.imdb.com/M/title-exact?Close%20Shave...\n299\n4\n877877847\n29\nM\ndoctor\n63108\n20-29\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n11173\nApt Pupil (1998)\n315\n23-Oct-1998\nNaN\nhttp://us.imdb.com/Title?Apt+Pupil+(1998)\n931\n5\n891037577\n60\nM\neducator\n33556\n60-69\n\n\n11174\nApt Pupil (1998)\n315\n23-Oct-1998\nNaN\nhttp://us.imdb.com/Title?Apt+Pupil+(1998)\n934\n4\n891188403\n61\nM\nengineer\n22902\n60-69\n\n\n11175\nApt Pupil (1998)\n315\n23-Oct-1998\nNaN\nhttp://us.imdb.com/Title?Apt+Pupil+(1998)\n940\n4\n884801125\n32\nM\nadministrator\n02215\n30-39\n\n\n11176\nApt Pupil (1998)\n315\n23-Oct-1998\nNaN\nhttp://us.imdb.com/Title?Apt+Pupil+(1998)\n942\n4\n891282355\n48\nF\nlibrarian\n78209\n40-49\n\n\n11177\nApt Pupil (1998)\n315\n23-Oct-1998\nNaN\nhttp://us.imdb.com/Title?Apt+Pupil+(1998)\n926\n4\n888351623\n49\nM\nentertainment\n01701\n40-49\n\n\n\n\n11178 rows × 13 columns\n\n\n\n\nby_age.groupby(['title','age_group']).agg({'rating': 'mean'})\n\n\n\n\n\n\n\n\n\nrating\n\n\ntitle\nage_group\n\n\n\n\n\n12 Angry Men (1957)\n0-9\nNaN\n\n\n10-19\n4.500000\n\n\n20-29\n4.230769\n\n\n30-39\n4.382353\n\n\n40-49\n4.500000\n\n\n...\n...\n...\n\n\nWrong Trousers, The (1993)\n30-39\n4.558824\n\n\n40-49\n4.538462\n\n\n50-59\n4.625000\n\n\n60-69\n5.000000\n\n\n70-79\nNaN\n\n\n\n\n400 rows × 1 columns\n\n\n\n\n\nQuais são os filmes que mulheres e homens mais se diferem em gostos ?\n\nlens.index\n\nIndex([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\n       ...\n       99989, 99990, 99991, 99992, 99993, 99994, 99995, 99996, 99998, 99999],\n      dtype='int64', length=99548)\n\n\n\npivoted = lens.pivot_table(index=['title'], \n                           columns=['sex'], \n                           values='rating', \n                           fill_value=0)\n                           \npivoted.head(5)\n\n\n\n\n\n\n\nsex\nF\nM\n\n\ntitle\n\n\n\n\n\n\n'Til There Was You (1997)\n2.200000\n2.500000\n\n\n1-900 (1994)\n1.000000\n3.000000\n\n\n101 Dalmatians (1996)\n3.116279\n2.772727\n\n\n12 Angry Men (1957)\n4.269231\n4.363636\n\n\n187 (1997)\n3.500000\n2.870968\n\n\n\n\n\n\n\nVamos calcular a diferença entre as notas de homens e mulheres:\n\npivoted['diff']=pivoted.M - pivoted.F\n\npivoted.head(5)\n\n\n\n\n\n\n\nsex\nF\nM\ndiff\n\n\ntitle\n\n\n\n\n\n\n\n'Til There Was You (1997)\n2.200000\n2.500000\n0.300000\n\n\n1-900 (1994)\n1.000000\n3.000000\n2.000000\n\n\n101 Dalmatians (1996)\n3.116279\n2.772727\n-0.343552\n\n\n12 Angry Men (1957)\n4.269231\n4.363636\n0.094406\n\n\n187 (1997)\n3.500000\n2.870968\n-0.629032\n\n\n\n\n\n\n\n\ndisagreements = pivoted[pivoted.index.isin(best50.index)]['diff']\n\n\ndisagreements_sorted = disagreements.sort_values()\n\ndisagreements_sorted\n\ntitle\nSling Blade (1996)                                                            -0.284314\nPhiladelphia Story, The (1940)                                                -0.239583\nSchindler's List (1993)                                                       -0.226519\nIt's a Wonderful Life (1946)                                                  -0.224182\nSecrets & Lies (1996)                                                         -0.212963\nGraduate, The (1967)                                                          -0.198571\nClose Shave, A (1995)                                                         -0.169213\nHenry V (1989)                                                                -0.167255\nShawshank Redemption, The (1994)                                              -0.151541\nGood Will Hunting (1997)                                                      -0.132911\nTo Kill a Mockingbird (1962)                                                  -0.081159\nTaxi Driver (1976)                                                            -0.061303\nManchurian Candidate, The (1962)                                              -0.054348\nTitanic (1997)                                                                -0.047139\nDr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)   -0.044608\nSilence of the Lambs, The (1991)                                              -0.040690\nPsycho (1960)                                                                 -0.038637\nWrong Trousers, The (1993)                                                     0.028083\nBraveheart (1995)                                                              0.031136\nRear Window (1954)                                                             0.048148\nUsual Suspects, The (1995)                                                     0.065728\nCinema Paradiso (1988)                                                         0.071970\nCasablanca (1942)                                                              0.073404\nChinatown (1974)                                                               0.087179\nKilling Fields, The (1984)                                                     0.091039\n12 Angry Men (1957)                                                            0.094406\nCitizen Kane (1941)                                                            0.109337\nMr. Smith Goes to Washington (1939)                                            0.110000\nAs Good As It Gets (1997)                                                      0.119048\nBoot, Das (1981)                                                               0.129814\nL.A. Confidential (1997)                                                       0.146463\nStar Wars (1977)                                                               0.153115\nAmadeus (1984)                                                                 0.172094\nPrincess Bride, The (1987)                                                     0.176333\nGreat Escape, The (1963)                                                       0.182092\nOne Flew Over the Cuckoo's Nest (1975)                                         0.183333\nFargo (1996)                                                                   0.185044\nBlade Runner (1982)                                                            0.192640\nNorth by Northwest (1959)                                                      0.200366\nGodfather, The (1972)                                                          0.201082\nRaiders of the Lost Ark (1981)                                                 0.217328\nVertigo (1958)                                                                 0.231177\nAfrican Queen, The (1951)                                                      0.245614\nApt Pupil (1998)                                                               0.263323\nEmpire Strikes Back, The (1980)                                                0.300964\nGodfather: Part II, The (1974)                                                 0.323521\nMaltese Falcon, The (1941)                                                     0.355844\nLawrence of Arabia (1962)                                                      0.398715\nRaging Bull (1980)                                                             0.471655\nBridge on the River Kwai, The (1957)                                           0.621978\nName: diff, dtype: float64\n\n\n\n# Create the horizontal bar plot\ndisagreements_sorted.plot(kind='barh', figsize=[9, 15])\n\n# Add title and labels\nplt.title('Diferença Média no Rating\\n(Diferença &gt; 0 = Favoritado por homens)')\nplt.ylabel('Título')\nplt.xlabel('Diferença Média no Rating')\n\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "pydata4/india/Maps_with_diffeent_approaches.html",
    "href": "pydata4/india/Maps_with_diffeent_approaches.html",
    "title": "Using Matplotlib",
    "section": "",
    "text": "# required imports\n\nimport pandas as pd\nimport seaborn as sns\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\nsns.set_style('whitegrid')\n\n\n#Reading the shape file\nfp = r'C:\\Users\\pc\\Desktop\\chloropleth\\india-polygon.shp'\nmap_df = gpd.read_file(fp)\nmap_df.head()\n\n\n\n\n\n\n\n\nid\nst_nm\ngeometry\n\n\n\n\n0\nNone\nAndaman and Nicobar Islands\nMULTIPOLYGON (((93.84831 7.24028, 93.92705 7.0...\n\n\n1\nNone\nArunachal Pradesh\nPOLYGON ((95.23643 26.68105, 95.19594 27.03612...\n\n\n2\nNone\nAssam\nPOLYGON ((95.19594 27.03612, 95.08795 26.94578...\n\n\n3\nNone\nBihar\nPOLYGON ((88.11357 26.54028, 88.28006 26.37640...\n\n\n4\nNone\nChandigarh\nPOLYGON ((76.84208 30.76124, 76.83758 30.72552...\n\n\n\n\n\n\n\n\n#Reading the excel\ndata_df = pd.read_excel(r'C:\\Users\\pc\\Desktop\\chloropleth\\data_ecxel.xlsx')\ndata_df.head()\n\n\n\n\n\n\n\n\nS. No.\nName of State / UT\nTotal Confirmed cases\nCured/Discharged/\nDeath\n\n\n\n\n0\n1\nAndaman and Nicobar Islands\n12\n11\n0\n\n\n1\n2\nAndhra Pradesh\n603\n42\n15\n\n\n2\n3\nArunachal Pradesh\n1\n0\n0\n\n\n3\n4\nAssam\n35\n9\n1\n\n\n4\n5\nBihar\n85\n37\n2\n\n\n\n\n\n\n\n\n#Merging the data\nmerged = map_df.set_index('st_nm').join(data_df.set_index('Name of State / UT'))\nmerged.head()\n\n\n\n\n\n\n\n\nid\ngeometry\nS. No.\nTotal Confirmed cases\nCured/Discharged/\nDeath\n\n\nst_nm\n\n\n\n\n\n\n\n\n\n\nAndaman and Nicobar Islands\nNone\nMULTIPOLYGON (((93.84831 7.24028, 93.92705 7.0...\n1.0\n12.0\n11.0\n0.0\n\n\nArunachal Pradesh\nNone\nPOLYGON ((95.23643 26.68105, 95.19594 27.03612...\n3.0\n1.0\n0.0\n0.0\n\n\nAssam\nNone\nPOLYGON ((95.19594 27.03612, 95.08795 26.94578...\n4.0\n35.0\n9.0\n1.0\n\n\nBihar\nNone\nPOLYGON ((88.11357 26.54028, 88.28006 26.37640...\n5.0\n85.0\n37.0\n2.0\n\n\nChandigarh\nNone\nPOLYGON ((76.84208 30.76124, 76.83758 30.72552...\n6.0\n21.0\n9.0\n0.0\n\n\n\n\n\n\n\n\n# create figure and axes for Matplotlib and set the title\nfig, ax = plt.subplots(1, figsize=(10, 10))\nax.axis('off')\nax.set_title('Deaths', fontdict={'fontsize': '25', 'fontweight' : '10'})\n\n# plot the figure\nmerged.plot(column='Total Confirmed cases',cmap='YlOrRd', linewidth=0.8, ax=ax, edgecolor='0', legend=True,markersize=[39.739192, -104.990337])\n\n\n\n\n\n\n\n\n\nUsing GeoPlot\n\nimport geopandas as gdp\nimport geoplot as gplt\nimport geoplot.crs as gcrs\n\n\ngplt.polyplot(merged)\n\n\n\n\n\n\n\n\n\ngplt.choropleth(\n    merged, hue='Total Confirmed cases', projection=gcrs.AlbersEqualArea(),\n    edgecolor='black', linewidth=1,\n    cmap='YlOrRd', legend=True\n)\n\n\n\n\n\n\n\n\n\n\nUsing Folium\n\nimport folium as flm\nfrom folium import plugins\nimport ipywidgets\nimport geocoder\nimport geopy\nimport numpy as np\nimport pandas as pd\n\n\nimport json\n\n# load geo_json\n# shapefiles can be converted to geojson with QGIS\nwith open(r'C:\\Users\\pc\\Desktop\\chloropleth\\GeoJson-Data-of-Indian-States-master\\Indian_States') as f:\n    geojson_counties = json.load(f)\n\n\nfor i in geojson_counties['features']:\n    i['id'] = i['properties']['NAME_1']\n    \n# load data associated with geo_json\npop_df = pd.read_excel(r'C:\\Users\\pc\\Desktop\\chloropleth\\data_ecxel.xlsx')\npop_df.head()\n\n\n\n\n\n\n\n\nS. No.\nName of State / UT\nTotal Confirmed cases\nCured/Discharged/\nDeath\n\n\n\n\n0\n1\nAndaman and Nicobar Islands\n12\n11\n0\n\n\n1\n2\nAndhra Pradesh\n603\n42\n15\n\n\n2\n3\nArunachal Pradesh\n1\n0\n0\n\n\n3\n4\nAssam\n35\n9\n1\n\n\n4\n5\nBihar\n85\n37\n2\n\n\n\n\n\n\n\n\nmap1 = flm.Map(location=[20.5937,78.9629], zoom_start=4)\n\n\nflm.Choropleth(\n    geo_data=geojson_counties,\n    name='choropleth',\n    data=pop_df,\n    columns=['Name of State / UT', 'Total Confirmed cases'],\n    # see folium.Choropleth? for details on key_on\n    key_on='feature.id',\n    fill_color='YlGn',\n    fill_opacity=0.5,\n    line_opacity=0.5).add_to(map1)\n\n# layer control to turn choropleth on or off\nflm.LayerControl().add_to(map1)\n\n# display map\nmap1"
  },
  {
    "objectID": "Lab3.html",
    "href": "Lab3.html",
    "title": "Lab 3 - Métodos Supervisionados",
    "section": "",
    "text": "Para praticar nossas novas habilidades em Machine Learning, iremos utilizar dois datasets, um para treinar modelos de classificação (titanic.csv) e outro para treinar modelos de regressão (houseprice.csv). Para isso vamos carregar os pacotes a seguir e eventualmente outros se forem necessários.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(tidymodels)\nlibrary(ranger)\nlibrary(rpart)\nlibrary(rpart.plot)\n\nA entrega deverá incluir uma análise exploratória de dados, prévia à construção dos modelos de machine learning. Deverão ser utilizados os três modelos aprendidos: regressão linear múltivariada, árvores de decisão e random forests para o problema de regressão, e regressão logística, árvores de decisão e random forests para o problema de classificação.\nAdicionalmente, será avaliada a estrutura e as etapas de machine learning para a resolução dos problemas, incluindo 1) a separação em dados de treino e teste, 2) o uso de indicadores de acurácia específicos para regressão ou classificação, 3) a escolha das variáveis a serem utilizadas para o treino e a justificativa de inclusão de cada variável.\nO documento deverá ser entregue em formato PDF pelo moodle, ou seja, deverá ser realizado no RStudio e deverá conter, além dos scripts utilizados e dos resultados gráficos e numéricos, uma descrição das atividades realizadas (comentários que ajudem à compreensão do que foi feito)."
  },
  {
    "objectID": "Lab3.html#instruções",
    "href": "Lab3.html#instruções",
    "title": "Lab 3 - Métodos Supervisionados",
    "section": "",
    "text": "Para praticar nossas novas habilidades em Machine Learning, iremos utilizar dois datasets, um para treinar modelos de classificação (titanic.csv) e outro para treinar modelos de regressão (houseprice.csv). Para isso vamos carregar os pacotes a seguir e eventualmente outros se forem necessários.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(tidymodels)\nlibrary(ranger)\nlibrary(rpart)\nlibrary(rpart.plot)\n\nA entrega deverá incluir uma análise exploratória de dados, prévia à construção dos modelos de machine learning. Deverão ser utilizados os três modelos aprendidos: regressão linear múltivariada, árvores de decisão e random forests para o problema de regressão, e regressão logística, árvores de decisão e random forests para o problema de classificação.\nAdicionalmente, será avaliada a estrutura e as etapas de machine learning para a resolução dos problemas, incluindo 1) a separação em dados de treino e teste, 2) o uso de indicadores de acurácia específicos para regressão ou classificação, 3) a escolha das variáveis a serem utilizadas para o treino e a justificativa de inclusão de cada variável.\nO documento deverá ser entregue em formato PDF pelo moodle, ou seja, deverá ser realizado no RStudio e deverá conter, além dos scripts utilizados e dos resultados gráficos e numéricos, uma descrição das atividades realizadas (comentários que ajudem à compreensão do que foi feito)."
  },
  {
    "objectID": "Lab3.html#sobre-o-dataset-titanic",
    "href": "Lab3.html#sobre-o-dataset-titanic",
    "title": "Lab 3 - Métodos Supervisionados",
    "section": "Sobre o Dataset Titanic",
    "text": "Sobre o Dataset Titanic\nPara mostrar a utilidade destes modelos, vamos usar um dataset sobre os sobrevivientes do Titanic. Há várias formas de carregar os dados, incluindo o pacote Titanic. Os dados mostram cada um dos passageiros do Titanic, incluindo informações como o nome, a idade, se estavam com filhos, esposos/esposas, em qual categoria de cabine se encontravam, quanto pagaram pela passagem, etc. Mais informações podem ser encontradas em: https://www.kaggle.com/c/titanic/data"
  },
  {
    "objectID": "Lab3.html#sobre-o-dataset-houseprice",
    "href": "Lab3.html#sobre-o-dataset-houseprice",
    "title": "Lab 3 - Métodos Supervisionados",
    "section": "Sobre o Dataset HousePrice",
    "text": "Sobre o Dataset HousePrice\nOs dados referem-se a 12 características de propriedades à venda na Índia que foram coletadas por diferentes motores de busca. A construção do modelo deve ajudar na previsão do preço final de venda. Mais detalhes podem ser encontrados em: https://www.kaggle.com/datasets/anmolkumar/house-price-prediction-challenge"
  },
  {
    "objectID": "Lab6.html",
    "href": "Lab6.html",
    "title": "Lab 6 - Métodos Não-Supervisionados em Machine Learning",
    "section": "",
    "text": "Para este lab vamos usar 3 datasets:\n\nO primeiro sobre a satisfação e lealdade de clientes;\nO segundo sobre compras de produtos (para realizar segmentação de mercado);\nO último dataset sobre salários de diferentes profissões (para agrupamento de profissões com crescimento de salários similares).\n\nVocê pode baixar os dados\n\n\naqui."
  },
  {
    "objectID": "Lab6.html#sobre-os-dados",
    "href": "Lab6.html#sobre-os-dados",
    "title": "Lab 6 - Métodos Não-Supervisionados em Machine Learning",
    "section": "",
    "text": "Para este lab vamos usar 3 datasets:\n\nO primeiro sobre a satisfação e lealdade de clientes;\nO segundo sobre compras de produtos (para realizar segmentação de mercado);\nO último dataset sobre salários de diferentes profissões (para agrupamento de profissões com crescimento de salários similares).\n\nVocê pode baixar os dados\n\n\naqui."
  },
  {
    "objectID": "Lab6.html#carregando-pacotes",
    "href": "Lab6.html#carregando-pacotes",
    "title": "Lab 6 - Métodos Não-Supervisionados em Machine Learning",
    "section": "Carregando pacotes",
    "text": "Carregando pacotes\n\nlibrary(tidyverse)\nlibrary(dendextend) # para colorir os agrupamentos do cluster hirárquico\nlibrary(cluster) #para usar função pam\nlibrary(cowplot) #para plotar vários gráficos juntos\nlibrary(plotly)\nlibrary(factoextra)"
  },
  {
    "objectID": "Lab6.html#segmentação-de-clientes---k-means",
    "href": "Lab6.html#segmentação-de-clientes---k-means",
    "title": "Lab 6 - Métodos Não-Supervisionados em Machine Learning",
    "section": "Segmentação de clientes - K-means",
    "text": "Segmentação de clientes - K-means\nVamos começar com um exemplo simples de segmentação de clientes, usando apenas dois atributos: a satisfação do cliente e a sua lealdade.\n\ncustomers &lt;- read.csv(\"data5/customers.csv\")\n\ncustomers %&gt;% \n  ggplot(aes(Satisfaction, Loyalty))+\n  geom_point(size=2)\n\n\n\n\n\n\n\ncustomers &lt;- scale(customers)\n\ncustomers_df &lt;- as.tibble(customers)\n\nmodel &lt;- kmeans(customers_df, centers = 2)\n\ncluster_km &lt;- model$cluster\n\ncustomers_df %&gt;% \n  mutate(cluster = cluster_km) %&gt;% \n  ggplot(aes(Satisfaction, Loyalty, color = factor(cluster)))+\n  geom_point(size=3)+\n  theme_bw()+\n  scale_color_viridis_d()\n\n\n\n\n\n\n\n\nPara escolher o número ideal de clusters, podemos usar o método do “Elbow”. Para isto, vamos criar primeiramente uma função que facilite a criação de modelos k-means, logo iremos usar esta função criar_k_means() para gerar os valores das somas dos quadrados para diferentes valores de k.\n\nMétodo Elbow\n\ncriar_k_means &lt;- function(k){\n  model &lt;- kmeans(customers_df, centers = k)\n  model$tot.withinss\n}\n\nk = 10\n\nelbow &lt;- data.frame(k=1:k)\nelbow$tot.withinss &lt;- 0\n\n\nfor(i in 1:k) {\n  elbow$tot.withinss[i] &lt;- criar_k_means(i)\n}\n\nelbow %&gt;% \n  ggplot(aes(k, tot.withinss))+\n  geom_line(color=\"red\")+\n  geom_point(size=4, color=\"red\")+\n  scale_x_continuous(breaks=1:10)+\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nMétodo da Silueta\nSimilarmente, usaremos um segundo método para identificar o número ideal de cluster, o método da silueta.\n\nlibrary(cluster)\n\ncriar_siluetas &lt;- function(k){\n  model &lt;- pam(customers_df, k = k)\n  model$silinfo$avg.width\n}\n\nk = 10\n\nsil_df &lt;- data.frame(k=2:k)\nsil_df$sil_width &lt;- 0\n\n\nfor(i in 2:k) {\n  sil_df$sil_width[i-1] &lt;- criar_siluetas(i)\n}\n\nsil_df %&gt;% \n  ggplot(aes(k, sil_width))+\n  geom_line(color=\"#e32d91\")+\n  geom_point(size=4, color=\"#e32d91\")+\n  scale_x_continuous(breaks=1:10)+\n  theme_bw()\n\n\n\n\n\n\n\n\nNeste método, quanto maior for o valor do sil_width, melhor será a escolha do número de clusters. Para o exemplo, o valor mais alto é quando são 2 clusters (0.47).\n\n\nCom factoextra\nVamos repetir o exercício anterior usando o pacote factoextra e a função eclust().\n\nres.km &lt;- eclust(customers_df, \"kmeans\", k=2)\n\n\n\n\n\n\n\n\nGráfico de dispersão:\n\nfviz_cluster(res.km)\n\n\n\n\n\n\n\n\nNo gráfico da silueta abaixo, podemos verificar que o valor médio do width é de 0.47. Quanto mais alto for este número melhor.\n\nfviz_silhouette(res.km)\n\n  cluster size ave.sil.width\n1       1   22          0.44\n2       2    8          0.53\n\n\n\n\n\n\n\n\n\nO objeto res.km contém as informações dos clusters:\n\nres.km$cluster\n\n [1] 2 1 2 1 1 2 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 1 2 1 1 1 1 2 1\n\n\nO que significa que podemos incluir essa coluna no dataframe original para assignar um cluster a cada observação:\n\ncustomers_segmentados &lt;- customers_df %&gt;% \n  mutate(cluster = res.km$cluster)"
  },
  {
    "objectID": "Lab6.html#segmentação-de-clientes---cluster-hierárquico",
    "href": "Lab6.html#segmentação-de-clientes---cluster-hierárquico",
    "title": "Lab 6 - Métodos Não-Supervisionados em Machine Learning",
    "section": "Segmentação de clientes - Cluster hierárquico",
    "text": "Segmentação de clientes - Cluster hierárquico\ncalcular a distância euclidiana entre clientes\n\nclientes &lt;- readRDS(\"data5/ws_customers.rds\")\n\ndist_clientes &lt;- dist(clientes)\n\ncriar uma análise usando o algoritmo da distância completa:\n\nch_clientes &lt;- hclust(dist_clientes, method = \"complete\")\n\ngraficar o dendograma:\n\nplot(ch_clientes)\n\n\n\n\n\n\n\n\nCriar uma asignação de clusters no valor de altura h = 15000\n\nclust_clientes &lt;- cutree(ch_clientes, h=15000)\n\nGerar um dataframe com os clientes segmentados\n\nclientes_segmentados &lt;- mutate(clientes, cluster = clust_clientes)\n\nCalcular o número de clientes que fazer parte de cada cluster\n\ncount(clientes_segmentados, cluster)\n\n  cluster  n\n1       1  5\n2       2 29\n3       3  5\n4       4  6\n\n\ncolorir o dendograma com base no valor de cutoff\n\ndend_clientes &lt;- as.dendrogram(ch_clientes)\ndend_colorido &lt;- color_branches(dend_clientes, h=15000)\n\nplot(dend_colorido)\n\n\n\n\n\n\n\n\ncalcular a média para cada cluster\n\nclientes_segmentados %&gt;% \n  group_by(cluster) %&gt;% \n  summarise_all(mean)\n\n# A tibble: 4 × 4\n  cluster   Milk Grocery Frozen\n    &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1       1 16950   12891.   991.\n2       2  2513.   5229.  1796.\n3       3 10452.  22551.  1355.\n4       4  1250.   3917. 10889.\n\n\nCom base neste resultado, podemos inferir que:\n\nOs clientes do cluster 1 gastaram mais em Leite do que os outros grupos\nOs clientes do cluster 3 gastaram mais em mercado do que os outros grupos\nos clientes do cluster 4 gastaram mais em frozen iogurte do que os outros grupos\nOs clientes do cluster 2 gastaram consideravelmente menos do que os outros clusters\n\n\nCom factoextra\nVamos repetir o exercício anterior usando o pacote factoextra e a função eclust().\nfviz_dend(res.hc, rect = TRUE)\n\nclientes_df &lt;- scale(clientes)\n\nres.hc &lt;- eclust(clientes_df, \"hclust\")\n\nfviz_dend(res.hc, rect = TRUE)\n\n\n\n\n\n\n\n\nGráfico da silueta\n\nfviz_silhouette(res.hc)\n\n  cluster size ave.sil.width\n1       1   10          0.35\n2       2   29          0.62\n3       3    6          0.57\n\n\n\n\n\n\n\n\n\nGráfico de dispersão:\n\nfviz_cluster(res.hc)\n\n\n\n\n\n\n\n\nO objeto res.hc contém as informações dos clusters:\n\nres.hc$cluster\n\n [1] 1 2 2 2 2 2 2 2 1 2 2 1 1 1 2 2 1 3 2 1 2 2 1 2 2 2 1 2 2 2 2 2 2 2 3 2 2 1\n[39] 2 3 2 2 3 3 3\n\n\nO que significa que podemos incluir essa coluna no dataframe original para assignar um cluster a cada observação:\n\nclientes_segmentados &lt;- clientes %&gt;% \n  mutate(cluster = res.hc$cluster)"
  },
  {
    "objectID": "Lab6.html#agrupamento-de-profissões-a-partir-do-crescimento-dos-salários",
    "href": "Lab6.html#agrupamento-de-profissões-a-partir-do-crescimento-dos-salários",
    "title": "Lab 6 - Métodos Não-Supervisionados em Machine Learning",
    "section": "Agrupamento de profissões a partir do crescimento dos salários",
    "text": "Agrupamento de profissões a partir do crescimento dos salários\nVamos carregar os dados:\n\noes &lt;- readRDS(\"data5/oes.rds\")\n\n\nClustering hierárquico\npassos necessários conforme o método\n\n# Calculate Euclidean distance between the occupations\ndist_oes &lt;- dist(oes, method = \"euclidean\")\n\n# Generate an average linkage analysis \nhc_oes &lt;- hclust(dist_oes, method = \"average\")\n\n# Create a dendrogram object from the hclust variable\ndend_oes &lt;- as.dendrogram(hc_oes)\n\n# Plot the dendrogram\nplot(dend_oes)\n\n\n\n\n\n\n\n# Color branches by cluster formed from the cut at a height of 100000\ndend_colored &lt;- color_branches(dend_oes, h = 100000)\n\n# Plot the colored dendrogram\nplot(dend_colored)\n\n\n\n\n\n\n\n\nagora, vamos ‘recortar’ a árvore e vamos incluir o agrupamento num novo dataframe.\n\n# Use rownames_to_column to move the rownames into a column of the data frame\ndf_oes &lt;- rownames_to_column(as.data.frame(oes), var = 'occupation')\n\n# Create a cluster assignment vector at h = 100,000\ncut_oes &lt;- cutree(hc_oes, h = 100000)\n\n# Generate the segmented the oes data frame\nclust_oes &lt;- mutate(df_oes, cluster = cut_oes)\n\n# Create a tidy data frame by gathering the year and values into two columns\ngathered_oes &lt;- gather(data = clust_oes, \n                       key = year, \n                       value = mean_salary, \n                       -occupation, -cluster)\n\nAgora vamos plotar os grupos de clusters para cada ano:\n\n# Plot the relationship between mean_salary and year and color the lines by the assigned cluster\n\nclust3plot &lt;- ggplot(gathered_oes, aes(x = year, y = mean_salary, color = factor(cluster))) + \n    geom_line(aes(group = occupation))+\n  labs(title = \"Clustering Hierárquico com k = 3\")\n\nclust3plot\n\n\n\n\n\n\n\n\n\n\nK-means\nO algoritmo do k-means segue a lógica descrita no exercício anterior. O primeiro passo é criar duas funções customizadas, que nos ajudarão a extrair as informações necessárias para a escolha do número de \\(k\\) grupos.\n\n# Use map_dbl to run many models with varying value of k (centers)\ntot_withinss &lt;- map_dbl(1:10,  function(k){\n  model &lt;- kmeans(x = oes, centers = k)\n  model$tot.withinss\n})\n\n# Generate a data frame containing both k and tot_withinss\nelbow_df &lt;- data.frame(\n  k = 1:10,\n  tot_withinss = tot_withinss\n)\n\n# Plot the elbow plot\nggplot(elbow_df, aes(x = k, y = tot_withinss)) +\n  geom_line() +\n  scale_x_continuous(breaks = 1:10)\n\n\n\n\n\n\n\n\nO interessante é que o método do Elbow sugere 2 clusters, diferentemente do clustering hierárquico que sugeria 3.\n\ncluster_2 &lt;- kmeans(oes, centers = 2)\n\nclust2_oes &lt;- mutate(df_oes, cluster = cluster_2$cluster)\n\nclust2plot &lt;- clust2_oes %&gt;% \n  pivot_longer(2:16, names_to = \"year\", \n               values_to = \"mean_salary\") %&gt;% \n  ggplot(aes(year, mean_salary, color = factor(cluster)))+\n  geom_line(aes(group=occupation))+\n  labs(title = \"Kmeans com k = 2\")\n\nclust2plot\n\n\n\n\n\n\n\n\nVamos usar a largura média da silhueta (ASW) para tentar decidir qual número de clusters é o melhor.\n\n# Use map_dbl to run many models with varying value of k\nsil_width &lt;- map_dbl(2:10,  function(k){\n  model &lt;- pam(oes, k = k)\n  model$silinfo$avg.width\n})\n\n# Generate a data frame containing both k and sil_width\nsil_df &lt;- data.frame(\n  k = 2:10,\n  sil_width = sil_width\n)\n\n# Plot the relationship between k and sil_width\nggplot(sil_df, aes(x = k, y = sil_width)) +\n  geom_line(color = \"#e32d91\", size =1) +\n  scale_x_continuous(breaks = 2:10)\n\n\n\n\n\n\n\n\nNeste método, quanto mais alto for o valor de um referido cluster, melhor, ou seja, este método sugere que o melhor número de clusters seria 7, pois obteve o valor mais alto de largura, seguido pelo cluster k = 2.\n\ncluster_7 &lt;- kmeans(oes, centers = 7)\n\nclust7_oes &lt;- mutate(df_oes, cluster = cluster_7$cluster)\n\nclust7plot &lt;- clust7_oes %&gt;% \n  pivot_longer(2:16, names_to = \"year\", \n               values_to = \"mean_salary\") %&gt;% \n  ggplot(aes(year, mean_salary, color = factor(cluster)))+\n  geom_line(aes(group=occupation))+\n  labs(title = \"Kmeans com k = 7\")\n\nclust7plot\n\n\n\n\n\n\n\n\nVamos comparar os três resultados:\n\nplot_grid(clust2plot, clust3plot, clust7plot, ncol = 3, label_size = 10)\n\n\n\n\n\n\n\n\n\n\nCom factoextra\nVamos refazer este terceiro exercício usando factoextra para k-means e cluster hierárquico.\n\nK-means\nOs dados já se encontram em formato de matriz podemos rodar o algoritmo\n\noes_scale &lt;- scale(oes)\n\nres.km &lt;- eclust(oes_scale, \"kmeans\", k=6)\n\n\n\n\n\n\n\nfviz_silhouette(res.km)\n\n  cluster size ave.sil.width\n1       1    1          0.00\n2       2    6          0.53\n3       3    4          0.61\n4       4    2          0.70\n5       5    5          0.71\n6       6    4          0.73\n\n\n\n\n\n\n\n\n\nAgora podemos criar um dataframe dos dados originais com os clusters\n\ndf_oes &lt;- rownames_to_column(as.data.frame(oes), var = 'occupation')\n\ndf_oes &lt;- df_oes %&gt;% \n  mutate(cluster_km = res.km$cluster)\n\n\n\nCluster hierárquico\nvamos usar a matriz escalada:\n\nres.hc &lt;- eclust(oes_scale, \"hclust\", k=6)\n\nfviz_silhouette(res.hc)\n\n  cluster size ave.sil.width\n1       1    2          0.63\n2       2    3          0.68\n3       3    2          0.60\n4       4    6          0.52\n5       5    5          0.70\n6       6    4          0.74\n\n\n\n\n\n\n\n\n\nGraficar o dendograma\n\nfviz_dend(res.hc, rect = TRUE)\n\n\n\n\n\n\n\n\nVamos incluir os resultados no df:\n\ndf_oes &lt;- df_oes %&gt;% \n  mutate(cluster_hc = res.hc$cluster)\n\nFinalmente, podemos graficar as profissões por clusters\n\ndf_oes %&gt;% \n  pivot_longer(2:16, names_to = \"year\", values_to = \"mean_salary\") %&gt;% \n  ggplot(aes(year, mean_salary, color=factor(cluster_km)))+\n  geom_line(aes(group = occupation))+\n  theme_bw()\n\n\n\n\n\n\n\ndf_oes %&gt;% \n  pivot_longer(2:16, names_to = \"year\", values_to = \"mean_salary\") %&gt;% \n  ggplot(aes(year, mean_salary, color=factor(cluster_hc)))+\n  geom_line(aes(group = occupation))+\n  theme_bw()"
  },
  {
    "objectID": "data1/Tutorial1.html",
    "href": "data1/Tutorial1.html",
    "title": "Tutorial 1 - Tidyverse",
    "section": "",
    "text": "Carregando os dados\nNeste tutorial vamos aprender um pouco mais sobre o tidyverse, em particular vamos conhecer as funções do dplyr:\n\nselect() - seleciona colunas\narrange() - ordena os dados\nfilter() - filtra linhas\nmutate() - cria/modifica colunas\ngroup_by() - agrupa dados por categorias\nsummarize() - sumariza dados\n\nPara praticar, vamos usar o dataset netflix_series_limpo.xlsx e o dataset imdb_series.xlsx. Portanto, o primeiro passo será carregar o dataset:\nAlternativamente, pode usar este link para criar uma cópia no Workspace.\n\nlibrary(tidyverse)\nlibrary(readxl) # para fazer leitura de arquivos excel\nlibrary(knitr) # para formatar tabelas\n\nnetflix &lt;- read_xlsx(\"netflix_series_limpo.xlsx\")\n\nimdb &lt;- read_xlsx(\"imdb_series.xlsx\")\n\nUma visão geral dos dados é mostrada a seguir, usando a função head() para visualizar apenas as primeiras linhas do dataframe netflix, pois o dataset completo conta com 923 linhas.\nTambém podemos ver rapidamente a estrutura de cada dataset, utilizando a função str().\nBem como as dimensões utilizando dim():\n\n\nSelecionando colunas\nPara selecionar colunas, utilizamos a função select(), por exemplo, podemos selecionar a coluna episode como o script a seguir:\nTambém é possível selecionar várias colunas, separando-as com ,:\nA função select() conta com vários outros argumentos, recomenda-se a leitura do Capítulo 7 do Livro “Curso-R”.\n\n\nOrdenando os dados\nPara ordenar as linhas, podemos utilizar a função arrange() de forma a termos, por exemplo, uma lista de maior a menor de um determinado valor. Vamos usar o dataframe imdb para exemplificar, ordenando as linhas por ordem crescente de UserRating:\nGeralmente o uso do arrange() servirá para identificar os maiores valores dentro do dataframe, por exemplo, queremos saber quais são os capítulos com maior UserRating ou seja, ordenar de forma decrescente, além disso, vamos aproveitar o nosso conhecimento de select() para selecionar somente algumas colunas de interesse:\nVocê pode ver mais exemplos do arrange() no livro Curso-R e no livro Ibpad.\n\n\nFiltrando linhas\nPara filtrar valores de um dataframe, primeiro precisamos identificar qual a coluna de interesse e logo utilizar a função filter(). Por exemplo, se quisermos identificar todos os capítulos do dataframe imdb com UserVotes maior a 10.000:\nPara filtrar textos sem correspondência exata, podemos utilizar a função auxiliar str_detect() do pacote stringr. Ela serve para verificar se cada string de um vetor contém um determinado padrão de texto.\nUsando este comando, podemos encontrar qualquer caractere ou string de interesse. Por exemplo, se quisermos identificar todos os capítulos com o string and no título:\nA tabela de cima nos mostra todos os capítulos com and no título porém não nos diz quantos capítulos em total cada seriado tem com o referido string. A próxima função vai nos ajudar a atingir esse objetivo.\nVocê pode ver mais exemplos do filter() no livro Curso-R e no livro Ibpad.\n\n\nAgrupando ou juntando dados por categorias e sumarizando\nA função que usamos para agrupar dados é group_by() e serve para agregar os dados de acordo com algum critério de interesse. Comumente é usado junto com uma função para sumarizar o resultado, chamada summarize(). Vamos mostrar o uso de ambos em conjunto.\nVamos aproveitar o exercício anterior (capítulos com o string and) e agrupar por seriado para finalmente, contabilizar o número de capítulos com o string and para cada seriado.\nVocê pode ver mais exemplos do summarize() no livro Curso-R e de group_by() no livro Ibpad.\n\n\nModificando e/ou criando novas colunas\nPara modificar uma coluna existente ou criar uma nova coluna, utilizamos a função mutate(). Por exemplo, podemos usá-la para criar uma nova coluna no dataframe netflix contendo apenas o ano em que cada capítulo foi exibido (no futuro, esta nova coluna pode nos ajudar a criar um gráfico de barras para cada ano):\nUm segundo exemplo com o dataframe imdb mostra a soma das colunas r1 a r10, numa nova coluna chamada r_total. As referidas colunas são na verdade proporções do número de pessoas que votou com nota 1 (r1) até nota 10 (r10), portanto a soma delas deveria ser igual a \\(1\\).\nVocê pode ver mais exemplos do mutate() no livro Curso-R\nExistem muitas mais funções no tidyverse além das principais que já vimos, dentre elas, vamos mostrar a função join.\n\n\nJuntando dois ou mais dataframes\nExistem vários tipos de join mas aqui vamos focar especificamente na função left_join() que mantém todas as observações da primeira base e adiciona colunas da segunda (para mais detalhes consultar o livro R for Data Science e o livro Curso-R).\nVejamos no exemplo a seguir. Vamos juntar as duas bases, netflix e imdb, tomando como a base de referência netflix. O que queremos é extrair a nota UserRating de imdb e inclui-la como nova coluna na base netflix para cada capítulo e seriado.\nNote que para o R identificar corretamente cada capítulo, neste caso devemos comparar duas colunas de cada base, a do título do seriado e a do título do capítulo.\nFinalmente vamos limpar as colunas e deixar apenas aquelas que nos interessam."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gestão Estratégica da Tecnologia de Informação",
    "section": "",
    "text": "Photo by Jaime Lopes on unsplash\n\n\n\n\nBem-vindos!\nNeste site vocês encontrarão todas as tarefas e webinars que serão realizados ao longo do semestre.\nAgradecemos o apoio de Datacamp."
  },
  {
    "objectID": "Lab4.html",
    "href": "Lab4.html",
    "title": "Lab 4 - Tidymodels - Classificação",
    "section": "",
    "text": "Carregando os dados\nEste webinar usará dados sobre reservas de Hotéis que incluem crianças (versus aquelas que não. Os dados fazem parte deste estudo.\nVamos carregar os dados e incluir apenas as reservas que não foram canceladas uma vez que mais informações são coletadas do hospede na hora do check-in. Neste caso, os dados são automaticamente baixados do site do tidytuesday.\n\nlibrary(tidyverse)\n\nhotels &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\n\n\n\nLimpeza dos dados\nInicialmente é necessário fazer uma limpeza dos dados. Por exemplo, vamos considerar apenas as reservas efetivadas (e não as canceladas), portanto precisamos filtrá-las. Também vamos considerar bebês e crianças dentro da mesma categoria, então iremos criar uma nova coluna ‘children’.\n\nhotel_stays &lt;- hotels %&gt;%\n  filter(is_canceled == 0) %&gt;%\n  mutate(\n    children = case_when(\n      children + babies &gt; 0 ~ \"children\",\n      TRUE ~ \"none\"\n    ),\n    required_car_parking_spaces = case_when(\n      required_car_parking_spaces &gt; 0 ~ \"parking\",\n      TRUE ~ \"none\"\n    )\n  ) %&gt;%\n  select(-is_canceled, -reservation_status, -babies)\n\nhotel_stays %&gt;% \n  head()\n\n# A tibble: 6 × 29\n  hotel    lead_time arrival_date_year arrival_date_month arrival_date_week_nu…¹\n  &lt;chr&gt;        &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;                               &lt;dbl&gt;\n1 Resort …       342              2015 July                                   27\n2 Resort …       737              2015 July                                   27\n3 Resort …         7              2015 July                                   27\n4 Resort …        13              2015 July                                   27\n5 Resort …        14              2015 July                                   27\n6 Resort …        14              2015 July                                   27\n# ℹ abbreviated name: ¹​arrival_date_week_number\n# ℹ 24 more variables: arrival_date_day_of_month &lt;dbl&gt;,\n#   stays_in_weekend_nights &lt;dbl&gt;, stays_in_week_nights &lt;dbl&gt;, adults &lt;dbl&gt;,\n#   children &lt;chr&gt;, meal &lt;chr&gt;, country &lt;chr&gt;, market_segment &lt;chr&gt;,\n#   distribution_channel &lt;chr&gt;, is_repeated_guest &lt;dbl&gt;,\n#   previous_cancellations &lt;dbl&gt;, previous_bookings_not_canceled &lt;dbl&gt;,\n#   reserved_room_type &lt;chr&gt;, assigned_room_type &lt;chr&gt;, …\n\n\nObservamos que há quase 10x mais reservas sem crianças.\n\nhotel_stays %&gt;%\n  count(children)\n\n# A tibble: 2 × 2\n  children     n\n  &lt;chr&gt;    &lt;int&gt;\n1 children  6073\n2 none     69093\n\n\n\n\nAnálise exploratória de dados - EDA\nVamos usar a função skim que ajuda a identificar de forma rápida as características do nosso dataframe, por exemplo, se há valores NA bem como os máximos e mínimos das colunas numéricas, etc.\n\nlibrary(skimr)\n\nskim(hotel_stays)\n\n\nData summary\n\n\nName\nhotel_stays\n\n\nNumber of rows\n75166\n\n\nNumber of columns\n29\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n14\n\n\nDate\n1\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nhotel\n0\n1\n10\n12\n0\n2\n0\n\n\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\n\n\nchildren\n0\n1\n4\n8\n0\n2\n0\n\n\nmeal\n0\n1\n2\n9\n0\n5\n0\n\n\ncountry\n0\n1\n2\n4\n0\n166\n0\n\n\nmarket_segment\n0\n1\n6\n13\n0\n7\n0\n\n\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\n\n\nreserved_room_type\n0\n1\n1\n1\n0\n9\n0\n\n\nassigned_room_type\n0\n1\n1\n1\n0\n10\n0\n\n\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\n\n\nagent\n0\n1\n1\n4\n0\n315\n0\n\n\ncompany\n0\n1\n1\n4\n0\n332\n0\n\n\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\n\n\nrequired_car_parking_spaces\n0\n1\n4\n7\n0\n2\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreservation_status_date\n0\n1\n2015-07-01\n2017-09-14\n2016-09-01\n805\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlead_time\n0\n1\n79.98\n91.11\n0.00\n9.0\n45.0\n124\n737\n▇▂▁▁▁\n\n\narrival_date_year\n0\n1\n2016.15\n0.70\n2015.00\n2016.0\n2016.0\n2017\n2017\n▃▁▇▁▆\n\n\narrival_date_week_number\n0\n1\n27.08\n13.90\n1.00\n16.0\n28.0\n38\n53\n▆▇▇▇▆\n\n\narrival_date_day_of_month\n0\n1\n15.84\n8.78\n1.00\n8.0\n16.0\n23\n31\n▇▇▇▇▆\n\n\nstays_in_weekend_nights\n0\n1\n0.93\n0.99\n0.00\n0.0\n1.0\n2\n19\n▇▁▁▁▁\n\n\nstays_in_week_nights\n0\n1\n2.46\n1.92\n0.00\n1.0\n2.0\n3\n50\n▇▁▁▁▁\n\n\nadults\n0\n1\n1.83\n0.51\n0.00\n2.0\n2.0\n2\n4\n▁▂▇▁▁\n\n\nis_repeated_guest\n0\n1\n0.04\n0.20\n0.00\n0.0\n0.0\n0\n1\n▇▁▁▁▁\n\n\nprevious_cancellations\n0\n1\n0.02\n0.27\n0.00\n0.0\n0.0\n0\n13\n▇▁▁▁▁\n\n\nprevious_bookings_not_canceled\n0\n1\n0.20\n1.81\n0.00\n0.0\n0.0\n0\n72\n▇▁▁▁▁\n\n\nbooking_changes\n0\n1\n0.29\n0.74\n0.00\n0.0\n0.0\n0\n21\n▇▁▁▁▁\n\n\ndays_in_waiting_list\n0\n1\n1.59\n14.78\n0.00\n0.0\n0.0\n0\n379\n▇▁▁▁▁\n\n\nadr\n0\n1\n99.99\n49.21\n-6.38\n67.5\n92.5\n125\n510\n▇▆▁▁▁\n\n\ntotal_of_special_requests\n0\n1\n0.71\n0.83\n0.00\n0.0\n1.0\n1\n5\n▇▁▁▁▁\n\n\n\n\n\nAs reservas de Hotel variam de acordo com o mês? há diferenças entre Hotéis Resort e Hotéis de Executivo?\n\nhotel_stays %&gt;%\n  mutate(arrival_date_month = factor(arrival_date_month,\n    levels = month.name\n  )) %&gt;%\n  count(hotel, arrival_date_month, children) %&gt;%\n  group_by(hotel, children) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;%\n  ggplot(aes(arrival_date_month, proportion, fill = children)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  facet_wrap(~hotel, nrow = 2) +\n  labs(\n    x = NULL,\n    y = \"Proporção de reservas\",\n    fill = NULL\n  )\n\n\n\n\n\n\n\n\nVamos explorar algumas outras relações entre as variáveis:\n\nlibrary(GGally)\n\nhotel_stays %&gt;%\n  select(\n    children, adr,\n    required_car_parking_spaces,\n    total_of_special_requests\n  ) %&gt;%\n  ggpairs(mapping = aes(color = children))\n\n\n\n\n\n\n\n\n\n\nModelagem supervisionada usando Tidymodels\nO pacote tidymodels é uma evolução do caret e procura facilitar a construção de modelos de machine learning, seguindo um padrão que independe do modelo a ser construído (regressão linear, árvores de decisão, etc.).\nPara isto, vamos selecionar apenas algumas colunas de interesse e criar um novo objeto denominado hotels_df.\n\nhotels_df &lt;- hotel_stays %&gt;%\n  select(\n    children, hotel, arrival_date_month, meal, adr, adults,\n    required_car_parking_spaces, total_of_special_requests,\n    stays_in_week_nights, stays_in_weekend_nights\n  ) %&gt;%\n  mutate_if(is.character, factor)\n\nVamos dividir o dataset em treino e teste:\n\nlibrary(tidymodels)\n\nset.seed(1234)\nhotel_split &lt;- initial_split(hotels_df, strata=children)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nPara aprimorar a robustez das nossas estimações, iremos usar um procedimento denominado k-fold cross validation ou validação cruzada. Neste procedimento, os dados de treino são subdivididos aleatoriamente em treino e teste e seus parâmetros de acurácia são calculados. Este procedimento é repetido ‘k’ vezes de forma que sejam calculados os parâmetros de acurácia de cada ‘k’. Uma vez que o procedimento é completado (ajustando o modelo a cada ‘k’ subdataset de treino e teste), calcula-se a média de todos os parâmetros de acurácia. Mais detalhes aqui.\n\nhotel_fold &lt;- vfold_cv(hotel_train)\n\nhotel_rec &lt;- recipe(children ~ ., data = hotel_train) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric())\n\nhotel_wf &lt;- workflow() %&gt;% \n  add_recipe(hotel_rec)\n\nVamos treinar um modelo de Regressão Logística (usando glm), logo um de Árvore de Decisão (usando rpart) e finalmente um de Random Forest (usando ranger)\n\nglm_spec &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\")\n\ntree_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\nrf_spec &lt;- rand_forest(trees=1000) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"classification\")\n\nSegue os resultados do modelo de Regressão Logística:\n\ndoParallel::registerDoParallel()\n\nglm_rs &lt;- hotel_wf %&gt;% \n  add_model(glm_spec) %&gt;% \n  fit_resamples(resamples=hotel_fold,\n                metrics=metric_set(roc_auc, accuracy, sensitivity, specificity),\n                control=control_resamples(save_pred=TRUE))\n\ncollect_metrics(glm_rs)\n\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.919     10 0.00121  Preprocessor1_Model1\n2 roc_auc     binary     0.799     10 0.00266  Preprocessor1_Model1\n3 sensitivity binary     0.0896    10 0.00308  Preprocessor1_Model1\n4 specificity binary     0.992     10 0.000439 Preprocessor1_Model1\n\nglm_rs %&gt;% \n  conf_mat_resampled()\n\n# A tibble: 4 × 3\n  Prediction Truth      Freq\n  &lt;fct&gt;      &lt;fct&gt;     &lt;dbl&gt;\n1 children   children   41  \n2 children   none       39.1\n3 none       children  418. \n4 none       none     5140. \n\nglm_rs %&gt;% \n  collect_predictions() %&gt;% \n  group_by(id) %&gt;% \n  roc_curve(children, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\nSegue os resultados do modelo de Árvore de Decisão:\n\ntree_rs &lt;- hotel_wf %&gt;% \n  add_model(tree_spec) %&gt;% \n  fit_resamples(resamples=hotel_fold,\n                metrics=metric_set(roc_auc, accuracy, sensitivity, specificity),\n                control=control_resamples(save_pred=TRUE))\n\ncollect_metrics(tree_rs)\n\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.921     10 0.00126 Preprocessor1_Model1\n2 roc_auc     binary     0.574     10 0.0246  Preprocessor1_Model1\n3 sensitivity binary     0.0481    10 0.0198  Preprocessor1_Model1\n4 specificity binary     0.998     10 0.00109 Preprocessor1_Model1\n\ntree_rs %&gt;% \n  conf_mat_resampled()\n\n# A tibble: 4 × 3\n  Prediction Truth      Freq\n  &lt;fct&gt;      &lt;fct&gt;     &lt;dbl&gt;\n1 children   children   22.2\n2 children   none       10.1\n3 none       children  436. \n4 none       none     5169. \n\ntree_rs %&gt;% \n  collect_predictions() %&gt;% \n  group_by(id) %&gt;% \n  roc_curve(children, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\nSegue os resultados do modelo Random Forest:\n\nrf_rs &lt;- hotel_wf %&gt;% \n  add_model(rf_spec) %&gt;% \n  fit_resamples(resamples=hotel_fold,\n                metrics=metric_set(roc_auc, accuracy, sensitivity, specificity),\n                control=control_resamples(save_pred=TRUE))\n\ncollect_metrics(rf_rs)\n\n# A tibble: 4 × 6\n  .metric     .estimator  mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.928    10 0.000787 Preprocessor1_Model1\n2 roc_auc     binary     0.872    10 0.00199  Preprocessor1_Model1\n3 sensitivity binary     0.152    10 0.00387  Preprocessor1_Model1\n4 specificity binary     0.997    10 0.000342 Preprocessor1_Model1\n\nrf_rs %&gt;% \n  conf_mat_resampled()\n\n# A tibble: 4 × 3\n  Prediction Truth      Freq\n  &lt;fct&gt;      &lt;fct&gt;     &lt;dbl&gt;\n1 children   children   70  \n2 children   none       16.7\n3 none       children  389. \n4 none       none     5162  \n\nrf_rs %&gt;% \n  collect_predictions() %&gt;% \n  group_by(id) %&gt;% \n  roc_curve(children, .pred_children) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\nO melhor modelo é o Random Forest, portanto o usaremos para realizar as estimações finais no dataset de teste. A função last_fit() realiza um último ajuste (fit) usando os dados de treino (dentro da especificação rf_spec) e automaticamente uma última estimação, usando os dados de teste (a função reconhece os dados de treino e teste no objeto hotel_split).\nNote que até agora, não usamos em nenhum momento os dados de teste.\n\nmodelo_final &lt;- hotel_wf %&gt;% \n  add_model(rf_spec) %&gt;% \n  last_fit(hotel_split)\n\ncollect_metrics(modelo_final)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.930 Preprocessor1_Model1\n2 roc_auc  binary         0.868 Preprocessor1_Model1\n\ncollect_predictions(modelo_final) %&gt;% \n  conf_mat(children, .pred_class)\n\n          Truth\nPrediction children  none\n  children      234    62\n  none         1252 17244\n\ncollect_predictions(modelo_final) %&gt;% \n  roc_curve(children, .pred_children) %&gt;% \n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line(size = 1.5, color = \"midnightblue\") +\n  geom_abline(\n    lty = 2, alpha = 0.5,\n    color = \"gray50\",\n    size = 1.2\n  ) +\n  coord_equal()"
  },
  {
    "objectID": "Tutorial3c.html",
    "href": "Tutorial3c.html",
    "title": "Tutorial - Dashboards",
    "section": "",
    "text": "Os dashboards são provavelmente a forma mais flexível de comunicar os resultados de um projeto de Data Science/Business Analytics. Na linguagem R, um dos principais pacotes para a produção de dashboards é o flexdashboard pois facilita a criação e manutenção do dashboard diretamente pelo RStudio.\nSe além disso se quer um Dashboard responsivo, isto é, que responda a controles manipulados pela pessoa que está consumindo o Dashboard, deve-se utilizar um pacote adicional chamado shiny.\nNeste tutorial iremos construir um Dashboard utilizando os dados do IMDB, disponíveis aqui."
  },
  {
    "objectID": "Tutorial3c.html#sec-opções-para-o-sidebar",
    "href": "Tutorial3c.html#sec-opções-para-o-sidebar",
    "title": "Tutorial - Dashboards",
    "section": "Opções para o Sidebar",
    "text": "Opções para o Sidebar\nCriar uma coluna sidebar usando o código: ## Column{data-width=200 .sidebar}\n\n# Menu desplegável\n\n\nselectInput(\"nome\",\n            label = \"Selecione ...\",\n            choices = c(____),\n            selected = ____)\n\n# Slider\n\nsliderInput(\"nome2\",\n            label = \"Escolha ....\",\n            min = \"___\", max = \"____\", value = \"___\", step = '___', \n            dragRange = TRUE)\n\n# Data\n\ndateInput(inputid = \"data1\", \n          label = \"____\", \n          value = \"YYYY-MM-DD\", #informar a data\n          format = \"mm/dd/yy\",\n          language = \"pt\")\n\n# Intervalo de datas - produzirá um objeto com dois valores\n\ndateRangeInput(\"data2\", label = \"______\",\n                 start  = \"_____\", #informar a data de início\n                 end    = \"_____\", #informar a data de finalização\n                 format = \"mm/dd/yy\",\n                 language = \"pt\",\n                 separator = \" - \")"
  },
  {
    "objectID": "Tutorial3c.html#opções-para-o-corpo",
    "href": "Tutorial3c.html#opções-para-o-corpo",
    "title": "Tutorial - Dashboards",
    "section": "Opções para o corpo",
    "text": "Opções para o corpo\n\n# KPIs\n\nrenderValueBox({\n\nvalueBox(prettyNum(______, big.mark = ','), \n         icon = 'fa-ship', caption = \"_____\",\n         color=\"#9b5de5\")\n}\n)\n\n# Requer library(plotly)\n\nrenderPlotly({  \ndf &lt;- _____\nggplotly(df)\n  })\n\n# Gauges - velocímetros\n\nrenderGauge({\n  df &lt;- _____\n  gauge(____, min = ___, max = ___, symbol = '%', gaugeSectors(\n    success = c(____, ___), warning = c(____, ___ ), danger = c(____, ___)\n  ))\n})\n\n\n# Tabelas\n\nrenderTable({  \ndf %&gt;% \n  mutate(____) %&gt;% \n  filter(_____) %&gt;% \n  group_by(____) %&gt;% \n  count() %&gt;% \n  arrange(desc(n)) \n})"
  },
  {
    "objectID": "Tutorial3b.html",
    "href": "Tutorial3b.html",
    "title": "Tutorial 3b - Quarto Dashboards",
    "section": "",
    "text": "Os dashboards são provavelmente a forma mais flexível de comunicar os resultados de um projeto de Data Science/Business Analytics. Na linguagem R, um dos principais pacotes para a produção de dashboards é o flexdashboard pois facilita a criação e manutenção do dashboard diretamente pelo RStudio.\nSe além disso se quer um Dashboard responsivo, isto é, que responda a controles manipulados pela pessoa que está consumindo o Dashboard, deve-se utilizar um pacote adicional chamado shiny.\nO Quarto recentemente lançou um formato dashboard como parte integral dos seus tipos de arquivos. Para isto, basta carregar format = dashboard dentro das opções do yaml conforme figura abaixo.\n\nPara nosso tutorial de esta aula,\n\n\nVocê pode baixar os dados aqui.\n\n  AB_NYC_2019.csv\n  NYC Accidents 2020.csv\n  nyccrimes.csv\n  NYCZipcodes.csv\n  restaurant_week_2018_final.csv"
  },
  {
    "objectID": "Tutorial3b.html#dados-de-ny",
    "href": "Tutorial3b.html#dados-de-ny",
    "title": "Tutorial 3b - Quarto Dashboards",
    "section": "Dados de NY",
    "text": "Dados de NY\nPara facilitar a construção do nosso dashboard, vamos utilizar como exemplo três datasets diferentes sobre a cidade de Nova Iorque (NY). Um deles contém a lista de restaurantes na cidade, o outro contém dados sobre propriedades para alugar via airbnb e a última contém dados sobre crimes ocorridos na cidade. Vamos carregar os dados:\n\ncrime &lt;- read.csv(\"datadash/nyccrimes.csv\", stringsAsFactors=TRUE)\n\nairbnb &lt;- read_csv(\"datadash/AB_NYC_2019.csv\", \n    col_types = cols(last_review = col_date(format = \"%Y-%m-%d\")))\n\nrest &lt;- read_csv(\"datadash/restaurant_week_2018_final.csv\")\n\nPara construir dashboards é muito importante definir uma paleta de cores padrão para ser utilizada uniformemente em todos os gráficos, tabelas e KPIs, para isto vamos criar um objeto com cores previamente selecionadas:\n\nminhascores &lt;- c(\"#9b5de5\",\"#f15bb5\",\"#fee440\",\"#00bbf9\",\"#00f5d4\",\"#264653\",\"#2A9D8F\",\"#E9C46A\",\"#F4A261\",\n                 \"#E76F51\")\n\n\nRestaurantes\nAgora vamos realizar alguns gráficos para o dataset de restaurantes:\n\nrest %&gt;% \n  group_by(restaurant_type) %&gt;% \n  summarize(Media = mean(average_review)) %&gt;%\n  head(10) %&gt;% \n  ggplot(aes(reorder(restaurant_type,Media), \n             Media,fill=restaurant_type))+\n  geom_col()+\n  coord_flip()+\n  scale_fill_manual(values=minhascores)\n\n\n\n\n\n\n\n\n\nrest %&gt;% \n  group_by(restaurant_type, price_range) %&gt;% \n  summarize(Media = mean(average_review)) %&gt;%\n # head(10) %&gt;% \n  ggplot(aes(reorder(restaurant_type,Media), Media, fill=price_range))+\n  geom_col()+\n  coord_flip()+\n  facet_wrap(vars(price_range), scales = \"free\")+\n  scale_fill_manual(values=minhascores)+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nrest %&gt;%  \n  group_by(restaurant_type, price_range) %&gt;% \n  summarize(Media = mean(average_review)) %&gt;%\n  #head(10) %&gt;% \n  ggplot(aes(price_range, Media, fill=price_range))+\n  geom_boxplot()+\n  scale_fill_manual(values=minhascores)+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nAirbnb\nDa mesma forma, vamos criar alguns gráficos para o dataset Airbnb.\n\nboxplot &lt;- airbnb %&gt;% \n    ggplot(aes(neighbourhood_group, price, \n             fill=neighbourhood_group))+\n  geom_boxplot()+\n  scale_fill_manual(values=minhascores)+\n  scale_y_log10()\n\nboxplot\n\n\n\n\n\n\n\n\nPodemos converter o boxplot anterior em interativo, utilizando plotly\n\nggplotly(boxplot)\n\n\n\n\n\n\nairbnb %&gt;% \n  ggplot(aes(last_review, number_of_reviews))+\n  geom_point(color=\"#9b5de5\",\n             alpha=0.5)\n\n\n\n\n\n\n\n\n\ndensidade &lt;- airbnb %&gt;% \n  ggplot(aes(price))+\n  geom_density(color=\"#F4A261\",\n               fill = \"#F4A261\",\n               alpha=0.7)+\n  scale_x_log10()\n\ndensidade\n\n\n\n\n\n\n\n\nPodemos converter o gráfico anterior em interativo utilizando plotly\n\nggplotly(densidade)\n\n\n\n\n\n\n\nCrimes\nAgora, por fim, vamos criar alguns gráficos para visualizar os crimes em NY.\n\ncrime %&gt;%\n  sample_frac(0.01) %&gt;% #para escolher 1% dos dados\n  filter(LAW_CAT_CD==\"FELONY\") %&gt;% \n  filter(BORO_NM==\"MANHATTAN\") %&gt;% \n  leaflet() %&gt;% \n  addTiles() %&gt;% \n  addMarkers()"
  },
  {
    "objectID": "Tutorial3b.html#sec-opções-para-o-sidebar",
    "href": "Tutorial3b.html#sec-opções-para-o-sidebar",
    "title": "Tutorial 3b - Quarto Dashboards",
    "section": "Opções para o Sidebar",
    "text": "Opções para o Sidebar\nCriar uma coluna sidebar usando o código: ## {.sidebar}\n\n# Menu desplegável\n\n\nselectInput(\"nome\",\n            label = \"Selecione ...\",\n            choices = c(____),\n            selected = ____)\n\n# Slider\n\nsliderInput(\"nome2\",\n            label = \"Escolha ....\",\n            min = \"___\", max = \"____\", value = \"___\", step = '___', \n            dragRange = TRUE)\n\n# Data\n\ndateInput(inputid = \"data1\", \n          label = \"____\", \n          value = \"YYYY-MM-DD\", #informar a data\n          format = \"mm/dd/yy\",\n          language = \"pt\")\n\n# Intervalo de datas - produzirá um objeto com dois valores\n\ndateRangeInput(\"data2\", label = \"______\",\n                 start  = \"_____\", #informar a data de início\n                 end    = \"_____\", #informar a data de finalização\n                 format = \"mm/dd/yy\",\n                 language = \"pt\",\n                 separator = \" - \")"
  },
  {
    "objectID": "Tutorial3b.html#opções-para-o-corpo",
    "href": "Tutorial3b.html#opções-para-o-corpo",
    "title": "Tutorial 3b - Quarto Dashboards",
    "section": "Opções para o corpo",
    "text": "Opções para o corpo\n\n# KPIs\n\nrenderValueBox({\n\nvalueBox(prettyNum(______, big.mark = ','), \n         icon = 'fa-ship', caption = \"_____\",\n         color=\"#9b5de5\")\n}\n)\n\n# Requer library(plotly)\n\nrenderPlotly({  \ndf &lt;- _____\nggplotly(df)\n  })\n\n# Gauges - velocímetros\n\nrenderGauge({\n  df &lt;- _____\n  gauge(____, min = ___, max = ___, symbol = '%', gaugeSectors(\n    success = c(____, ___), warning = c(____, ___ ), danger = c(____, ___)\n  ))\n})\n\n\n# Tabelas\n\nrenderTable({  \ndf %&gt;% \n  mutate(____) %&gt;% \n  filter(_____) %&gt;% \n  group_by(____) %&gt;% \n  count() %&gt;% \n  arrange(desc(n)) \n})"
  },
  {
    "objectID": "Tutorial5regtidymodels.html",
    "href": "Tutorial5regtidymodels.html",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "",
    "text": "Você pode baixar os dados do IMDB aqui.\nVamos iniciar carregando os pacotes necessários:\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nimdb &lt;- read_rds(\"https://github.com/curso-r/livro-material/raw/master/assets/data/imdb.rds\")\n\n\n\n\n\n\n\nCode\nstr(imdb)\n\n\ntibble [11,340 × 20] (S3: tbl_df/tbl/data.frame)\n $ id_filme            : chr [1:11340] \"tt0092699\" \"tt0037931\" \"tt0183505\" \"tt0033945\" ...\n $ titulo              : chr [1:11340] \"Broadcast News\" \"Murder, He Says\" \"Me, Myself & Irene\" \"Never Give a Sucker an Even Break\" ...\n $ ano                 : num [1:11340] 1987 1945 2000 1941 2005 ...\n $ data_lancamento     : chr [1:11340] \"1988-04-01\" \"1945-06-23\" \"2000-09-08\" \"1947-05-02\" ...\n $ generos             : chr [1:11340] \"Comedy, Drama, Romance\" \"Comedy, Crime, Mystery\" \"Comedy\" \"Comedy, Musical\" ...\n $ duracao             : num [1:11340] 133 91 116 71 99 87 114 104 95 111 ...\n $ pais                : chr [1:11340] \"USA\" \"USA\" \"USA\" \"USA\" ...\n $ idioma              : chr [1:11340] \"English, Spanish, French, German\" \"English\" \"English, German\" \"English\" ...\n $ orcamento           : num [1:11340] 2.0e+07 NA 5.1e+07 NA NA NA 1.5e+07 3.5e+07 NA NA ...\n $ receita             : num [1:11340] 6.73e+07 NA 1.49e+08 NA 3.09e+05 ...\n $ receita_eua         : num [1:11340] 51249404 NA 90570999 NA 309404 ...\n $ nota_imdb           : num [1:11340] 7.2 7.1 6.6 7.2 5.9 6.1 7.1 5.5 7.1 5.7 ...\n $ num_avaliacoes      : num [1:11340] 26257 1639 219069 2108 2953 ...\n $ direcao             : chr [1:11340] \"James L. Brooks\" \"George Marshall\" \"Bobby Farrelly, Peter Farrelly\" \"Edward F. Cline\" ...\n $ roteiro             : chr [1:11340] \"James L. Brooks\" \"Lou Breslow, Jack Moffitt\" \"Peter Farrelly, Mike Cerrone\" \"John T. Neville, Prescott Chaplin\" ...\n $ producao            : chr [1:11340] \"Amercent Films\" \"Paramount Pictures\" \"Twentieth Century Fox\" \"Universal Pictures\" ...\n $ elenco              : chr [1:11340] \"William Hurt, Albert Brooks, Holly Hunter, Robert Prosky, Lois Chiles, Joan Cusack, Peter Hackes, Christian Cle\"| __truncated__ \"Fred MacMurray, Helen Walker, Marjorie Main, Jean Heather, Porter Hall, Peter Whitney, Mabel Paige, Barbara Pepper\" \"Jim Carrey, Renée Zellweger, Anthony Anderson, Mongo Brownlee, Jerod Mixon, Chris Cooper, Michael Bowman, Richa\"| __truncated__ \"W.C. Fields, Gloria Jean, Leon Errol, Billy Lenhart, Kenneth Brown, Margaret Dumont, Susan Miller, Franklin Pan\"| __truncated__ ...\n $ descricao           : chr [1:11340] \"Take two rival television reporters: one handsome, one talented, both male. Add one Producer, female. Mix well,\"| __truncated__ \"A pollster stumbles on a family of murderous hillbillies, and joins in their search for hidden treasure.\" \"A nice-guy cop with Dissociative Identity Disorder must protect a woman on the run from a corrupt ex-boyfriend \"| __truncated__ \"A filmmaker attempts to sell a surreal script he has written, which comes to life as he pitches it.\" ...\n $ num_criticas_publico: num [1:11340] 142 35 502 35 48 26 125 45 145 52 ...\n $ num_criticas_critica: num [1:11340] 62 10 161 18 15 14 72 74 55 29 ...\n\n\n\n\n\n\nAgora sim podemos criar os datasets de treino e teste:\n\n\n\nCode\nset.seed(2024)\n\nimdb_split &lt;- initial_split(imdb,\n                            prop = 0.75,\n                            strata = nota_imdb)\n\n\nEscolhemos strata=nota_imdb porque queremos que nota_imdb seja nossa variável de saída, ou seja, queremos prever a nota de um determinado filme, com base em certos atributos (iremos escolher o atributos na etapa seguinte).\n\n\n\nAgora que criamos um objeto split , podemos usá-lo para separar nossos dados em treino e teste, utilizando as funções a seguir:\n\n\nCode\ntreino &lt;- training(imdb_split)\nteste &lt;- testing(imdb_split)\n\n\nPodemos verificar que os dados foram divididos conforme solicitado:\n\n\nCode\nnrow(treino)/nrow(imdb)\n\n\n[1] 0.7498236\n\n\nCode\nnrow(teste)/nrow(imdb)\n\n\n[1] 0.2501764"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#resampling-e-baixar-dados",
    "href": "Tutorial5regtidymodels.html#resampling-e-baixar-dados",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "",
    "text": "Você pode baixar os dados do IMDB aqui.\nVamos iniciar carregando os pacotes necessários:\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nimdb &lt;- read_rds(\"https://github.com/curso-r/livro-material/raw/master/assets/data/imdb.rds\")"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#resampling",
    "href": "Tutorial5regtidymodels.html#resampling",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "",
    "text": "Code\nstr(imdb)\n\n\ntibble [11,340 × 20] (S3: tbl_df/tbl/data.frame)\n $ id_filme            : chr [1:11340] \"tt0092699\" \"tt0037931\" \"tt0183505\" \"tt0033945\" ...\n $ titulo              : chr [1:11340] \"Broadcast News\" \"Murder, He Says\" \"Me, Myself & Irene\" \"Never Give a Sucker an Even Break\" ...\n $ ano                 : num [1:11340] 1987 1945 2000 1941 2005 ...\n $ data_lancamento     : chr [1:11340] \"1988-04-01\" \"1945-06-23\" \"2000-09-08\" \"1947-05-02\" ...\n $ generos             : chr [1:11340] \"Comedy, Drama, Romance\" \"Comedy, Crime, Mystery\" \"Comedy\" \"Comedy, Musical\" ...\n $ duracao             : num [1:11340] 133 91 116 71 99 87 114 104 95 111 ...\n $ pais                : chr [1:11340] \"USA\" \"USA\" \"USA\" \"USA\" ...\n $ idioma              : chr [1:11340] \"English, Spanish, French, German\" \"English\" \"English, German\" \"English\" ...\n $ orcamento           : num [1:11340] 2.0e+07 NA 5.1e+07 NA NA NA 1.5e+07 3.5e+07 NA NA ...\n $ receita             : num [1:11340] 6.73e+07 NA 1.49e+08 NA 3.09e+05 ...\n $ receita_eua         : num [1:11340] 51249404 NA 90570999 NA 309404 ...\n $ nota_imdb           : num [1:11340] 7.2 7.1 6.6 7.2 5.9 6.1 7.1 5.5 7.1 5.7 ...\n $ num_avaliacoes      : num [1:11340] 26257 1639 219069 2108 2953 ...\n $ direcao             : chr [1:11340] \"James L. Brooks\" \"George Marshall\" \"Bobby Farrelly, Peter Farrelly\" \"Edward F. Cline\" ...\n $ roteiro             : chr [1:11340] \"James L. Brooks\" \"Lou Breslow, Jack Moffitt\" \"Peter Farrelly, Mike Cerrone\" \"John T. Neville, Prescott Chaplin\" ...\n $ producao            : chr [1:11340] \"Amercent Films\" \"Paramount Pictures\" \"Twentieth Century Fox\" \"Universal Pictures\" ...\n $ elenco              : chr [1:11340] \"William Hurt, Albert Brooks, Holly Hunter, Robert Prosky, Lois Chiles, Joan Cusack, Peter Hackes, Christian Cle\"| __truncated__ \"Fred MacMurray, Helen Walker, Marjorie Main, Jean Heather, Porter Hall, Peter Whitney, Mabel Paige, Barbara Pepper\" \"Jim Carrey, Renée Zellweger, Anthony Anderson, Mongo Brownlee, Jerod Mixon, Chris Cooper, Michael Bowman, Richa\"| __truncated__ \"W.C. Fields, Gloria Jean, Leon Errol, Billy Lenhart, Kenneth Brown, Margaret Dumont, Susan Miller, Franklin Pan\"| __truncated__ ...\n $ descricao           : chr [1:11340] \"Take two rival television reporters: one handsome, one talented, both male. Add one Producer, female. Mix well,\"| __truncated__ \"A pollster stumbles on a family of murderous hillbillies, and joins in their search for hidden treasure.\" \"A nice-guy cop with Dissociative Identity Disorder must protect a woman on the run from a corrupt ex-boyfriend \"| __truncated__ \"A filmmaker attempts to sell a surreal script he has written, which comes to life as he pitches it.\" ...\n $ num_criticas_publico: num [1:11340] 142 35 502 35 48 26 125 45 145 52 ...\n $ num_criticas_critica: num [1:11340] 62 10 161 18 15 14 72 74 55 29 ..."
  },
  {
    "objectID": "Tutorial5regtidymodels.html#resampling-1",
    "href": "Tutorial5regtidymodels.html#resampling-1",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "",
    "text": "Agora sim podemos criar os datasets de treino e teste:\n\n\n\nCode\nset.seed(2024)\n\nimdb_split &lt;- initial_split(imdb,\n                            prop = 0.75,\n                            strata = nota_imdb)\n\n\nEscolhemos strata=nota_imdb porque queremos que nota_imdb seja nossa variável de saída, ou seja, queremos prever a nota de um determinado filme, com base em certos atributos (iremos escolher o atributos na etapa seguinte)."
  },
  {
    "objectID": "Tutorial5regtidymodels.html#resampling-2",
    "href": "Tutorial5regtidymodels.html#resampling-2",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "",
    "text": "Agora que criamos um objeto split , podemos usá-lo para separar nossos dados em treino e teste, utilizando as funções a seguir:\n\n\nCode\ntreino &lt;- training(imdb_split)\nteste &lt;- testing(imdb_split)\n\n\nPodemos verificar que os dados foram divididos conforme solicitado:\n\n\nCode\nnrow(treino)/nrow(imdb)\n\n\n[1] 0.7498236\n\n\nCode\nnrow(teste)/nrow(imdb)\n\n\n[1] 0.2501764"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#o-pacote-parsnip",
    "href": "Tutorial5regtidymodels.html#o-pacote-parsnip",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "O pacote parsnip",
    "text": "O pacote parsnip\n\n\nVamos construir um modelo simples de regressão linear, onde a nota_imdb depende de:\n\nnum_avaliacoes,\nnum_criticas_publico e\nnum_criticas_critica.\n\n\n\nO código é:\n\n\n\nCode\nlm_model &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n\n\nPara treinar o modelo, usaremos fit:\n\n\n\nCode\nformula &lt;- nota_imdb ~ num_avaliacoes + \n        num_criticas_publico +\n        num_criticas_critica\n\nlm_fit &lt;- lm_model %&gt;% \n  fit(formula,\n      data=treino)"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#resultados-do-modelo",
    "href": "Tutorial5regtidymodels.html#resultados-do-modelo",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Resultados do modelo",
    "text": "Resultados do modelo\nO modelo treinado lm_fit pode ser observado mais de perto utilizando tidy:\n\n\nCode\ntidy(lm_fit)\n\n\n# A tibble: 4 × 5\n  term                    estimate   std.error statistic  p.value\n  &lt;chr&gt;                      &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)           5.93       0.0150         396.   0       \n2 num_avaliacoes        0.00000274 0.000000173     15.9  3.82e-56\n3 num_criticas_publico -0.000220   0.0000635       -3.46 5.34e- 4\n4 num_criticas_critica  0.00155    0.000189         8.20 2.84e-16"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#avaliando-a-performance-do-modelo",
    "href": "Tutorial5regtidymodels.html#avaliando-a-performance-do-modelo",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Avaliando a performance do modelo",
    "text": "Avaliando a performance do modelo\nPara avaliar a performance do modelo, usamos last_fit no objeto lm_model e calculamos os indicadores de performance utilizando collect_metrics:\n\n\nCode\nlm_last &lt;- lm_model %&gt;% \n  last_fit(formula, \n           split = imdb_split)\n\nlm_last %&gt;% \n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard      1.05   Preprocessor1_Model1\n2 rsq     standard      0.0782 Preprocessor1_Model1"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#utilizando-o-pacote-recipes",
    "href": "Tutorial5regtidymodels.html#utilizando-o-pacote-recipes",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Utilizando o pacote recipes",
    "text": "Utilizando o pacote recipes\n\nVamos aplicar algumas transformações utilizando step\nPrimeiro, vamos aplicar step_log para num_avaliacoes\n\n\n\nCode\nlm_model_rec &lt;- recipe(formula,\n                       data=treino) %&gt;% \n  step_log(num_avaliacoes, base = 10)\n\n\n\nagora, vamos imputar valores para num_criticas_publico e num_criticas_critica:\n\n\n\nCode\nlm_model_rec &lt;- recipe(formula,\n                       data=treino) %&gt;% \n  step_log(num_avaliacoes, base = 10) %&gt;% \n  step_impute_knn(all_predictors())"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#treinando-o-objeto-recipe",
    "href": "Tutorial5regtidymodels.html#treinando-o-objeto-recipe",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Treinando o objeto recipe",
    "text": "Treinando o objeto recipe\n\nPara treinar o objeto lm_model_rec vamos utilizar prep()\n\n\n\nCode\nlm_model_rec_prep &lt;- lm_model_rec %&gt;% \n  prep(training = treino)\n\n\n\ne finalmente vamos produzir o novo dataset de treino com as transformações que fizemos.\nvamos utilizar new_data=NULL para produzir o dataset de treino\n\n\n\nCode\ntreino_prep &lt;- lm_model_rec_prep %&gt;% \n  bake(new_data = NULL)"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#preparando-os-dados",
    "href": "Tutorial5regtidymodels.html#preparando-os-dados",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Preparando os dados",
    "text": "Preparando os dados\nOs dados de treino pré-processados encontram-se no dataset treino_prep\n\n\nCode\ntreino_prep %&gt;% head(5)\n\n\n# A tibble: 5 × 4\n  num_avaliacoes num_criticas_publico num_criticas_critica nota_imdb\n           &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1           3.09                   56                   31       4.7\n2           3.22                   32                   46       4.8\n3           3.50                   28                   12       5.4\n4           3.84                   50                   26       4.4\n5           3.52                  111                    3       3.5\n\n\n\nassim também, precisaremos preparar os dados de teste, no dataset teste_prep"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#outras-transformações-step_corr",
    "href": "Tutorial5regtidymodels.html#outras-transformações-step_corr",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Outras transformações: step_corr",
    "text": "Outras transformações: step_corr\n\nExistem muitas outras transformações com uso de step_ que podem ser de utilidade dependendo do contexto dos dados.\nQuando duas variáveis estão muito correlacionadas, p.ex., o modelo de ML pode sofrer de multicolinearidade, utiliza-se:\n\n\n\nCode\nlm_model_rec &lt;- recipe(formula,\n                       data=treino) %&gt;%\n  step_log(num_avaliacoes, base = 10) %&gt;% \n  step_impute_knn(all_predictors()) %&gt;% \n  step_corr(all_numeric(), threshold = 0.85)\n\n\n\nAqui foi escolhido um threshold = 0.85 mas poderia ter sido escolhido outro valor, p.ex. 0.9"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#outras-transformações-step_normalize",
    "href": "Tutorial5regtidymodels.html#outras-transformações-step_normalize",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Outras transformações: step_normalize",
    "text": "Outras transformações: step_normalize\n\nTambém é possível normalizar colunas (processo similar ao utilizado no kmeans\nPara isso, cada valor da coluna e subtraido pela média e dividido pelo desvio padrão.\nÉ recomendado em todas as variáveis numéricas, utilizando step_normalize(all_numeric)\n\n\n\nCode\nlm_model_rec &lt;- recipe(formula,\n                       data=treino) %&gt;%\n  step_log(num_avaliacoes, base = 10) %&gt;% \n  step_impute_knn(all_predictors()) %&gt;% \n  step_corr(all_numeric(), threshold = 0.85) %&gt;% \n  step_normalize(all_numeric(), -all_outcomes())"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#outras-transformações-step_dummy",
    "href": "Tutorial5regtidymodels.html#outras-transformações-step_dummy",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Outras transformações: step_dummy",
    "text": "Outras transformações: step_dummy\n\nCom relação a variáveis do tipo factor é recomendável convertê-las a dummies.\nPara selecionar todas as colunas categóricas automaticamente, utilizamos all_nominal() como argumento do step_dummy\n\n\n\nCode\nlm_model_rec &lt;- recipe(formula,\n                       data=treino) %&gt;%\n  step_log(num_avaliacoes, base = 10) %&gt;% \n  step_impute_knn(all_predictors()) %&gt;% \n  step_corr(all_numeric(), threshold = 0.85) %&gt;% \n  step_normalize(all_numeric(), -all_outcomes()) %&gt;% \n  step_dummy(all_nominal())"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#preparando-os-novos-datasets",
    "href": "Tutorial5regtidymodels.html#preparando-os-novos-datasets",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Preparando os novos datasets",
    "text": "Preparando os novos datasets\n\nUma vez que todas as etapas de pré-processamento foram definidas, deve-se criar novos datasets de treino e teste.\nUtiliza-se a função prep() para preparar a recipe e bake() para criar os datasets\n\n\n\nCode\nlm_model_prep &lt;- lm_model_rec %&gt;% \n  prep(training=treino)\n\ntreino_prep &lt;- lm_model_prep %&gt;% \n  bake(new_data = NULL)\n\nteste_prep &lt;- lm_model_prep %&gt;% \n  bake(new_data = teste)"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#model-fitting-1",
    "href": "Tutorial5regtidymodels.html#model-fitting-1",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nVamos treinar novamente um modelo de regressão linear, só que agora com os datasets pré-processados.\n\n\n\nCode\nlm_fit_prep &lt;- lm_model %&gt;% \n  fit(formula,\n      data=treino_prep)\n\n\n\nE printar os resultados\n\n\n\nCode\nlm_fit_prep\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = nota_imdb ~ num_avaliacoes + num_criticas_publico + \n    num_criticas_critica, data = data)\n\nCoefficients:\n         (Intercept)        num_avaliacoes  num_criticas_publico  \n             6.10355               0.41911               0.04434  \nnum_criticas_critica  \n            -0.05613"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#model-fitting-2",
    "href": "Tutorial5regtidymodels.html#model-fitting-2",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nPara realizar as previsões podemos utilizar a função predict().\n\n\n\nCode\nprevisao &lt;- predict(lm_fit_prep,\n                    new_data = teste_prep)\n\nresultados &lt;- teste_prep %&gt;% \n  select(nota_imdb) %&gt;% \n  bind_cols(previsao)\n\n\n\nE calcular a performance dos modelos\n\n\n\nCode\nresultados %&gt;% \n  rsq(nota_imdb, estimate = .pred)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.118"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#plotando-os-resultados",
    "href": "Tutorial5regtidymodels.html#plotando-os-resultados",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Plotando os resultados",
    "text": "Plotando os resultados\n\n\nÉ possível fazer um gráfico mostrando a qualidade do ajuste entre a variável de saída real e a previsão.\n\n\nCode\nresultados %&gt;% \n  ggplot(aes(nota_imdb, .pred))+\n  geom_point()+\n  geom_abline(color=\"lightblue\", linetype =2)+\n  coord_obs_pred()"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#utilizando-workflows",
    "href": "Tutorial5regtidymodels.html#utilizando-workflows",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Utilizando workflows",
    "text": "Utilizando workflows\n\nÉ possível agilizar o processo de treinamento com a função workflow()\n\n\n\nCode\nwkfl &lt;- workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_recipe(lm_model_rec)\n\n\n\nCom este objeto workflows treina-se novamente o modelo com last_fit() e avalia-se novamente o modelo com collect_metrics()"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#utilizando-workflows-1",
    "href": "Tutorial5regtidymodels.html#utilizando-workflows-1",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Utilizando workflows",
    "text": "Utilizando workflows\n\n\nCode\nlm_fit_wkfl &lt;- wkfl %&gt;% \n  last_fit(split=imdb_split)\n\nlm_fit_wkfl %&gt;% \n  collect_metrics()\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       1.03  Preprocessor1_Model1\n2 rsq     standard       0.118 Preprocessor1_Model1"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#criar-o-objeto-de-validação-cruzada",
    "href": "Tutorial5regtidymodels.html#criar-o-objeto-de-validação-cruzada",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Criar o objeto de validação cruzada",
    "text": "Criar o objeto de validação cruzada\n\nO primeiro passo é criar um objeto que será o encarregado de fazer os splits sequencialmente, utilizando vfold_cv()\n\n\n\nCode\nset.seed(9988)\n\nfolds &lt;- vfold_cv(treino,\n                  v=10, strata = nota_imdb)\n\nfolds\n\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits             id    \n   &lt;list&gt;             &lt;chr&gt; \n 1 &lt;split [7651/852]&gt; Fold01\n 2 &lt;split [7651/852]&gt; Fold02\n 3 &lt;split [7652/851]&gt; Fold03\n 4 &lt;split [7653/850]&gt; Fold04\n 5 &lt;split [7653/850]&gt; Fold05\n 6 &lt;split [7653/850]&gt; Fold06\n 7 &lt;split [7653/850]&gt; Fold07\n 8 &lt;split [7653/850]&gt; Fold08\n 9 &lt;split [7653/850]&gt; Fold09\n10 &lt;split [7655/848]&gt; Fold10"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#model-fitting-com-validação-cruzada",
    "href": "Tutorial5regtidymodels.html#model-fitting-com-validação-cruzada",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Model Fitting com validação cruzada",
    "text": "Model Fitting com validação cruzada\n\nPode-se aproveitar o workflow (que inclui o modelo parsnip e a recipes ).\nMas deve-se utilizar fit_resamples ao inves de fit\n\n\n\nCode\nlm_fit_vc &lt;- wkfl %&gt;% \n  fit_resamples(resamples = folds)\n\nlm_fit_vc %&gt;% \n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   1.07     10 0.00488 Preprocessor1_Model1\n2 rsq     standard   0.127    10 0.00954 Preprocessor1_Model1"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#model-fitting-com-resultados-detalhados",
    "href": "Tutorial5regtidymodels.html#model-fitting-com-resultados-detalhados",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Model Fitting com resultados detalhados",
    "text": "Model Fitting com resultados detalhados\n\nSe summarize = FALSE dentro de collect_metrics(), então teremos um detalhamento das métricas por cada fold.\n\n\n\nCode\nlm_fit_vc %&gt;% \n  collect_metrics(summarize = FALSE)\n\n\n# A tibble: 20 × 5\n   id     .metric .estimator .estimate .config             \n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n 1 Fold01 rmse    standard      1.05   Preprocessor1_Model1\n 2 Fold01 rsq     standard      0.134  Preprocessor1_Model1\n 3 Fold02 rmse    standard      1.05   Preprocessor1_Model1\n 4 Fold02 rsq     standard      0.136  Preprocessor1_Model1\n 5 Fold03 rmse    standard      1.08   Preprocessor1_Model1\n 6 Fold03 rsq     standard      0.183  Preprocessor1_Model1\n 7 Fold04 rmse    standard      1.08   Preprocessor1_Model1\n 8 Fold04 rsq     standard      0.104  Preprocessor1_Model1\n 9 Fold05 rmse    standard      1.06   Preprocessor1_Model1\n10 Fold05 rsq     standard      0.0965 Preprocessor1_Model1\n11 Fold06 rmse    standard      1.06   Preprocessor1_Model1\n12 Fold06 rsq     standard      0.104  Preprocessor1_Model1\n13 Fold07 rmse    standard      1.06   Preprocessor1_Model1\n14 Fold07 rsq     standard      0.159  Preprocessor1_Model1\n15 Fold08 rmse    standard      1.09   Preprocessor1_Model1\n16 Fold08 rsq     standard      0.0853 Preprocessor1_Model1\n17 Fold09 rmse    standard      1.04   Preprocessor1_Model1\n18 Fold09 rsq     standard      0.133  Preprocessor1_Model1\n19 Fold10 rmse    standard      1.08   Preprocessor1_Model1\n20 Fold10 rsq     standard      0.138  Preprocessor1_Model1"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#comparação",
    "href": "Tutorial5regtidymodels.html#comparação",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Comparação",
    "text": "Comparação\n\nUma vez identificado todo o processo de treinamento e avaliação de modelo utilizando a regressão linear, podemos utilizar a mesma lógica de parsnip, recipes e workflows em outros tipos de modelos.\nPara treinar árvores de decisão utilizamos decision_tree() com set_engine(\"rpart\") e set_mode(\"regression\")\nPara treinar random forests utilizamos rand_forest() com set_engine(\"ranger\") e set_mode(\"regression\")"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#comparação-1",
    "href": "Tutorial5regtidymodels.html#comparação-1",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Comparação",
    "text": "Comparação\n\nPara treinar k-nearest neighbors utilizamos nearest_neighbor() com set_engine(\"knn\") e set_mode(\"regression\")\nPara treinar modelos de regressão lasso utilizamos linear_reg(penalty=0.1, mixture=1) e set_engine(\"glmnet\")\nPara treinar modelos de regressão ridge utilizamos linear_reg(penalty=0.1, mixture=0) e set_engine(\"glmnet\")"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#comparação-2",
    "href": "Tutorial5regtidymodels.html#comparação-2",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Comparação",
    "text": "Comparação\n\nPara treinar modelos de regressão elastic net utilizamos linear_reg(penalty=0.1, mixture=0.5) e set_engine(\"glmnet\")\nE varios outros aqui"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#resampling-3",
    "href": "Tutorial5regtidymodels.html#resampling-3",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Resampling",
    "text": "Resampling\n\nVamos escolher uma variável categórica para ser a nossa saída.\no primeiro ‘level’ deve ser a classe positiva\n\n\n\nCode\nset.seed(202401)\n\nimdb_class &lt;- imdb %&gt;% \n  mutate(Lucro = receita - orcamento,\n         Lucro_fac = factor(ifelse(Lucro&gt;0, 'yes' ,'no')))\n\nimdb_class$Lucro_fac &lt;- relevel(imdb_class$Lucro_fac, ref = \"yes\")\n\nlevels(imdb_class$Lucro_fac)\n\n\n[1] \"yes\" \"no\""
  },
  {
    "objectID": "Tutorial5regtidymodels.html#resampling-4",
    "href": "Tutorial5regtidymodels.html#resampling-4",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Resampling",
    "text": "Resampling\n\n\nCode\nimdb_split &lt;- initial_split(imdb_class,\n                            prop = 0.75,\n                            strata = Lucro_fac)\n\ntreino &lt;- training(imdb_split)\ntesting &lt;- testing(imdb_split)\n\n\nFazemos um novo split, agora utilizando a variável categórica"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#model-fitting-3",
    "href": "Tutorial5regtidymodels.html#model-fitting-3",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nVamos utilizar um modelo de regressão logística para fazer o treinamento.\n\n\n\nCode\nlogistic_model &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\")\n\nlogistic_fit &lt;- logistic_model %&gt;% \n  fit(Lucro_fac ~ duracao + nota_imdb + num_avaliacoes,\n      data = treino)\n\nclass_preds &lt;- logistic_fit %&gt;% \n  predict(new_data = teste,\n          type = \"class\")\n\nclass_preds\n\n\n# A tibble: 2,837 × 1\n   .pred_class\n   &lt;fct&gt;      \n 1 no         \n 2 no         \n 3 yes        \n 4 no         \n 5 no         \n 6 no         \n 7 no         \n 8 no         \n 9 no         \n10 no         \n# ℹ 2,827 more rows"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#workflows",
    "href": "Tutorial5regtidymodels.html#workflows",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Workflows",
    "text": "Workflows\n\nPodemos criar um workflow para facilitar a construção do modelo de treinamento.\nSimilarmente ao caso da regressão, podemos reaproveitar vários dos step_ que aprendemos antes.\n\n\n\nCode\nformula &lt;- Lucro_fac ~ duracao + nota_imdb + num_avaliacoes\n\nlogistic_rec &lt;- recipe(formula,\n                       data=treino) %&gt;%\n  step_log(num_avaliacoes, base = 10) %&gt;% \n  step_impute_knn(all_predictors()) %&gt;% \n  step_corr(all_numeric(), threshold = 0.85) %&gt;% \n  step_normalize(all_numeric()) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes())"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#workflows-1",
    "href": "Tutorial5regtidymodels.html#workflows-1",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Workflows",
    "text": "Workflows\n\nVamos incluir a especificação do modelo e a recipe criadas anteriormente\n\n\n\nCode\nwkfl &lt;- workflow() %&gt;% \n  add_model(logistic_model) %&gt;% \n  add_recipe(logistic_rec)\n\n\n\nE utilizar um procedimento de validação cruzada\n\n\n\nCode\nset.seed(2233)\n\nfolds &lt;- vfold_cv(treino,\n                  v=10, strata = Lucro_fac)"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#validação-cruzada-1",
    "href": "Tutorial5regtidymodels.html#validação-cruzada-1",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Validação Cruzada",
    "text": "Validação Cruzada\n\nVamos rodar o modelo treinando-o com v=10 folds\n\n\n\nCode\nmetrics_custom &lt;- metric_set(accuracy, roc_auc, sensitivity, specificity)\n\nlogistic_fit_vc &lt;- wkfl %&gt;% \n  fit_resamples(resamples = folds,\n                metrics = metrics_custom,\n                control = control_resamples(save_pred = TRUE))\n\nlogistic_fit_vc %&gt;% \n  collect_metrics()\n\n\n# A tibble: 4 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.738    10 0.00680 Preprocessor1_Model1\n2 roc_auc     binary     0.815    10 0.00505 Preprocessor1_Model1\n3 sensitivity binary     0.806    10 0.00664 Preprocessor1_Model1\n4 specificity binary     0.642    10 0.0151  Preprocessor1_Model1"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#plotando-os-resultados-1",
    "href": "Tutorial5regtidymodels.html#plotando-os-resultados-1",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Plotando os resultados",
    "text": "Plotando os resultados\n\nVamos ver como ficaram as previsões\n\n\n\nCode\nlogistic_fit_vc %&gt;% \n  collect_predictions()\n\n\n# A tibble: 8,504 × 7\n   .pred_class .pred_yes .pred_no id      .row Lucro_fac .config             \n   &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt; &lt;fct&gt;     &lt;chr&gt;               \n 1 no             0.383     0.617 Fold01    21 &lt;NA&gt;      Preprocessor1_Model1\n 2 no             0.0885    0.911 Fold01    27 &lt;NA&gt;      Preprocessor1_Model1\n 3 no             0.0901    0.910 Fold01    35 no        Preprocessor1_Model1\n 4 no             0.113     0.887 Fold01    48 &lt;NA&gt;      Preprocessor1_Model1\n 5 no             0.120     0.880 Fold01    60 &lt;NA&gt;      Preprocessor1_Model1\n 6 no             0.164     0.836 Fold01    77 &lt;NA&gt;      Preprocessor1_Model1\n 7 no             0.288     0.712 Fold01    95 &lt;NA&gt;      Preprocessor1_Model1\n 8 yes            0.703     0.297 Fold01    96 no        Preprocessor1_Model1\n 9 yes            0.699     0.301 Fold01    97 no        Preprocessor1_Model1\n10 no             0.107     0.893 Fold01   111 &lt;NA&gt;      Preprocessor1_Model1\n# ℹ 8,494 more rows"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#plotando-os-resultados-2",
    "href": "Tutorial5regtidymodels.html#plotando-os-resultados-2",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Plotando os resultados",
    "text": "Plotando os resultados\n\nVamos fazer um gráfico da curva ROC\n\n\n\nCode\nlogistic_fit_vc %&gt;% \n  collect_predictions() %&gt;% \n  group_by(id) %&gt;% \n  roc_curve(Lucro_fac, .pred_yes) %&gt;% \n  autoplot()"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#last_fit",
    "href": "Tutorial5regtidymodels.html#last_fit",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Last_fit",
    "text": "Last_fit\nPor fim vamos rodar novamente o modelo com os dados de teste utilizando last_fit\n\n\nCode\nfinal &lt;- wkfl %&gt;% \n  last_fit(imdb_split)\n\nresultados &lt;- final %&gt;% \n  collect_predictions()\n\nmetrics_custom(resultados,\n               truth = Lucro_fac,\n               estimate = .pred_class, .pred_yes)\n\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.720\n2 sensitivity binary         0.800\n3 specificity binary         0.602\n4 roc_auc     binary         0.789"
  },
  {
    "objectID": "Tutorial5regtidymodels.html#plotando-os-resultados-3",
    "href": "Tutorial5regtidymodels.html#plotando-os-resultados-3",
    "title": "Tutorial 5 - Métodos de Regressão e Classificação em R",
    "section": "Plotando os resultados",
    "text": "Plotando os resultados\n\n\n\nO gráfico ROC final:\n\n\n\nCode\nresultados %&gt;% \n  roc_curve(Lucro_fac, .pred_yes) %&gt;% \n  autoplot()"
  },
  {
    "objectID": "pyTutorial0.html",
    "href": "pyTutorial0.html",
    "title": "Datasets - 2025.1",
    "section": "",
    "text": "Os datasets para este semestre encontram-se disponíveis no Moodle e também aqui:\n\nimport pandas as pd\n\nPara ler os arquivos diretamente da nuvem (sem necessidade de fazer download):\n\ndf = pd.read_csv('https://eps7008ufsc.netlify.app/2025-1//G06_ufo3.csv')\n\nvamos mostrar as primeiras 7 linhas:\n\ndf.head(7)"
  },
  {
    "objectID": "Tutorial3.html",
    "href": "Tutorial3.html",
    "title": "Tutorial 3 - Dashboards",
    "section": "",
    "text": "Os dashboards são provavelmente a forma mais flexível de comunicar os resultados de um projeto de Data Science/Business Analytics. Na linguagem R, um dos principais pacotes para a produção de dashboards é o flexdashboard pois facilita a criação e manutenção do dashboard diretamente pelo RStudio.\nSe além disso se quer um Dashboard responsivo, isto é, que responda a controles manipulados pela pessoa que está consumindo o Dashboard, deve-se utilizar um pacote adicional chamado shiny.\nNeste tutorial iremos construir um Dashboard utilizando os dados do Titanic.\n\n\nVocê pode baixar os dados e o dashboard aqui."
  },
  {
    "objectID": "Tutorial3.html#boxplot",
    "href": "Tutorial3.html#boxplot",
    "title": "Tutorial 3 - Dashboards",
    "section": "Boxplot",
    "text": "Boxplot\nO primeiro será a distribuição dos passageiros por idade, sexo e se sobreviveu ou não. Para isto vamos escolher dois filtros que logo serão substituidos no código do shiny, os filtros são um para o gênero (que neste caso está incluindo tanto masculino quanto feminino) e o outro para a idade (que neste caso está incluindo todos os passageiros pois a idade do passageiro mais idoso era 80). Adicionalmente vamos incluir uma linha de código para aplicar a nossa paleta de cores personalizada, usando scale_fill_manual(values=minhascores).\n\ntitanic$Survived &lt;- factor(titanic$Survived, levels = c(0,1),\n                           labels=c(\"Óbitos\",\"Sobrevivientes\"))\n\nb1 &lt;- titanic %&gt;%\n      filter(Sex %in% c(\"male\",\"female\")) %&gt;% \n      filter(Age &lt; 81) %&gt;% \n    ggplot(aes(Survived, Age, fill = Survived))+\n    geom_boxplot()+\n    scale_fill_manual(values=minhascores)+\n    theme(axis.title.x = element_blank(),\n          legend.position = \"none\")\nb1\n\n\n\n\n\n\n\n\nAdicionalmente podemos usar um facet_wrap() para vermos as categorias de sobrevivência por gênero, por exemplo.\n\nb1 &lt;- titanic %&gt;%\n      filter(Sex %in% c(\"male\",\"female\")) %&gt;% \n      filter(Age &lt; 81) %&gt;% \n    ggplot(aes(Survived, Age, fill = Survived))+\n    geom_boxplot()+\n    scale_fill_manual(values=minhascores)+\n    facet_wrap(vars(Sex))+\n    theme(axis.title.x = element_blank(),\n          legend.position = \"none\")\nb1"
  },
  {
    "objectID": "Tutorial3.html#densidade",
    "href": "Tutorial3.html#densidade",
    "title": "Tutorial 3 - Dashboards",
    "section": "Densidade",
    "text": "Densidade\nTambém podemos realizar um gráfico mostrando a densidade, usando a camada geom_density() dentro do ggplot().\n\nb2 &lt;- titanic %&gt;% \n    filter(Sex %in% c(\"male\",\"female\")) %&gt;% \n    filter(Age &lt; 81) %&gt;%\n    ggplot(aes(Age, fill=Survived))+\n    geom_density(alpha=0.4)+\n    scale_fill_manual(values=minhascores)\n\nb2\n\n\n\n\n\n\n\n\nSe queremos o mesmo gráfico mas agora divido por gênero, podemos adicionar uma camada facet_wrap().\n\nb2 &lt;- titanic %&gt;% \n    filter(Sex %in% c(\"male\",\"female\")) %&gt;% \n    filter(Age &lt; 81) %&gt;%\n    ggplot(aes(Age, fill=Survived))+\n    geom_density(alpha=0.4)+\n  facet_wrap(vars(Sex))+\n    scale_fill_manual(values=minhascores)\n\nb2"
  },
  {
    "objectID": "Tutorial3.html#histograma",
    "href": "Tutorial3.html#histograma",
    "title": "Tutorial 3 - Dashboards",
    "section": "Histograma",
    "text": "Histograma\nPor fim, vamos incluir um histograma que demonstre a distribuição dos passageiros. Para o histograma, podemos definir também quantos intervalos queremos, por exemplo, 20\n\nb3 &lt;- titanic %&gt;% \n    filter(Sex %in% c(\"male\",\"female\")) %&gt;% \n     filter(Age &lt; 81) %&gt;%\n    ggplot(aes(Age, fill=Survived))+\n    geom_histogram(alpha=0.9, bins = 20)+\n    scale_fill_manual(values=minhascores)\n\nb3\n\n\n\n\n\n\n\n\nDa mesma forma que nos anteriores gráficos, se quissesemos ver a distribuição por gênero, precisariamos de um facet_wrap():\n\nb3 &lt;- titanic %&gt;% \n    filter(Sex %in% c(\"male\",\"female\")) %&gt;% \n     filter(Age &lt; 81) %&gt;%\n    ggplot(aes(Age, fill=Survived))+\n    geom_histogram(alpha=0.9, bins = 20)+\n    facet_wrap(vars(Sex))+\n    scale_fill_manual(values=minhascores)\n\nb3"
  },
  {
    "objectID": "Tutorial2.html",
    "href": "Tutorial2.html",
    "title": "Tutorial 2 - Ggplot2 e Tidyverse",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl) # para fazer leitura de arquivos excel\nlibrary(knitr) # para formatar tabelas"
  },
  {
    "objectID": "Tutorial2.html#exemplo-base",
    "href": "Tutorial2.html#exemplo-base",
    "title": "Tutorial 2 - Ggplot2 e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nPara realizar gráficos de dispersão, precisamos passar a função geom_point() como uma nova camada do ggplot. As coordenadas x e y devem ser numéricas necessariamente. Vamos usar como exemplo, o dataframe imdb_filmes.\nNo exemplo, a posição do ponto no eixo x pode ser dada pela coluna orcamento e a posição do ponto no eixo y pela coluna receita.\n\nimdb_filmes %&gt;% \n  ggplot()"
  },
  {
    "objectID": "Tutorial2.html#adicionando-cores",
    "href": "Tutorial2.html#adicionando-cores",
    "title": "Tutorial 2 - Ggplot2 e Tidyverse",
    "section": "Adicionando cores",
    "text": "Adicionando cores\nPodemos adicionar cores grupalmente para todos os pontos ou podemos usar alguma outra variável para criar cores em gradiente\nPara ver mais detalhamento da função geom_point() recomenda-se a leitura do Capítulo 8 do Livro “Curso-R”."
  },
  {
    "objectID": "Tutorial2.html#exemplo-base-1",
    "href": "Tutorial2.html#exemplo-base-1",
    "title": "Tutorial 2 - Ggplot2 e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nPara realizar gráficos de barras usamos geom_col(). Vamos usar o dataframe imdb_filmes para exemplificar, ordenando as linhas por ordem decrescente de UserRating. Primeiro vamos gerar o gráfico com as configuração padrão.\n\nimdb_filmes %&gt;% \n  arrange(desc(nota_imdb)) %&gt;% \n  head(10)  # adicionar código do ggplot\n\n# A tibble: 10 × 20\n   id_filme  titulo   ano data_lancamento generos duracao pais  idioma orcamento\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 tt102189… As I …  2019 2019-12-06      Drama,…      62 USA   Engli…     10000\n 2 tt6735740 Love …  2019 2019-06-23      Comedy      100 USA   Engli…   3000000\n 3 tt0111161 The S…  1994 1995-02-10      Drama       142 USA   Engli…  25000000\n 4 tt0068646 The G…  1972 1972-09-21      Crime,…     175 USA   Engli…   6000000\n 5 tt5980638 The T…  2018 2020-06-19      Music,…      96 USA   Engli…     90000\n 6 tt0071562 The G…  1974 1975-09-25      Crime,…     202 USA   Engli…  13000000\n 7 tt0050083 12 An…  1957 1957-09-04      Crime,…      96 USA   Engli…    350000\n 8 tt0110912 Pulp …  1994 1994-10-28      Crime,…     154 USA   Engli…   8000000\n 9 tt0108052 Schin…  1993 1994-03-11      Biogra…     195 USA   Engli…  22000000\n10 tt0419781 Grave…  2005 2005-04-22      Thrill…      90 USA   Engli…   1930000\n# ℹ 11 more variables: receita &lt;dbl&gt;, receita_eua &lt;dbl&gt;, nota_imdb &lt;dbl&gt;,\n#   num_avaliacoes &lt;dbl&gt;, direcao &lt;chr&gt;, roteiro &lt;chr&gt;, producao &lt;chr&gt;,\n#   elenco &lt;chr&gt;, descricao &lt;chr&gt;, num_criticas_publico &lt;dbl&gt;,\n#   num_criticas_critica &lt;dbl&gt;"
  },
  {
    "objectID": "Tutorial2.html#mudando-os-eixos-e-adicionando-cores",
    "href": "Tutorial2.html#mudando-os-eixos-e-adicionando-cores",
    "title": "Tutorial 2 - Ggplot2 e Tidyverse",
    "section": "Mudando os eixos e adicionando cores",
    "text": "Mudando os eixos e adicionando cores\nPodemos mudar os eixos (trocar de eixo) usando a função coord_flip()\n\nimdb_filmes %&gt;% \n  arrange(desc(nota_imdb)) %&gt;% \n  head(10)\n\n# A tibble: 10 × 20\n   id_filme  titulo   ano data_lancamento generos duracao pais  idioma orcamento\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 tt102189… As I …  2019 2019-12-06      Drama,…      62 USA   Engli…     10000\n 2 tt6735740 Love …  2019 2019-06-23      Comedy      100 USA   Engli…   3000000\n 3 tt0111161 The S…  1994 1995-02-10      Drama       142 USA   Engli…  25000000\n 4 tt0068646 The G…  1972 1972-09-21      Crime,…     175 USA   Engli…   6000000\n 5 tt5980638 The T…  2018 2020-06-19      Music,…      96 USA   Engli…     90000\n 6 tt0071562 The G…  1974 1975-09-25      Crime,…     202 USA   Engli…  13000000\n 7 tt0050083 12 An…  1957 1957-09-04      Crime,…      96 USA   Engli…    350000\n 8 tt0110912 Pulp …  1994 1994-10-28      Crime,…     154 USA   Engli…   8000000\n 9 tt0108052 Schin…  1993 1994-03-11      Biogra…     195 USA   Engli…  22000000\n10 tt0419781 Grave…  2005 2005-04-22      Thrill…      90 USA   Engli…   1930000\n# ℹ 11 more variables: receita &lt;dbl&gt;, receita_eua &lt;dbl&gt;, nota_imdb &lt;dbl&gt;,\n#   num_avaliacoes &lt;dbl&gt;, direcao &lt;chr&gt;, roteiro &lt;chr&gt;, producao &lt;chr&gt;,\n#   elenco &lt;chr&gt;, descricao &lt;chr&gt;, num_criticas_publico &lt;dbl&gt;,\n#   num_criticas_critica &lt;dbl&gt;\n\n\nVamos aplicar tudo o aprendido num segundo exemplo, usando o dataframe imdb para mostrar os seriados com maior número de votos UserVotes.\n\nimdb %&gt;% \n  group_by(series_name) %&gt;% \n  summarize(Votos = sum(UserVotes)) %&gt;% \n  arrange(desc(Votos)) #adicionar ggplot\n\n# A tibble: 27 × 2\n   series_name                  Votos\n   &lt;chr&gt;                        &lt;dbl&gt;\n 1 The Walking Dead           2394586\n 2 Arquivo X                   969628\n 3 Friends                     853287\n 4 Sobrenatural                499046\n 5 Era Uma Vez                 397332\n 6 Stranger Things             391288\n 7 Sherlock                    373114\n 8 Lúcifer                     298412\n 9 Dark                        264631\n10 Como Defender um Assassino  228352\n# ℹ 17 more rows\n\n\nVocê pode ver mais exemplos no Capítulo 8 do livro Curso-R"
  },
  {
    "objectID": "Tutorial2.html#exemplo-base-2",
    "href": "Tutorial2.html#exemplo-base-2",
    "title": "Tutorial 2 - Ggplot2 e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nVamos fazer um gráfico de linha usando a função geom_line() como camada do ggplot usando o dataset netflix_filmes para mostrar o total de capítulos assistidos por dia.\n\nnetflix_filmes %&gt;%  #colocar código ggplot\n  group_by(Date) %&gt;% \n  summarize(filmes=n())\n\n# A tibble: 114 × 2\n   Date       filmes\n   &lt;date&gt;      &lt;int&gt;\n 1 2015-08-24      2\n 2 2015-10-12      1\n 3 2015-11-16      1\n 4 2015-12-23      1\n 5 2016-01-17      2\n 6 2016-01-31      1\n 7 2016-12-29      1\n 8 2017-01-29      1\n 9 2017-03-05      2\n10 2017-03-12      1\n# ℹ 104 more rows\n\n\nPodemos agrupar contagens usando a função floor_date do pacote lubridate para meses, trimestres, etc. Adicionalmente, podemos incluir cores.\n\nnetflix_filmes %&gt;% \n  count(mes = lubridate::floor_date(Date, \"month\"))\n\n# A tibble: 48 × 2\n   mes            n\n   &lt;date&gt;     &lt;int&gt;\n 1 2015-08-01     2\n 2 2015-10-01     1\n 3 2015-11-01     1\n 4 2015-12-01     1\n 5 2016-01-01     3\n 6 2016-12-01     1\n 7 2017-01-01     1\n 8 2017-03-01     3\n 9 2017-05-01     1\n10 2017-06-01     8\n# ℹ 38 more rows"
  },
  {
    "objectID": "Tutorial2.html#adicionando-mais-de-uma-linha",
    "href": "Tutorial2.html#adicionando-mais-de-uma-linha",
    "title": "Tutorial 2 - Ggplot2 e Tidyverse",
    "section": "Adicionando mais de uma linha",
    "text": "Adicionando mais de uma linha\nVamos usar o dataset imdb_filmes para comparar o desempenho dos filmes (nota imdb) em função de ter lucro ou não. Vamos adicionar também uma camada extra para nos mostrar a tendência de ambas curvas, usando geom_smooth().\n\nimdb_filmes %&gt;% \n  mutate(lucro = receita - orcamento,\n         lucro_factor = ifelse(lucro &gt; 0, \"Sim\",\"Não\")) %&gt;% \n  filter(!is.na(lucro)) %&gt;% \n  group_by(lucro_factor,ano) %&gt;% \n  summarise(nota_media=mean(nota_imdb,na.rm=TRUE))\n\n`summarise()` has grouped output by 'lucro_factor'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 169 × 3\n# Groups:   lucro_factor [2]\n   lucro_factor   ano nota_media\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 Não           1921       8.3 \n 2 Não           1923       7   \n 3 Não           1925       8.2 \n 4 Não           1927       7.8 \n 5 Não           1928       8.1 \n 6 Não           1931       7.85\n 7 Não           1932       7.9 \n 8 Não           1933       7.65\n 9 Não           1934       7.55\n10 Não           1935       7.3 \n# ℹ 159 more rows\n\n\nVocê pode ver mais exemplos do geom_line() no livro Curso-R"
  },
  {
    "objectID": "Tutorial2.html#exemplo-base-3",
    "href": "Tutorial2.html#exemplo-base-3",
    "title": "Tutorial 2 - Ggplot2 e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nVamos usar os dados do dataset imdb para comparar as notas de UserRating em cada temporada do seriado The Walking Dead.\n\nseriado_escolhido &lt;- \"The Walking Dead\"\n\nimdb %&gt;% \n  filter(series_name == seriado_escolhido) %&gt;% \n  select(1,2,4,7) \n\n# A tibble: 288 × 4\n   series_name      Episode                     season UserRating\n   &lt;chr&gt;            &lt;chr&gt;                        &lt;dbl&gt;      &lt;dbl&gt;\n 1 The Walking Dead Seed                             3        8.9\n 2 The Walking Dead Sick                             3        8.6\n 3 The Walking Dead Walk with Me                     3        8.2\n 4 The Walking Dead Killer Within                    3        9.3\n 5 The Walking Dead Say the Word                     3        8.2\n 6 The Walking Dead Hounded                          3        8.3\n 7 The Walking Dead When the Dead Come Knocking      3        8.6\n 8 The Walking Dead Made to Suffer                   3        9  \n 9 The Walking Dead The Suicide King                 3        8.2\n10 The Walking Dead Home                             3        8.7\n# ℹ 278 more rows"
  },
  {
    "objectID": "Tutorial2.html#adicionando-cores-por-meio-de-paletas-pré-definidas",
    "href": "Tutorial2.html#adicionando-cores-por-meio-de-paletas-pré-definidas",
    "title": "Tutorial 2 - Ggplot2 e Tidyverse",
    "section": "Adicionando cores por meio de paletas pré-definidas",
    "text": "Adicionando cores por meio de paletas pré-definidas\nVamos adicionar cores para cada temporada, para isto precisamos incluir o argumento fill dentro da camada de estetica aes. Podemos também escolher as cores de paletas pré-definidas como: scale_fill_viridis_d() e scale_fill_brewer()\n\nimdb %&gt;% \n  filter(series_name == seriado_escolhido) %&gt;% \n  select(1,2,4,7) \n\n# A tibble: 288 × 4\n   series_name      Episode                     season UserRating\n   &lt;chr&gt;            &lt;chr&gt;                        &lt;dbl&gt;      &lt;dbl&gt;\n 1 The Walking Dead Seed                             3        8.9\n 2 The Walking Dead Sick                             3        8.6\n 3 The Walking Dead Walk with Me                     3        8.2\n 4 The Walking Dead Killer Within                    3        9.3\n 5 The Walking Dead Say the Word                     3        8.2\n 6 The Walking Dead Hounded                          3        8.3\n 7 The Walking Dead When the Dead Come Knocking      3        8.6\n 8 The Walking Dead Made to Suffer                   3        9  \n 9 The Walking Dead The Suicide King                 3        8.2\n10 The Walking Dead Home                             3        8.7\n# ℹ 278 more rows"
  },
  {
    "objectID": "Tutorial2.html#exemplo-base-4",
    "href": "Tutorial2.html#exemplo-base-4",
    "title": "Tutorial 2 - Ggplot2 e Tidyverse",
    "section": "Exemplo Base",
    "text": "Exemplo Base\nVamos continuar com os dados sobre UserRating usados anteriormente para graficar Boxplots. Neste caso, vamos graficar a distribuição do UserRating considerando apenas a contagem. Podemos usar o argumento bins ou binwidth para ajustar melhor o resultado.\n\nimdb %&gt;% \n  filter(series_name == seriado_escolhido) %&gt;% \n  select(1,2,4,7) \n\n# A tibble: 288 × 4\n   series_name      Episode                     season UserRating\n   &lt;chr&gt;            &lt;chr&gt;                        &lt;dbl&gt;      &lt;dbl&gt;\n 1 The Walking Dead Seed                             3        8.9\n 2 The Walking Dead Sick                             3        8.6\n 3 The Walking Dead Walk with Me                     3        8.2\n 4 The Walking Dead Killer Within                    3        9.3\n 5 The Walking Dead Say the Word                     3        8.2\n 6 The Walking Dead Hounded                          3        8.3\n 7 The Walking Dead When the Dead Come Knocking      3        8.6\n 8 The Walking Dead Made to Suffer                   3        9  \n 9 The Walking Dead The Suicide King                 3        8.2\n10 The Walking Dead Home                             3        8.7\n# ℹ 278 more rows\n\n\nAgora, vamos usar o dataset imdb_filmes para verificar quão lucrativo é um determinado genero.\n\ngenero_escolhido &lt;- \"Comedy\"\n\nimdb_filmes %&gt;% \n  #filter(str_detect(string = generos, pattern = genero_escolhido)) %&gt;% \n  filter(generos == genero_escolhido) %&gt;% \n  mutate(lucro = receita - orcamento)\n\n# A tibble: 560 × 21\n   id_filme  titulo   ano data_lancamento generos duracao pais  idioma orcamento\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 tt0183505 Me, M…  2000 2000-09-08      Comedy      116 USA   Engli…  51000000\n 2 tt1242423 Dear …  2009 2011-07-01      Comedy       87 USA   Engli…        NA\n 3 tt0120434 Vegas…  1997 1997-02-14      Comedy       93 USA   Engli…  25000000\n 4 tt5617916 Airpl…  2019 2019-08-02      Comedy       80 USA   Engli…        NA\n 5 tt7054636 A Mad…  2019 2019-03-01      Comedy      109 USA   Engli…  20000000\n 6 tt0460925 The S…  2006 2006-01-01      Comedy       86 USA   Engli…        NA\n 7 tt0071514 For P…  1974 1974-09-13      Comedy       90 USA   Engli…        NA\n 8 tt0064683 Mondo…  1969 1969-03-14      Comedy       95 USA   Engli…      2100\n 9 tt1073498 Meet …  2008 2008-04-24      Comedy       87 USA   Engli…  30000000\n10 tt0489085 I-See…  2006 2006-03-08      Comedy       92 USA   Engli…   6200000\n# ℹ 550 more rows\n# ℹ 12 more variables: receita &lt;dbl&gt;, receita_eua &lt;dbl&gt;, nota_imdb &lt;dbl&gt;,\n#   num_avaliacoes &lt;dbl&gt;, direcao &lt;chr&gt;, roteiro &lt;chr&gt;, producao &lt;chr&gt;,\n#   elenco &lt;chr&gt;, descricao &lt;chr&gt;, num_criticas_publico &lt;dbl&gt;,\n#   num_criticas_critica &lt;dbl&gt;, lucro &lt;dbl&gt;"
  },
  {
    "objectID": "Tutorial2.html#adicionando-cores-1",
    "href": "Tutorial2.html#adicionando-cores-1",
    "title": "Tutorial 2 - Ggplot2 e Tidyverse",
    "section": "Adicionando cores",
    "text": "Adicionando cores\nPodemos melhorar a apresentação, escolhendo cores diferentes adicionando o argumento fill dentro de geom_histogram().\n\nimdb_filmes %&gt;% \n  #filter(str_detect(string = generos, pattern = genero_escolhido)) %&gt;% \n  filter(generos == genero_escolhido) %&gt;% \n  mutate(lucro = receita - orcamento) \n\n# A tibble: 560 × 21\n   id_filme  titulo   ano data_lancamento generos duracao pais  idioma orcamento\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 tt0183505 Me, M…  2000 2000-09-08      Comedy      116 USA   Engli…  51000000\n 2 tt1242423 Dear …  2009 2011-07-01      Comedy       87 USA   Engli…        NA\n 3 tt0120434 Vegas…  1997 1997-02-14      Comedy       93 USA   Engli…  25000000\n 4 tt5617916 Airpl…  2019 2019-08-02      Comedy       80 USA   Engli…        NA\n 5 tt7054636 A Mad…  2019 2019-03-01      Comedy      109 USA   Engli…  20000000\n 6 tt0460925 The S…  2006 2006-01-01      Comedy       86 USA   Engli…        NA\n 7 tt0071514 For P…  1974 1974-09-13      Comedy       90 USA   Engli…        NA\n 8 tt0064683 Mondo…  1969 1969-03-14      Comedy       95 USA   Engli…      2100\n 9 tt1073498 Meet …  2008 2008-04-24      Comedy       87 USA   Engli…  30000000\n10 tt0489085 I-See…  2006 2006-03-08      Comedy       92 USA   Engli…   6200000\n# ℹ 550 more rows\n# ℹ 12 more variables: receita &lt;dbl&gt;, receita_eua &lt;dbl&gt;, nota_imdb &lt;dbl&gt;,\n#   num_avaliacoes &lt;dbl&gt;, direcao &lt;chr&gt;, roteiro &lt;chr&gt;, producao &lt;chr&gt;,\n#   elenco &lt;chr&gt;, descricao &lt;chr&gt;, num_criticas_publico &lt;dbl&gt;,\n#   num_criticas_critica &lt;dbl&gt;, lucro &lt;dbl&gt;\n\n\nVocê pode ver mais exemplos do Histogramas e Boxplots no Cap. 8 do livro Curso-R\nPodemos usar alternativamente um outro tipo de visualização muito similar, que é a geom_density(). Vamos usar os mesmos exemplos anteriores.\n\nimdb %&gt;% \n  filter(series_name == seriado_escolhido) %&gt;% \n  select(1,2,4,7)\n\n# A tibble: 288 × 4\n   series_name      Episode                     season UserRating\n   &lt;chr&gt;            &lt;chr&gt;                        &lt;dbl&gt;      &lt;dbl&gt;\n 1 The Walking Dead Seed                             3        8.9\n 2 The Walking Dead Sick                             3        8.6\n 3 The Walking Dead Walk with Me                     3        8.2\n 4 The Walking Dead Killer Within                    3        9.3\n 5 The Walking Dead Say the Word                     3        8.2\n 6 The Walking Dead Hounded                          3        8.3\n 7 The Walking Dead When the Dead Come Knocking      3        8.6\n 8 The Walking Dead Made to Suffer                   3        9  \n 9 The Walking Dead The Suicide King                 3        8.2\n10 The Walking Dead Home                             3        8.7\n# ℹ 278 more rows\n\n\nPodemos também customizar a cor da linha e da área abaixo da densidade usando os argumentos fill e color dentro do geom_density() bem como a transparência usando alpha.\n\nimdb_filmes %&gt;% \n  #filter(str_detect(string = generos, pattern = genero_escolhido)) %&gt;% \n  filter(generos == genero_escolhido) %&gt;% \n  mutate(lucro = receita - orcamento)\n\n# A tibble: 560 × 21\n   id_filme  titulo   ano data_lancamento generos duracao pais  idioma orcamento\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 tt0183505 Me, M…  2000 2000-09-08      Comedy      116 USA   Engli…  51000000\n 2 tt1242423 Dear …  2009 2011-07-01      Comedy       87 USA   Engli…        NA\n 3 tt0120434 Vegas…  1997 1997-02-14      Comedy       93 USA   Engli…  25000000\n 4 tt5617916 Airpl…  2019 2019-08-02      Comedy       80 USA   Engli…        NA\n 5 tt7054636 A Mad…  2019 2019-03-01      Comedy      109 USA   Engli…  20000000\n 6 tt0460925 The S…  2006 2006-01-01      Comedy       86 USA   Engli…        NA\n 7 tt0071514 For P…  1974 1974-09-13      Comedy       90 USA   Engli…        NA\n 8 tt0064683 Mondo…  1969 1969-03-14      Comedy       95 USA   Engli…      2100\n 9 tt1073498 Meet …  2008 2008-04-24      Comedy       87 USA   Engli…  30000000\n10 tt0489085 I-See…  2006 2006-03-08      Comedy       92 USA   Engli…   6200000\n# ℹ 550 more rows\n# ℹ 12 more variables: receita &lt;dbl&gt;, receita_eua &lt;dbl&gt;, nota_imdb &lt;dbl&gt;,\n#   num_avaliacoes &lt;dbl&gt;, direcao &lt;chr&gt;, roteiro &lt;chr&gt;, producao &lt;chr&gt;,\n#   elenco &lt;chr&gt;, descricao &lt;chr&gt;, num_criticas_publico &lt;dbl&gt;,\n#   num_criticas_critica &lt;dbl&gt;, lucro &lt;dbl&gt;\n\n\nE por fim, podemos comparar duas ou mais categorias. Vamos fazer isso, comparando os generos “Comedy” e “Drama” para saber se há diferenças significativas em termos de lucro. Para isto, vamos incluir o argumento group dentro da camada aes além de fill e color. Adicionalmente podemos incluir alpha dentro de geom_density().\n\nimdb_filmes %&gt;% \n  #filter(str_detect(string = generos, pattern = genero_escolhido)) %&gt;% \n  filter(generos %in% c(\"Comedy\",\"Drama\")) %&gt;% \n  mutate(lucro = receita - orcamento)\n\n# A tibble: 1,188 × 21\n   id_filme  titulo   ano data_lancamento generos duracao pais  idioma orcamento\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1 tt0183505 Me, M…  2000 2000-09-08      Comedy      116 USA   Engli…  51000000\n 2 tt3703836 Henry…  2015 2016-01-08      Drama        87 USA   Engli…        NA\n 3 tt1142798 The F…  2008 2008-09-12      Drama       111 USA   Engli…        NA\n 4 tt1242423 Dear …  2009 2011-07-01      Comedy       87 USA   Engli…        NA\n 5 tt0109514 Curse…  1994 1995-05-05      Drama       102 USA   Engli…  12577385\n 6 tt0120434 Vegas…  1997 1997-02-14      Comedy       93 USA   Engli…  25000000\n 7 tt0068528 The E…  1972 1973-05-30      Drama       100 USA   Engli…        NA\n 8 tt5617916 Airpl…  2019 2019-08-02      Comedy       80 USA   Engli…        NA\n 9 tt0049800 Storm…  1956 1956-07-31      Drama        85 USA   Engli…        NA\n10 tt7054636 A Mad…  2019 2019-03-01      Comedy      109 USA   Engli…  20000000\n# ℹ 1,178 more rows\n# ℹ 12 more variables: receita &lt;dbl&gt;, receita_eua &lt;dbl&gt;, nota_imdb &lt;dbl&gt;,\n#   num_avaliacoes &lt;dbl&gt;, direcao &lt;chr&gt;, roteiro &lt;chr&gt;, producao &lt;chr&gt;,\n#   elenco &lt;chr&gt;, descricao &lt;chr&gt;, num_criticas_publico &lt;dbl&gt;,\n#   num_criticas_critica &lt;dbl&gt;, lucro &lt;dbl&gt;"
  },
  {
    "objectID": "Tutorial1.html",
    "href": "Tutorial1.html",
    "title": "Tutorial 1 - Tidyverse",
    "section": "",
    "text": "Carregando os dados\nNeste tutorial vamos aprender um pouco mais sobre o tidyverse, em particular vamos conhecer as funções do dplyr:\n\nselect() - seleciona colunas\narrange() - ordena os dados\nfilter() - filtra linhas\nmutate() - cria/modifica colunas\ngroup_by() - agrupa dados por categorias\nsummarize() - sumariza dados\n\nPara praticar, vamos usar o dataset netflix_series_limpo.xlsx e o dataset imdb_series.xlsx. Portanto, o primeiro passo será carregar o dataset:\n\n\nVocê pode baixar os dados aqui.\n\n\nAlternativamente, pode usar este link para criar uma cópia no Workspace.\n\nlibrary(tidyverse)\nlibrary(readxl) # para fazer leitura de arquivos excel\nlibrary(knitr) # para formatar tabelas\n\n#netflix &lt;- read_xlsx(\"\")\n\n#imdb &lt;- read_xlsx(\"\")\n\nUma visão geral dos dados é mostrada a seguir, usando a função head() para visualizar apenas as primeiras linhas do dataframe netflix, pois o dataset completo conta com 923 linhas.\nTambém podemos ver rapidamente a estrutura de cada dataset, utilizando a função str().\nBem como as dimensões utilizando dim():\n\n\nSelecionando colunas\nPara selecionar colunas, utilizamos a função select(), por exemplo, podemos selecionar a coluna episode como o script a seguir:\nTambém é possível selecionar várias colunas, separando-as com ,:\nA função select() conta com vários outros argumentos, recomenda-se a leitura do Capítulo 7 do Livro “Curso-R”.\n\n\nOrdenando os dados\nPara ordenar as linhas, podemos utilizar a função arrange() de forma a termos, por exemplo, uma lista de maior a menor de um determinado valor. Vamos usar o dataframe imdb para exemplificar, ordenando as linhas por ordem crescente de UserRating:\n\n#|label: arrange\n\nGeralmente o uso do arrange() servirá para identificar os maiores valores dentro do dataframe, por exemplo, queremos saber quais são os capítulos com maior UserRating ou seja, ordenar de forma decrescente, além disso, vamos aproveitar o nosso conhecimento de select() para selecionar somente algumas colunas de interesse:\n\n#|label: desc\n\nVocê pode ver mais exemplos do arrange() no livro Curso-R e no livro Ibpad.\n\n\nFiltrando linhas\nPara filtrar valores de um dataframe, primeiro precisamos identificar qual a coluna de interesse e logo utilizar a função filter(). Por exemplo, se quisermos identificar todos os capítulos do dataframe imdb com UserVotes maior a 10.000:\nPara filtrar textos sem correspondência exata, podemos utilizar a função auxiliar str_detect() do pacote stringr. Ela serve para verificar se cada string de um vetor contém um determinado padrão de texto.\nUsando este comando, podemos encontrar qualquer caractere ou string de interesse. Por exemplo, se quisermos identificar todos os capítulos com o string and no título:\nA tabela de cima nos mostra todos os capítulos com and no título porém não nos diz quantos capítulos em total cada seriado tem com o referido string. A próxima função vai nos ajudar a atingir esse objetivo.\nVocê pode ver mais exemplos do filter() no livro Curso-R e no livro Ibpad.\n\n\nAgrupando ou juntando dados por categorias e sumarizando\nA função que usamos para agrupar dados é group_by() e serve para agregar os dados de acordo com algum critério de interesse. Comumente é usado junto com uma função para sumarizar o resultado, chamada summarize(). Vamos mostrar o uso de ambos em conjunto.\nVamos aproveitar o exercício anterior (capítulos com o string and) e agrupar por seriado para finalmente, contabilizar o número de capítulos com o string and para cada seriado.\nVocê pode ver mais exemplos do summarize() no livro Curso-R e de group_by() no livro Ibpad.\n\n\nModificando e/ou criando novas colunas\nPara modificar uma coluna existente ou criar uma nova coluna, utilizamos a função mutate(). Por exemplo, podemos usá-la para criar uma nova coluna no dataframe netflix contendo apenas o ano em que cada capítulo foi exibido (no futuro, esta nova coluna pode nos ajudar a criar um gráfico de barras para cada ano):\nUm segundo exemplo com o dataframe imdb mostra a soma das colunas r1 a r10, numa nova coluna chamada r_total. As referidas colunas são na verdade proporções do número de pessoas que votou com nota 1 (r1) até nota 10 (r10), portanto a soma delas deveria ser igual a \\(1\\).\nVocê pode ver mais exemplos do mutate() no livro Curso-R\nExistem muitas mais funções no tidyverse além das principais que já vimos, dentre elas, vamos mostrar a função join.\n\n\nJuntando dois ou mais dataframes\nExistem vários tipos de join mas aqui vamos focar especificamente na função left_join() que mantém todas as observações da primeira base e adiciona colunas da segunda (para mais detalhes consultar o livro R for Data Science e o livro Curso-R).\nVejamos no exemplo a seguir. Vamos juntar as duas bases, netflix e imdb, tomando como a base de referência netflix. O que queremos é extrair a nota UserRating de imdb e inclui-la como nova coluna na base netflix para cada capítulo e seriado.\nNote que para o R identificar corretamente cada capítulo, neste caso devemos comparar duas colunas de cada base, a do título do seriado e a do título do capítulo.\nFinalmente vamos limpar as colunas e deixar apenas aquelas que nos interessam."
  },
  {
    "objectID": "data2/dash_meu_netflix.html",
    "href": "data2/dash_meu_netflix.html",
    "title": "Meu Netflix em números",
    "section": "",
    "text": "selectInput(\"ano\",\n            label = \"Selecione o ano da sua preferência\",\n            choices = c(2015,2016,2017,2018,2019,2020,2021),\n            selected = 2020)\n\n\nSelecione o ano da sua preferência\n\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\nsliderInput(\"ncapitulos\",\n            label = \"Selecione o número mínimo de capítulos\",\n            min = 0,\n            max = 100,\n            value = 20,\n            step = 1,\n            dragRange = TRUE\n            )\n\n\nSelecione o número mínimo de capítulos\n\n\n\n\n\n\n\n\n\n\n\nminhascores &lt;- c(\"#9b5de5\",\"#f15bb5\",\"#fee440\",\"#00bbf9\",\"#00f5d4\")\ntheme_set(theme_bw())\n\nnetflix &lt;- read.csv(\"NetflixViewingHistory_20211116.csv\")\nnetflix$Date &lt;- as.Date(netflix$Date, format = \"%m/%d/%y\")\nnetflix2 &lt;- netflix %&gt;% \n  filter(str_detect(Title, \":\")) %&gt;% \n  separate(Title, c(\"Titulo\",\"Temporada\",\"Capitulo\"), \":\")\n\nWarning: Expected 3 pieces. Additional pieces discarded in 77 rows [5, 6, 7, 8, 9, 10,\n13, 14, 28, 29, 56, 70, 71, 72, 76, 121, 130, 131, 166, 225, ...].\n\n\nWarning: Expected 3 pieces. Missing pieces filled with `NA` in 45 rows [31, 32, 40, 77,\n79, 133, 164, 184, 185, 186, 187, 190, 191, 192, 196, 197, 198, 199, 200, 201,\n...].\n\nnetflix3 &lt;- netflix2 %&gt;% \n  filter(!is.na(Capitulo))\n\nrenderPlotly({  \nmais_assistidos &lt;- netflix3 %&gt;% \n  mutate(Ano = as.numeric(format(Date, '%Y'))) %&gt;% \n  filter(Ano == input$ano) %&gt;% \n  group_by(Titulo) %&gt;% \n  count() %&gt;% \n  arrange(desc(n)) %&gt;% \n  ggplot(aes(reorder(Titulo, -n), n))+\n  geom_bar(stat = \"identity\", fill = \"#ED79F9\") +\n  coord_flip()+\n  labs(title = glue(\"Seriados mais assistidos em {input$ano}\"))+\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank())+\n  scale_fill_manual(values=minhascores)\nggplotly(mais_assistidos)\n  })\n\n\n\n\n\n\n\n\nrenderPlotly({\n  netflix3 %&gt;% \n  group_by(Date) %&gt;% \n  count() %&gt;% \n  filter(Date &gt; glue(\"{input$ano}-01-01\")) %&gt;% \n  ggplot(aes(Date,n))+\n  geom_col(fill=\"#9b5de5\")+\n  geom_smooth(color=\"#e32d91\", size = 1.2, se=FALSE)\n  \n})\n\n\n\n\n\n\n\n\nrenderPlotly({\n  mais_assistidos_total &lt;- netflix3 %&gt;% \n  #mutate(Ano = as.numeric(format(Date, '%Y'))) %&gt;% \n  #filter(Ano == input$ano) %&gt;% \n  group_by(Titulo) %&gt;% \n  count() %&gt;% \n  arrange(desc(n)) %&gt;% \n  filter(n&gt;=input$ncapitulos) %&gt;% \n  ggplot(aes(reorder(Titulo, -n), factor(n), fill=factor(n)))+\n  geom_bar(stat = \"identity\") +\n  coord_flip()+\n  labs(title = \"Seriados mais assistidos em todo o período\")+\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position = \"none\")\n  \nggplotly(mais_assistidos_total)\n  \n})\n\n\n\n\n\n\n\n\n\n\n\nrenderTable({  \nnetflix3 %&gt;% \n  mutate(Ano = as.numeric(format(Date, '%Y'))) %&gt;% \n  filter(Ano == input$ano) %&gt;% \n  group_by(Titulo) %&gt;% \n  count() %&gt;% \n  arrange(desc(n)) %&gt;% \n  filter(n&gt;input$ncapitulos) \n})\n\n\n\n\n\n\n\n\ncapitulos_ano &lt;- netflix3 %&gt;% \n  mutate(Ano = as.numeric(format(Date, '%Y'))) %&gt;% \n  group_by(Ano) %&gt;% \n  count() %&gt;% \n  ggplot(aes(factor(Ano), n))+\n  geom_bar(stat = \"identity\", fill=\"#3ADAC6\")+\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank())+\n  labs(title =\"Número de capítulos por ano\")\n\ncapitulos_ano"
  },
  {
    "objectID": "data2/dash_meu_netflix.html#column",
    "href": "data2/dash_meu_netflix.html#column",
    "title": "Meu Netflix em números",
    "section": "",
    "text": "selectInput(\"ano\",\n            label = \"Selecione o ano da sua preferência\",\n            choices = c(2015,2016,2017,2018,2019,2020,2021),\n            selected = 2020)\n\n\nSelecione o ano da sua preferência\n\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n\n\n\n\nsliderInput(\"ncapitulos\",\n            label = \"Selecione o número mínimo de capítulos\",\n            min = 0,\n            max = 100,\n            value = 20,\n            step = 1,\n            dragRange = TRUE\n            )\n\n\nSelecione o número mínimo de capítulos"
  },
  {
    "objectID": "data2/dash_meu_netflix.html#column-1",
    "href": "data2/dash_meu_netflix.html#column-1",
    "title": "Meu Netflix em números",
    "section": "",
    "text": "minhascores &lt;- c(\"#9b5de5\",\"#f15bb5\",\"#fee440\",\"#00bbf9\",\"#00f5d4\")\ntheme_set(theme_bw())\n\nnetflix &lt;- read.csv(\"NetflixViewingHistory_20211116.csv\")\nnetflix$Date &lt;- as.Date(netflix$Date, format = \"%m/%d/%y\")\nnetflix2 &lt;- netflix %&gt;% \n  filter(str_detect(Title, \":\")) %&gt;% \n  separate(Title, c(\"Titulo\",\"Temporada\",\"Capitulo\"), \":\")\n\nWarning: Expected 3 pieces. Additional pieces discarded in 77 rows [5, 6, 7, 8, 9, 10,\n13, 14, 28, 29, 56, 70, 71, 72, 76, 121, 130, 131, 166, 225, ...].\n\n\nWarning: Expected 3 pieces. Missing pieces filled with `NA` in 45 rows [31, 32, 40, 77,\n79, 133, 164, 184, 185, 186, 187, 190, 191, 192, 196, 197, 198, 199, 200, 201,\n...].\n\nnetflix3 &lt;- netflix2 %&gt;% \n  filter(!is.na(Capitulo))\n\nrenderPlotly({  \nmais_assistidos &lt;- netflix3 %&gt;% \n  mutate(Ano = as.numeric(format(Date, '%Y'))) %&gt;% \n  filter(Ano == input$ano) %&gt;% \n  group_by(Titulo) %&gt;% \n  count() %&gt;% \n  arrange(desc(n)) %&gt;% \n  ggplot(aes(reorder(Titulo, -n), n))+\n  geom_bar(stat = \"identity\", fill = \"#ED79F9\") +\n  coord_flip()+\n  labs(title = glue(\"Seriados mais assistidos em {input$ano}\"))+\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank())+\n  scale_fill_manual(values=minhascores)\nggplotly(mais_assistidos)\n  })\n\n\n\n\n\n\n\n\nrenderPlotly({\n  netflix3 %&gt;% \n  group_by(Date) %&gt;% \n  count() %&gt;% \n  filter(Date &gt; glue(\"{input$ano}-01-01\")) %&gt;% \n  ggplot(aes(Date,n))+\n  geom_col(fill=\"#9b5de5\")+\n  geom_smooth(color=\"#e32d91\", size = 1.2, se=FALSE)\n  \n})\n\n\n\n\n\n\n\n\nrenderPlotly({\n  mais_assistidos_total &lt;- netflix3 %&gt;% \n  #mutate(Ano = as.numeric(format(Date, '%Y'))) %&gt;% \n  #filter(Ano == input$ano) %&gt;% \n  group_by(Titulo) %&gt;% \n  count() %&gt;% \n  arrange(desc(n)) %&gt;% \n  filter(n&gt;=input$ncapitulos) %&gt;% \n  ggplot(aes(reorder(Titulo, -n), factor(n), fill=factor(n)))+\n  geom_bar(stat = \"identity\") +\n  coord_flip()+\n  labs(title = \"Seriados mais assistidos em todo o período\")+\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position = \"none\")\n  \nggplotly(mais_assistidos_total)\n  \n})"
  },
  {
    "objectID": "data2/dash_meu_netflix.html#column-2",
    "href": "data2/dash_meu_netflix.html#column-2",
    "title": "Meu Netflix em números",
    "section": "",
    "text": "renderTable({  \nnetflix3 %&gt;% \n  mutate(Ano = as.numeric(format(Date, '%Y'))) %&gt;% \n  filter(Ano == input$ano) %&gt;% \n  group_by(Titulo) %&gt;% \n  count() %&gt;% \n  arrange(desc(n)) %&gt;% \n  filter(n&gt;input$ncapitulos) \n})\n\n\n\n\n\n\n\n\ncapitulos_ano &lt;- netflix3 %&gt;% \n  mutate(Ano = as.numeric(format(Date, '%Y'))) %&gt;% \n  group_by(Ano) %&gt;% \n  count() %&gt;% \n  ggplot(aes(factor(Ano), n))+\n  geom_bar(stat = \"identity\", fill=\"#3ADAC6\")+\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank())+\n  labs(title =\"Número de capítulos por ano\")\n\ncapitulos_ano"
  },
  {
    "objectID": "data3a/webinar3a.html",
    "href": "data3a/webinar3a.html",
    "title": "Webinar 3a - Business Analytics e Data Mining",
    "section": "",
    "text": "Para este webinar vamos usar dados do Titanic (para os métodos supervisionados a partir de kaggle) e um dataset hipotético sobre clientes (para o método não supervisionado)"
  },
  {
    "objectID": "data3a/webinar3a.html#criar-test-e-train-datasets",
    "href": "data3a/webinar3a.html#criar-test-e-train-datasets",
    "title": "Webinar 3a - Business Analytics e Data Mining",
    "section": "Criar test e train datasets",
    "text": "Criar test e train datasets\nPodemos usar a função initial_split() do pacote tidymodels.\n\nset.seed &lt;- 2020\n\n#split &lt;- initial_split(titanic_df, strata =Survived)\n\n#titanic_train &lt;- training(split)\n#titanic_test &lt;- testing(split)\n\nMas neste caso, o kaggle já nos deu os datasets separados, portanto, o nosso dataset titanic_df é o dataset de treino. Além disso, vamos retirar eventuais observações com NA.\n\nlibrary(skimr)\n\nskim(titanic_df)\n\n\nData summary\n\n\nName\ntitanic_df\n\n\nNumber of rows\n891\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nPclass\n0\n1\nFALSE\n3\n3: 491, 1: 216, 2: 184\n\n\nSex\n0\n1\nFALSE\n2\nmal: 577, fem: 314\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSurvived\n0\n1.0\n0.38\n0.49\n0.00\n0.00\n0.00\n1\n1.00\n▇▁▁▁▅\n\n\nAge\n177\n0.8\n29.70\n14.53\n0.42\n20.12\n28.00\n38\n80.00\n▂▇▅▂▁\n\n\nSiblings_and_Spouses\n0\n1.0\n0.52\n1.10\n0.00\n0.00\n0.00\n1\n8.00\n▇▁▁▁▁\n\n\nParents_and_Children\n0\n1.0\n0.38\n0.81\n0.00\n0.00\n0.00\n0\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1.0\n32.20\n49.69\n0.00\n7.91\n14.45\n31\n512.33\n▇▁▁▁▁\n\n\n\n\ntitanic_df &lt;- titanic_df %&gt;% \n  filter(!is.na(Age))\n\ntitanic_train &lt;- titanic_df\n\nNesta análise, vamos formular algumas perguntas:\n\nQual é a relação entre as características e as chances de sobrevivência de um passageiro.\nPrevisão de sobrevivência para o navio inteiro."
  },
  {
    "objectID": "data3a/webinar3a.html#regressão-linear",
    "href": "data3a/webinar3a.html#regressão-linear",
    "title": "Webinar 3a - Business Analytics e Data Mining",
    "section": "Regressão Linear",
    "text": "Regressão Linear\nVamos usar primeiramente um modelo linear para fazer a previsão de sobrevivência.\n\ntitanic_lm &lt;- lm(Survived~., data=titanic_train)\n\nsummary(titanic_lm)\n\n\nCall:\nlm(formula = Survived ~ ., data = titanic_train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.12858 -0.23132 -0.06947  0.23280  0.99556 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           1.1740788  0.0652667  17.989  &lt; 2e-16 ***\nPclass2              -0.1980470  0.0481540  -4.113 4.37e-05 ***\nPclass3              -0.3892163  0.0472077  -8.245 8.07e-16 ***\nSexmale              -0.4886489  0.0314332 -15.546  &lt; 2e-16 ***\nAge                  -0.0065370  0.0011327  -5.771 1.18e-08 ***\nSiblings_and_Spouses -0.0533662  0.0173869  -3.069  0.00223 ** \nParents_and_Children -0.0120393  0.0190338  -0.633  0.52725    \nFare                  0.0002971  0.0003564   0.834  0.40480    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3824 on 706 degrees of freedom\nMultiple R-squared:  0.4006,    Adjusted R-squared:  0.3947 \nF-statistic: 67.41 on 7 and 706 DF,  p-value: &lt; 2.2e-16\n\n\nNote que Parents e Fare não são significativos pois os valores p são 0,52725 e 0,4048 respectivamente, ambos maiores a 0,05 de nível de significância.\nPorém, o valor de Survived inicia com um intercepto de 1,1740, ou seja, acima do valor de 1 que indicaria sobrevivência.\nVamos calcular a probabilidade de sobrevivência para cada passageiro e plotar um gráfico para reforçar este argumento.\n\npred_lm &lt;- predict(titanic_lm, data=titanic_train)\n\ntitanic_df &lt;- titanic_df %&gt;% \n  mutate(prob_lm = pred_lm)\n\n\ntitanic_df %&gt;% \n  ggplot(aes(factor(Survived),prob_lm,\n             color=factor(Survived)))+\n  geom_boxplot()+\n  scale_y_continuous(limits = c(-0.2,1.1),\n                     n.breaks = 12)+\n  scale_color_brewer(palette = \"Dark2\")+\n  theme_minimal()"
  },
  {
    "objectID": "data3a/webinar3a.html#regressão-logística",
    "href": "data3a/webinar3a.html#regressão-logística",
    "title": "Webinar 3a - Business Analytics e Data Mining",
    "section": "Regressão Logística",
    "text": "Regressão Logística\nUm modelo que responde melhor quando a variável de saída está entre 0 e 1 é o de regressão logística. Podemos ver os resultados do modelo abaixo.\n\ntitanic_logistic_model &lt;- glm(Survived~., \n                      data = titanic_train, \n                      family = \"binomial\")\n\nsummary(titanic_logistic_model)\n\n\nCall:\nglm(formula = Survived ~ ., family = \"binomial\", data = titanic_train)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           4.179995   0.503420   8.303  &lt; 2e-16 ***\nPclass2              -1.292538   0.321756  -4.017 5.89e-05 ***\nPclass3              -2.501069   0.338744  -7.383 1.54e-13 ***\nSexmale              -2.637451   0.220077 -11.984  &lt; 2e-16 ***\nAge                  -0.044159   0.008264  -5.343 9.12e-08 ***\nSiblings_and_Spouses -0.376847   0.127483  -2.956  0.00312 ** \nParents_and_Children -0.061268   0.122927  -0.498  0.61820    \nFare                  0.002043   0.002564   0.797  0.42543    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 964.52  on 713  degrees of freedom\nResidual deviance: 635.78  on 706  degrees of freedom\nAIC: 651.78\n\nNumber of Fisher Scoring iterations: 5\n\n\nOs coeficientes do modelo logístico são difíceis de intepretar. Para variáveis categóricas (fatores), o coeficiente representa o odds ratio entre por exemplo para a variável ‘Sex’, a interpretação é:\n\\[log(oddsratio)=log(\\beta)\\] portanto, \\[oddsratio=e^{\\beta}=e^{-2.6374}=0.07154\\] Ou seja, um passageiro homem tinha \\(0,07154\\) vezes menos chances (odds) de sobrevivência do que uma passageira mulher. Ou em outras palavras uma passageira mulher tinha \\(1/0,07154=13.97\\) vezes mais chances de sobrevivência.\nNota: \\(odds=\\frac{p}{1-p}\\) , sendo \\(p\\) a probabilidade de um evento acontecer.\nIgualmente, podemos interpretar os coeficientes relacionados com a classe: um passageiro na 2a classe tinha \\(e^{-1.292538}=0,2745\\) vezes menos chances de sobrevivência do que um passageiro da 1a classe (ou um passageiro de 1a classe tinha \\(1/0,2745=3,64\\) vezes mais chances de sobrevivência do que um de 2a classe). Já, um passageiro da 3a classe tinha \\(e^{-2.501069}=0,0819\\) vezes menos chances de sobrevivência do que um passageiro da 1a classe (ou um passageiro de 1a classe tinha \\(1/0,0819=12,21\\) vezes mais chances de sobrevivência do que um de 3a classe).\nPara as variáveis continuas, a interpretação é um pouco mais direta. Por exemplo, para cada adicional de idade, as chances de sobrevivência reduzem em \\(e^{-0.0441}=0,9568\\) vezes (aproximadamente em 50% de probabilidade).\nObservação: Em alguns casos, vale a pena alterar o fator base de uma determinada variável, principalmente quando os níveis do fator possuem um valor de referência. Neste caso pode usar-se a função relevel.\nAgora que temos um objeto denominado titanic_logistic_model, podemos prever os valores de sobrevivência (0,1) para o nosso dataset de treino, usando a função predict. Após, mostraremos uma primeira medida de precisão do modelo que é calcular o Pseudo \\(R^{2}\\):\n\nlibrary(broom)\n\nglance(titanic_logistic_model) %&gt;% \n  summarize(pR2 = 1 - deviance / null.deviance)\n\n# A tibble: 1 × 1\n    pR2\n  &lt;dbl&gt;\n1 0.341\n\ntitanic_df &lt;- titanic_df %&gt;% \n  mutate(prob_log = predict(titanic_logistic_model, \n                        type = \"response\"))\n\nmedia &lt;- mean(titanic_train$Survived)\n\n\ntitanic_df %&gt;% \n  ggplot(aes(factor(Survived),prob_log,\n             color=factor(Survived)))+\n  geom_boxplot()+\n  scale_y_continuous(limits = c(-0.2,1.1),\n                     n.breaks = 12)+\n  scale_color_brewer(palette = \"Dark2\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nA probabilidade média de sobrevivência é 0.41, portanto podemos definir o limiar da previsão como sendo esse valor.\n\ntitanic_df$pred_log &lt;- ifelse(titanic_df$prob_log &gt; media, 1, 0)\n\nPara calcular a precisão da previsão, podemos comparar as médias.\n\npred_log &lt;- mean(titanic_df$Survived==titanic_df$pred_log)\n\nDesta forma, chegamos ao valor de 79.27 %.\nUma forma mais elegante de verificar a precisão do modelo é usar a Curva ROC e o valor de AUC.\n\ntitanic_df$Survived &lt;- factor(titanic_df$Survived, levels=c(1,0))\n\nautoplot(roc_curve(titanic_df, Survived, prob_log))\n\n\n\n\n\n\n\nroc_auc(titanic_df, Survived, prob_log)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.859"
  },
  {
    "objectID": "data3a/webinar3a.html#como-lidar-com-dados-faltantes",
    "href": "data3a/webinar3a.html#como-lidar-com-dados-faltantes",
    "title": "Webinar 3a - Business Analytics e Data Mining",
    "section": "Como lidar com dados faltantes",
    "text": "Como lidar com dados faltantes\nComo vimos anteriormente, o dataset continha um grande número de valores NA na variável Age. No procedimento anterior, simplesmente eliminamos as linhas com NA, ou seja, 177 de um total de 891.\nVamos conhecer um método de inputar valores para esses NA, e ver o efeito que pode dar nosso modelo. Vamos retornar ao dataframe titanic.\n\n# Imputar usando média\ntitanic_train_age &lt;- titanic %&gt;% \n  mutate(Age_imputada = ifelse(is.na(Age),\n                               round(mean(Age,na.rm=TRUE),digits = 2),\n                               Age)) %&gt;% \n# Criar indicador para saber qual foi imputado e qual não\n  mutate(faltante_age = ifelse(is.na(Age),1,0)) %&gt;% \n  select(-PassengerId,-Name,-Ticket, -Cabin, -Embarked) %&gt;% \n  rename(Siblings_and_Spouses = SibSp,\n         Parents_and_Children = Parch)\n\nUma vez imputados os valores de idade, podemos calcular novamente o modelo logístico:\n\ntitanic_train_age &lt;- titanic_train_age %&gt;% \n  select(-Age, -faltante_age)\n\ntitanic_logistic_model2 &lt;- glm(Survived~., \n                      data = titanic_train_age, family = \"binomial\")\n\nsummary(titanic_logistic_model2)\n\n\nCall:\nglm(formula = Survived ~ ., family = \"binomial\", data = titanic_train_age)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           3.840871   0.446533   8.602  &lt; 2e-16 ***\nPclass2              -1.023118   0.293891  -3.481 0.000499 ***\nPclass3              -2.149863   0.289626  -7.423 1.15e-13 ***\nSexmale              -2.760928   0.199254 -13.856  &lt; 2e-16 ***\nSiblings_and_Spouses -0.350082   0.109612  -3.194 0.001404 ** \nParents_and_Children -0.113337   0.117621  -0.964 0.335258    \nFare                  0.002991   0.002447   1.223 0.221493    \nAge_imputada         -0.039490   0.007843  -5.035 4.78e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.66  on 890  degrees of freedom\nResidual deviance:  788.67  on 883  degrees of freedom\nAIC: 804.67\n\nNumber of Fisher Scoring iterations: 5\n\n\nO valor do Pseudo R2 para o modelo logístico 2 é:\n\nglance(titanic_logistic_model2) %&gt;% summarize(pR2 = 1-deviance/null.deviance)\n\n# A tibble: 1 × 1\n    pR2\n  &lt;dbl&gt;\n1 0.335\n\n\ne por fim, a curva ROC e o valor de AUC:\n\ntitanic_train_age &lt;- titanic_train_age %&gt;% \n  mutate(prob_log_age = predict(titanic_logistic_model2, \n                        type = \"response\"))\n\ntitanic_train_age$Survived &lt;- factor(titanic_train_age$Survived, levels=c(1,0))\n\nautoplot(roc_curve(titanic_train_age, Survived, prob_log_age))\n\n\n\n\n\n\n\nroc_auc &lt;- roc_auc(titanic_train_age, Survived, prob_log_age)\n\nroc_auc\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.856\n\nlibrary(WVPlots)\nGainCurvePlot(titanic_train_age, \"prob_log_age\",\"Survived\",\"Curva de Ganho\")\n\n\n\n\n\n\n\n\nE a área abaixo da curva (AUC) é roc_auc, binary, 0.8560088. Mais sobre ROC, GainCurve e Matriz de Confusão podem ser encontradas em: https://community.tibco.com/wiki/gains-vs-roc-curves-do-you-understand-difference"
  },
  {
    "objectID": "data3a/webinar3a.html#árvores-de-decisão",
    "href": "data3a/webinar3a.html#árvores-de-decisão",
    "title": "Webinar 3a - Business Analytics e Data Mining",
    "section": "Árvores de Decisão",
    "text": "Árvores de Decisão\nUsaremos o dataframe titanic_train.\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\n\n\ntitanic_tree_model &lt;- rpart(Survived~.,\n                    data = titanic_train, \n                    method = \"class\", \n                    control = rpart.control(cp = 0))\n\n# Plot the loan_model with default settings\nrpart.plot(titanic_tree_model)\n\n\n\n\n\n\n\nrpart.plot(titanic_tree_model, type = 3, box.palette = c(\"#f16727\", \"#1a954d\"), fallen.leaves = TRUE)\n\n\n\n\n\n\n\n\nAgora que a árvore de decisão foi criada, podemos verificar a sua precisão comparando as previsões com a coluna Survived.\n\npred_tree &lt;- predict(titanic_tree_model, titanic_train,\n                             type = \"class\")\n\n\ncaret::confusionMatrix(as.factor(pred_tree),as.factor(titanic_train$Survived))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 391  69\n         1  33 221\n                                         \n               Accuracy : 0.8571         \n                 95% CI : (0.8293, 0.882)\n    No Information Rate : 0.5938         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.6979         \n                                         \n Mcnemar's Test P-Value : 0.0005292      \n                                         \n            Sensitivity : 0.9222         \n            Specificity : 0.7621         \n         Pos Pred Value : 0.8500         \n         Neg Pred Value : 0.8701         \n             Prevalence : 0.5938         \n         Detection Rate : 0.5476         \n   Detection Prevalence : 0.6443         \n      Balanced Accuracy : 0.8421         \n                                         \n       'Positive' Class : 0              \n                                         \n\ntitanic_df &lt;- titanic_df %&gt;% \n  mutate(pred_tree=pred_tree) \n\ntitanic_df$pred_tree &lt;- factor(titanic_df$pred_tree, levels = c(1,0))\n\ntitanic_df %&gt;%   \n  conf_mat(Survived, pred_tree)\n\n          Truth\nPrediction   1   0\n         1 221  33\n         0  69 391\n\n\nVamos calcular a curva ROC, AUC e GainCurve para o modelo de Árvore:\n\ntitanic_df$pred_tree &lt;- as.character(titanic_df$pred_tree)\ntitanic_df$pred_tree &lt;- as.numeric(titanic_df$pred_tree)\n\n#autoplot(roc_curve(titanic_df, truth = Survived, estimate = pred_tree))\n\n#roc_auc(titanic_df, truth = Survived, estimate = pred_tree)\n\nGainCurvePlot(titanic_df, \"pred_tree\",\"Survived\",\"Gain Curve para o Modelo Arvore de Decisão\")"
  },
  {
    "objectID": "data3a/webinar3a.html#random-forest",
    "href": "data3a/webinar3a.html#random-forest",
    "title": "Webinar 3a - Business Analytics e Data Mining",
    "section": "Random Forest",
    "text": "Random Forest\nPara construir o modelo RF, é necessário eliminar os NAs que aparecem na variável Age. Como nosso dataset não tem mais NAs, podemos continuar com a previsão.\n\ntitanic_rf_model &lt;- ranger(Survived~., # formula \n                         titanic_train, # data\n                         num.trees = 500, \n                         respect.unordered.factors = \"order\", \n                         seed = set.seed,\n                         classification = TRUE)\n\nAgora sim, podemos usar o modelo para fazer as previsões usando predict\n\npred_rf &lt;- predict(titanic_rf_model, titanic_train)$predictions\n\n\ntitanic_rf_model\n\nRanger result\n\nCall:\n ranger(Survived ~ ., titanic_train, num.trees = 500, respect.unordered.factors = \"order\",      seed = set.seed, classification = TRUE) \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      714 \nNumber of independent variables:  6 \nMtry:                             2 \nTarget node size:                 1 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error:             17.79 % \n\ntitanic_df &lt;- titanic_df %&gt;% \n  mutate(pred_rf = pred_rf)\n\nA curva ROC, o AUC:\n\n#autoplot(roc_curve(titanic_df, truth = Survived, estimate = pred_rf))\n\n\n#roc_auc(titanic_df, truth = Survived, estimate = pred_rf)\n\nE a Matriz de confusão para o modelo de Random Forest é:\n\ntitanic_df$pred_rf &lt;- factor(titanic_df$pred_rf, levels = c(1,0))\n\ntitanic_df %&gt;%   \n  conf_mat(Survived, pred_rf)\n\n          Truth\nPrediction   1   0\n         1 244  14\n         0  46 410"
  }
]